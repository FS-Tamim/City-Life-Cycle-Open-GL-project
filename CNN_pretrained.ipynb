{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FS-Tamim/City-Life-Cycle-Open-GL-project/blob/main/CNN_pretrained.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bV9aHrv9qAiv",
        "outputId": "231210e0-3ac4-47c7-bb2c-12645425ddc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Keras-Preprocessing\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from Keras-Preprocessing) (1.23.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from Keras-Preprocessing) (1.16.0)\n",
            "Installing collected packages: Keras-Preprocessing\n",
            "Successfully installed Keras-Preprocessing-1.1.2\n",
            "Collecting ktrain\n",
            "  Downloading ktrain-0.37.6.tar.gz (25.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.3/25.3 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from ktrain) (1.2.2)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from ktrain) (3.7.1)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from ktrain) (1.5.3)\n",
            "Requirement already satisfied: fastprogress>=0.1.21 in /usr/local/lib/python3.10/dist-packages (from ktrain) (1.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ktrain) (2.31.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from ktrain) (1.3.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ktrain) (23.1)\n",
            "Collecting langdetect (from ktrain)\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (from ktrain) (0.42.1)\n",
            "Collecting cchardet (from ktrain)\n",
            "  Downloading cchardet-2.1.7.tar.gz (653 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m653.6/653.6 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from ktrain) (4.0.0)\n",
            "Collecting syntok>1.3.3 (from ktrain)\n",
            "  Downloading syntok-1.4.4-py3-none-any.whl (24 kB)\n",
            "Collecting tika (from ktrain)\n",
            "  Downloading tika-2.6.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers>=4.17.0 (from ktrain)\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece (from ktrain)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras_bert>=0.86.0 (from ktrain)\n",
            "  Downloading keras-bert-0.89.0.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting whoosh (from ktrain)\n",
            "  Downloading Whoosh-2.7.4-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.8/468.8 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras_bert>=0.86.0->ktrain) (1.23.5)\n",
            "Collecting keras-transformer==0.40.0 (from keras_bert>=0.86.0->ktrain)\n",
            "  Downloading keras-transformer-0.40.0.tar.gz (9.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting keras-pos-embd==0.13.0 (from keras-transformer==0.40.0->keras_bert>=0.86.0->ktrain)\n",
            "  Downloading keras-pos-embd-0.13.0.tar.gz (5.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting keras-multi-head==0.29.0 (from keras-transformer==0.40.0->keras_bert>=0.86.0->ktrain)\n",
            "  Downloading keras-multi-head-0.29.0.tar.gz (13 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting keras-layer-normalization==0.16.0 (from keras-transformer==0.40.0->keras_bert>=0.86.0->ktrain)\n",
            "  Downloading keras-layer-normalization-0.16.0.tar.gz (3.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting keras-position-wise-feed-forward==0.8.0 (from keras-transformer==0.40.0->keras_bert>=0.86.0->ktrain)\n",
            "  Downloading keras-position-wise-feed-forward-0.8.0.tar.gz (4.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting keras-embed-sim==0.10.0 (from keras-transformer==0.40.0->keras_bert>=0.86.0->ktrain)\n",
            "  Downloading keras-embed-sim-0.10.0.tar.gz (3.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting keras-self-attention==0.51.0 (from keras-multi-head==0.29.0->keras-transformer==0.40.0->keras_bert>=0.86.0->ktrain)\n",
            "  Downloading keras-self-attention-0.51.0.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->ktrain) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->ktrain) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->ktrain) (4.42.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->ktrain) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->ktrain) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->ktrain) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->ktrain) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->ktrain) (2023.3)\n",
            "Requirement already satisfied: regex>2016 in /usr/local/lib/python3.10/dist-packages (from syntok>1.3.3->ktrain) (2023.6.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.17.0->ktrain) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers>=4.17.0->ktrain)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.17.0->ktrain) (6.0.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers>=4.17.0->ktrain)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers>=4.17.0->ktrain)\n",
            "  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.17.0->ktrain) (4.66.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect->ktrain) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->ktrain) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ktrain) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->ktrain) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ktrain) (2023.7.22)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->ktrain) (1.10.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->ktrain) (3.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tika->ktrain) (67.7.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers>=4.17.0->ktrain) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers>=4.17.0->ktrain) (4.7.1)\n",
            "Building wheels for collected packages: ktrain, keras_bert, keras-transformer, keras-embed-sim, keras-layer-normalization, keras-multi-head, keras-pos-embd, keras-position-wise-feed-forward, keras-self-attention, cchardet, langdetect, tika\n",
            "  Building wheel for ktrain (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ktrain: filename=ktrain-0.37.6-py3-none-any.whl size=25321127 sha256=2d081a1f4f28648fc805431ae5e183cecaf5838d67d1c83faa09aa3a9a395af4\n",
            "  Stored in directory: /root/.cache/pip/wheels/24/d0/6c/23f9342c14068bdffd16bb983773f9d2387a00468a5d33e58b\n",
            "  Building wheel for keras_bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras_bert: filename=keras_bert-0.89.0-py3-none-any.whl size=33499 sha256=e5628ab88e5315acba907c31efed7174097828154e35210f70e32916c43ad87b\n",
            "  Stored in directory: /root/.cache/pip/wheels/89/0c/04/646b6fdf6375911b42c8d540a8a3fda8d5d77634e5dcbe7b26\n",
            "  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-transformer: filename=keras_transformer-0.40.0-py3-none-any.whl size=12286 sha256=0c41f2d158bf4fce5444f62141500457806934807b1669f3cc0691056d0331d8\n",
            "  Stored in directory: /root/.cache/pip/wheels/f2/cb/22/75a0ad376129177f7c95c0d91331a18f5368fd657f4035ba7c\n",
            "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.10.0-py3-none-any.whl size=3945 sha256=3eab9bade2905f8ba5b71c328dabba8825474fb1e19bef940a7882ef076a4350\n",
            "  Stored in directory: /root/.cache/pip/wheels/82/32/c7/fd35d0d1b840a6c7cbd4343f808d10d0f7b87d271a4dbe796f\n",
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.16.0-py3-none-any.whl size=4655 sha256=8512a03595fd390616f40b8b9ae9e54d456999a122fa83895042491a1fd06836\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/3a/4b/21db23c0cc56c4b219616e181f258eb7c57d36cc5d056fae9a\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-multi-head: filename=keras_multi_head-0.29.0-py3-none-any.whl size=14977 sha256=f1c4f00190d23dbd05419549cf95e5565c1efb069d257dc79a4d47ec82b74378\n",
            "  Stored in directory: /root/.cache/pip/wheels/cb/23/4b/06d7ae21714f70fcc25b48f972cc8e5e7f4b6b764a038b509d\n",
            "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.13.0-py3-none-any.whl size=6947 sha256=dfbc58b8fef2dd75bec6e6f69119da6b6bbf4fb695c8f656ff4639253026e7a9\n",
            "  Stored in directory: /root/.cache/pip/wheels/78/07/1b/b1ca47b6ac338554b75c8f52c54e6a2bfbe1b07d79579979a4\n",
            "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.8.0-py3-none-any.whl size=4970 sha256=895276d6f93c72f88b3dce0578a2f9197c1361a99c0b166df2e6801493900c8d\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/6a/04/d1706a53b23b2cb5f9a0a76269bf87925daa1bca09eac01b21\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.51.0-py3-none-any.whl size=18895 sha256=4b3dcfa3d91e94c455c2015a9dc620abdb52e8b2670e9901ac2e9393d50446e0\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/f7/24/607b483144fb9c47b4ba2c5fba6b68e54aeee2d5bf6c05302e\n",
            "  Building wheel for cchardet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cchardet: filename=cchardet-2.1.7-cp310-cp310-linux_x86_64.whl size=249348 sha256=11808850584cd9e6399fc5496e936b5b624dd7ff2980bf41420b69bbd839e7db\n",
            "  Stored in directory: /root/.cache/pip/wheels/ee/e0/ab/e01326f15c59438d080b1496dbab8091e952ec72f35e3c437e\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=00a0c59660a36477d870fc4b4ef6a9b0f33b10c3d2f59d13fa347e4cd9d49d1a\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "  Building wheel for tika (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tika: filename=tika-2.6.0-py3-none-any.whl size=32622 sha256=55fbbce7f849f9859ce21b667ffc83abe57108dcad9e0b167cd6ef264e278caf\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/71/c7/b757709531121b1700cffda5b6b0d4aad095fb507ec84316d0\n",
            "Successfully built ktrain keras_bert keras-transformer keras-embed-sim keras-layer-normalization keras-multi-head keras-pos-embd keras-position-wise-feed-forward keras-self-attention cchardet langdetect tika\n",
            "Installing collected packages: whoosh, tokenizers, sentencepiece, safetensors, cchardet, syntok, langdetect, keras-self-attention, keras-position-wise-feed-forward, keras-pos-embd, keras-layer-normalization, keras-embed-sim, tika, keras-multi-head, huggingface-hub, transformers, keras-transformer, keras_bert, ktrain\n",
            "Successfully installed cchardet-2.1.7 huggingface-hub-0.16.4 keras-embed-sim-0.10.0 keras-layer-normalization-0.16.0 keras-multi-head-0.29.0 keras-pos-embd-0.13.0 keras-position-wise-feed-forward-0.8.0 keras-self-attention-0.51.0 keras-transformer-0.40.0 keras_bert-0.89.0 ktrain-0.37.6 langdetect-1.0.9 safetensors-0.3.2 sentencepiece-0.1.99 syntok-1.4.4 tika-2.6.0 tokenizers-0.13.3 transformers-4.31.0 whoosh-2.7.4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!pip install Keras-Preprocessing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import tensorflow as tf\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import reuters\n",
        "from nltk.corpus import brown\n",
        "from nltk.corpus import gutenberg\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import SnowballStemmer\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import pickle\n",
        "import joblib\n",
        "from collections import Counter\n",
        "from textblob import Word\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier, VotingClassifier\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, f1_score, recall_score\n",
        "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.layers import Activation, Dense, Embedding, LSTM, SpatialDropout1D, Dropout, Flatten, GRU, Conv1D, MaxPooling1D, Bidirectional\n",
        "from wordcloud import WordCloud,ImageColorGenerator\n",
        "from PIL import Image\n",
        "import urllib\n",
        "import requests\n",
        "import re\n",
        "!pip install ktrain\n",
        "import ktrain\n",
        "from ktrain import text\n",
        "sns.set()\n",
        "%matplotlib inline\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('brown')\n",
        "nltk.download(\"reuters\")\n",
        "nltk.download('words')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4KAy8JyUQAw",
        "outputId": "b224c963-04a9-45ba-c53a-2929530f1b8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NHRzQRlsC_9o"
      },
      "outputs": [],
      "source": [
        "sentence_test= pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Shared Tasl/Copy of Test_data_with_labels - test_data.csv')\n",
        "sentence_train= pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Shared Tasl/train_data (1).csv')\n",
        "sentence_val= pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Shared Tasl/dev_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IkkCOUNq075",
        "outputId": "48a4495b-4ab6-4dac-a864-63a96eaedb51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#examples in training set:7201\n",
            "#examples in validation set:3245\n",
            "#examples in test set:499\n"
          ]
        }
      ],
      "source": [
        "print(f\"#examples in training set:{ sentence_train.shape[0]}\\n#examples in validation set:{ sentence_val.shape[0]}\\n#examples in test set:{ sentence_test.shape[0]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_test"
      ],
      "metadata": {
        "id": "C9t2o5KfF_AS",
        "outputId": "2a48b2d7-c711-4601-c562-d630e11b5143",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             Pid                                               text  \\\n",
              "0      test_id_1  This is me. Don鈥檛 get me wrong, it鈥檚 better th...   \n",
              "1      test_id_2  I hate that people don鈥檛 understand that i don...   \n",
              "2      test_id_3  But here I am, 24 years old man and doing exac...   \n",
              "3      test_id_4  I鈥檓 trapped inside. Does anyone else get that ...   \n",
              "4      test_id_5  I read a lot of posts on here of people strugg...   \n",
              "..           ...                                                ...   \n",
              "494  test_id_495  I'm 14\\nmy mom doesn't take my mental health s...   \n",
              "495  test_id_496  I was quite shocked at their reactions. I sort...   \n",
              "496  test_id_497  Lying on my bed..... fantasising another life ...   \n",
              "497  test_id_498  I was bullied in elementary school, and I alwa...   \n",
              "498  test_id_499  I can't go on. Im after finishing a 26oz bottl...   \n",
              "\n",
              "             labels  \n",
              "0          moderate  \n",
              "1            severe  \n",
              "2          moderate  \n",
              "3          moderate  \n",
              "4          moderate  \n",
              "..              ...  \n",
              "494          severe  \n",
              "495        moderate  \n",
              "496  not depression  \n",
              "497        moderate  \n",
              "498          severe  \n",
              "\n",
              "[499 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-9e45edce-fb6a-4b76-a4df-8b1976ce1556\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pid</th>\n",
              "      <th>text</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>test_id_1</td>\n",
              "      <td>This is me. Don鈥檛 get me wrong, it鈥檚 better th...</td>\n",
              "      <td>moderate</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>test_id_2</td>\n",
              "      <td>I hate that people don鈥檛 understand that i don...</td>\n",
              "      <td>severe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>test_id_3</td>\n",
              "      <td>But here I am, 24 years old man and doing exac...</td>\n",
              "      <td>moderate</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>test_id_4</td>\n",
              "      <td>I鈥檓 trapped inside. Does anyone else get that ...</td>\n",
              "      <td>moderate</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>test_id_5</td>\n",
              "      <td>I read a lot of posts on here of people strugg...</td>\n",
              "      <td>moderate</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>494</th>\n",
              "      <td>test_id_495</td>\n",
              "      <td>I'm 14\\nmy mom doesn't take my mental health s...</td>\n",
              "      <td>severe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>495</th>\n",
              "      <td>test_id_496</td>\n",
              "      <td>I was quite shocked at their reactions. I sort...</td>\n",
              "      <td>moderate</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>496</th>\n",
              "      <td>test_id_497</td>\n",
              "      <td>Lying on my bed..... fantasising another life ...</td>\n",
              "      <td>not depression</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>497</th>\n",
              "      <td>test_id_498</td>\n",
              "      <td>I was bullied in elementary school, and I alwa...</td>\n",
              "      <td>moderate</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>498</th>\n",
              "      <td>test_id_499</td>\n",
              "      <td>I can't go on. Im after finishing a 26oz bottl...</td>\n",
              "      <td>severe</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>499 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9e45edce-fb6a-4b76-a4df-8b1976ce1556')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-dca8321d-17cc-4cf6-a25b-85c17ab21824\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-dca8321d-17cc-4cf6-a25b-85c17ab21824')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-dca8321d-17cc-4cf6-a25b-85c17ab21824 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9e45edce-fb6a-4b76-a4df-8b1976ce1556 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9e45edce-fb6a-4b76-a4df-8b1976ce1556');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "O15PAnnfq1B2"
      },
      "outputs": [],
      "source": [
        "train_text = sentence_train.text.tolist()\n",
        "train_labels = sentence_train.labels.tolist()\n",
        "\n",
        "val_text = sentence_val.text.tolist()\n",
        "val_labels = sentence_val.labels.tolist()\n",
        "\n",
        "test_text = sentence_test.text.tolist()\n",
        "test_labels = sentence_test.labels.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "K5adCQ66q1Dl"
      },
      "outputs": [],
      "source": [
        "number_of_classes =3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkLohoU_q1HY",
        "outputId": "458a50fe-a2c4-4031-8d2c-8b0e5e4e99f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1.6 s, sys: 6.6 ms, total: 1.61 s\n",
            "Wall time: 1.67 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "max_words = 1000\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(num_words = max_words, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\t\\n-',\n",
        "                      split=' ', char_level=False, oov_token=None, document_count=0)\n",
        "\n",
        "tokenizer.fit_on_texts(train_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWqlVsitr2xY",
        "outputId": "15887159-efbd-4a12-d2fc-43ee8113ca82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21626\n",
            "CPU times: user 154 µs, sys: 0 ns, total: 154 µs\n",
            "Wall time: 121 µs\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "word_counts = tokenizer.word_counts\n",
        "word_docs = tokenizer.word_docs\n",
        "word_index = tokenizer.word_index\n",
        "document_count = tokenizer.document_count\n",
        "\n",
        "print(len(word_counts))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7H8QOi5sM_K"
      },
      "source": [
        "# Frequency Distribution of Data Length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "AAhtF4Khr3DO"
      },
      "outputs": [],
      "source": [
        "Length_frequency = {}\n",
        "\n",
        "for i in range(len(train_text)):\n",
        "     index = len(train_text[i])\n",
        "     Length_frequency[index] = Length_frequency.get(index, 0)+1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "id": "czEeNUbZsbB8",
        "outputId": "35f5b65f-a8ce-4114-eec6-ad9ae4a60d3a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Length-Frequency Distribution')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "import seaborn as sns\n",
        "\n",
        "plt.bar(list(Length_frequency.keys()), Length_frequency.values(), color ='r')\n",
        "plt.xlim(1, 1024)\n",
        "\n",
        "plt.xlabel('Lenght of the Texts')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Length-Frequency Distribution')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKj_F8KKsbD1",
        "outputId": "144b2e40-25aa-49bc-fcf9-2a436a8133c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2.02 s, sys: 13.3 ms, total: 2.03 s\n",
            "Wall time: 3.41 s\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%time\n",
        "# Convert string into list of integer indices\n",
        "train_sequences = tokenizer.texts_to_sequences(train_text)\n",
        "val_sequences = tokenizer.texts_to_sequences(val_text)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INqFIylQsbIH",
        "outputId": "8fce6106-f8d3-4dc9-9584-2de9a99c07ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 21626 unique tokens.\n"
          ]
        }
      ],
      "source": [
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "# print(dataset['cleaned'][3])\n",
        "# print(sequences[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydp-7u25sbJt",
        "outputId": "6c149c01-a2e3-46a4-d775-d0df5c4d047a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum Text length 4169\n",
            "[  5 161  88  24 324  39   5 829 136   6   3  40   1 170 380  46   6 222\n",
            " 315 124  97  24  26 743  18  17 162 110  24  26 615  14   6 179  13   6\n",
            " 222 315   7  15  29   1  32   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0]\n",
            "CPU times: user 158 ms, sys: 6.68 ms, total: 164 ms\n",
            "Wall time: 190 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "## Before padding length of different training examples\n",
        "mx = len(train_sequences[0])\n",
        "for x in train_sequences:\n",
        "  mx =max(mx, len(x))\n",
        "print(\"Maximum Text length\",mx)\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "##We can also determine maxlen by plotting the frequency distribution of the lengths\n",
        "train_pad = pad_sequences(train_sequences, value=0.0, padding='post', maxlen= 200)\n",
        "## We get the maxlen value from the Length frequency distribution\n",
        "val_pad = pad_sequences(val_sequences, value=0.0, padding='post', maxlen= 200)\n",
        "test_pad = pad_sequences(test_sequences, value=0.0, padding='post', maxlen= 200)\n",
        "\n",
        "print(train_pad[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chcBzSO_tDwM",
        "outputId": "abfe543f-6a72-4584-cebd-15e317dccbe1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the train set:  (7201, 200)\n"
          ]
        }
      ],
      "source": [
        "print(\"Shape of the train set: \", train_pad.shape)\n",
        "\n",
        "# for i in range(2):\n",
        "#     print(dataset['cleaned'][i],\"\\n\",corpus[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daC3UEQRv6Zm"
      },
      "source": [
        "# CNN with Keras Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "_XSs1gmRtDyA"
      },
      "outputs": [],
      "source": [
        "# Keras Functional API\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.layers import Input, Dense, Activation, Dropout,Flatten,Embedding\n",
        "from keras.layers import Conv1D,MaxPooling1D,GlobalAveragePooling1D\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam,SGD,Nadam,RMSprop\n",
        "from tensorflow.keras.models import load_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "RW0O6Y2LLrdh"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Convert train_labels and val_labels to strings\n",
        "train_labels = [str(label) for label in train_labels]\n",
        "val_labels = [str(label) for label in val_labels]\n",
        "test_labels = [str(label) for label in test_labels]\n",
        "\n",
        "# Create an instance of LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit label encoder on the combined labels\n",
        "combined_labels = train_labels + val_labels+test_labels\n",
        "label_encoder.fit(combined_labels)\n",
        "\n",
        "# Transform the training labels\n",
        "train_labels_encoded = label_encoder.transform(train_labels)\n",
        "\n",
        "# Transform the validation labels\n",
        "val_labels_encoded = label_encoder.transform(val_labels)\n",
        "test_labels_encoded=label_encoder.transform(test_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "7uDib6xlMMh_"
      },
      "outputs": [],
      "source": [
        "\n",
        "''' Callbacks with Checkpoint'''\n",
        "\n",
        "accuracy_threshold1 = 0.99\n",
        "\n",
        "class myCallback(keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "      if(logs.get('accuracy')>accuracy_threshold1):\n",
        "        print(\"\\nReached %2.2f%% accuracy so we will stop trianing\" % (accuracy_threshold1*100))\n",
        "        self.model.stop_training = True\n",
        "\n",
        "acc_callback1 = myCallback()\n",
        "  # Saved the Best Model\n",
        "filepath = \"mymodel5.h5\"\n",
        "checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_accuracy', verbose=2, save_best_only=True,\n",
        "                                             save_weights_only=False, mode='max')\n",
        "  # callback list\n",
        "callback_list1= [acc_callback1, checkpoint]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "HfBKgECeMSBJ"
      },
      "outputs": [],
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "train_labels_encoded = to_categorical(train_labels_encoded, num_classes=number_of_classes)\n",
        "val_labels_encoded = to_categorical(val_labels_encoded, num_classes=number_of_classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJnoglYSv_JN",
        "outputId": "75d46109-03f3-49e8-ea22-10b07dbdb6c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 200)]             0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 200, 64)           64000     \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 199, 32)           4128      \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 99, 32)           0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 3168)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 3)                 9507      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 77,635\n",
            "Trainable params: 77,635\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "keras.backend.clear_session()\n",
        "\n",
        "max_length = 200\n",
        "embedding_dim = 64\n",
        "number_of_classes = 3\n",
        "# define CNN model\n",
        "\n",
        "def CNN():\n",
        "\n",
        "  input = Input(shape=(max_length,))\n",
        "  embedding = Embedding(max_words, embedding_dim, input_length = 200)(input)\n",
        "  conv1 = Conv1D(32,2,activation='relu')(embedding)\n",
        "  pool1 = MaxPooling1D(2)(conv1)\n",
        "  flat = Flatten()(pool1)\n",
        "  output_layer = Dense(3, activation='softmax')(flat)\n",
        "  model = Model(inputs=input, outputs=output_layer)\n",
        "\n",
        "  return model\n",
        "\n",
        "# call the model\n",
        "cnn_model = CNN()\n",
        "\n",
        "cnn_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIUKOfbUMdxM",
        "outputId": "591c1a10-bd0f-4653-a27f-32e0c8e0a05f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 200)]             0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 200, 64)           64000     \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 199, 32)           4128      \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 99, 32)           0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 3168)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 3)                 9507      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 77,635\n",
            "Trainable params: 77,635\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "print(cnn_model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cp8otfu-v_K6",
        "outputId": "f50d077f-7982-44ab-f8ba-86f84836098a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "224/226 [============================>.] - ETA: 0s - loss: 0.8945 - accuracy: 0.5760\n",
            "Epoch 1: val_accuracy improved from -inf to 0.62188, saving model to mymodel5.h5\n",
            "226/226 [==============================] - 5s 19ms/step - loss: 0.8947 - accuracy: 0.5756 - val_loss: 0.8205 - val_accuracy: 0.6219\n",
            "Epoch 2/10\n",
            "224/226 [============================>.] - ETA: 0s - loss: 0.8226 - accuracy: 0.6145\n",
            "Epoch 2: val_accuracy improved from 0.62188 to 0.65701, saving model to mymodel5.h5\n",
            "226/226 [==============================] - 5s 24ms/step - loss: 0.8229 - accuracy: 0.6144 - val_loss: 0.7675 - val_accuracy: 0.6570\n",
            "Epoch 3/10\n",
            "225/226 [============================>.] - ETA: 0s - loss: 0.7097 - accuracy: 0.6812\n",
            "Epoch 3: val_accuracy did not improve from 0.65701\n",
            "226/226 [==============================] - 3s 14ms/step - loss: 0.7097 - accuracy: 0.6813 - val_loss: 0.8313 - val_accuracy: 0.6055\n",
            "Epoch 4/10\n",
            "224/226 [============================>.] - ETA: 0s - loss: 0.5554 - accuracy: 0.7702\n",
            "Epoch 4: val_accuracy did not improve from 0.65701\n",
            "226/226 [==============================] - 3s 15ms/step - loss: 0.5553 - accuracy: 0.7703 - val_loss: 0.9055 - val_accuracy: 0.5624\n",
            "Epoch 5/10\n",
            "224/226 [============================>.] - ETA: 0s - loss: 0.4139 - accuracy: 0.8537\n",
            "Epoch 5: val_accuracy did not improve from 0.65701\n",
            "226/226 [==============================] - 4s 16ms/step - loss: 0.4138 - accuracy: 0.8539 - val_loss: 0.9315 - val_accuracy: 0.5720\n",
            "Epoch 6/10\n",
            "224/226 [============================>.] - ETA: 0s - loss: 0.3010 - accuracy: 0.9009\n",
            "Epoch 6: val_accuracy did not improve from 0.65701\n",
            "226/226 [==============================] - 5s 24ms/step - loss: 0.3006 - accuracy: 0.9011 - val_loss: 1.0570 - val_accuracy: 0.5578\n",
            "Epoch 7/10\n",
            "222/226 [============================>.] - ETA: 0s - loss: 0.2320 - accuracy: 0.9307\n",
            "Epoch 7: val_accuracy did not improve from 0.65701\n",
            "226/226 [==============================] - 4s 16ms/step - loss: 0.2317 - accuracy: 0.9311 - val_loss: 1.1765 - val_accuracy: 0.5319\n",
            "Epoch 8/10\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.1844 - accuracy: 0.9513\n",
            "Epoch 8: val_accuracy did not improve from 0.65701\n",
            "226/226 [==============================] - 3s 15ms/step - loss: 0.1844 - accuracy: 0.9513 - val_loss: 1.2803 - val_accuracy: 0.5368\n",
            "Epoch 9/10\n",
            "223/226 [============================>.] - ETA: 0s - loss: 0.1574 - accuracy: 0.9591\n",
            "Epoch 9: val_accuracy did not improve from 0.65701\n",
            "226/226 [==============================] - 3s 14ms/step - loss: 0.1576 - accuracy: 0.9590 - val_loss: 1.3322 - val_accuracy: 0.5760\n",
            "Epoch 10/10\n",
            "225/226 [============================>.] - ETA: 0s - loss: 0.1357 - accuracy: 0.9657\n",
            "Epoch 10: val_accuracy did not improve from 0.65701\n",
            "226/226 [==============================] - 5s 23ms/step - loss: 0.1356 - accuracy: 0.9657 - val_loss: 1.4335 - val_accuracy: 0.5516\n"
          ]
        }
      ],
      "source": [
        "history5 = cnn_model.fit(train_pad, train_labels_encoded, epochs=10, batch_size=32, verbose=1, validation_data=(val_pad, val_labels_encoded), callbacks=callback_list1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "N2_ygIuc2Xe-"
      },
      "outputs": [],
      "source": [
        "model112 = load_model(\"mymodel5.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xroQAy1oNDry",
        "outputId": "dc982467-d9ea-4342-ec9d-6f851d8859e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16/16 [==============================] - 0s 4ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,\n",
              "       0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,\n",
              "       0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,\n",
              "       1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,\n",
              "       0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
              "       0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "y_pred112 = np.argmax(model112.predict(test_pad), axis=-1)\n",
        "y_pred112"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lok2fT5LNL8q",
        "outputId": "a9a10b84-c6ae-49d4-ccb3-b1bb078b841c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.76      0.64       275\n",
            "           1       0.39      0.37      0.38       135\n",
            "           2       0.00      0.00      0.00        89\n",
            "\n",
            "    accuracy                           0.52       499\n",
            "   macro avg       0.32      0.38      0.34       499\n",
            "weighted avg       0.41      0.52      0.46       499\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(test_labels_encoded,y_pred112))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUIonxL3BXqg"
      },
      "source": [
        "# CNN+BiLSTM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bR18eOcPF9z",
        "outputId": "0b8908f8-d8aa-4a75-95f6-d3d37fb8c929"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 200, 64)           64000     \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 199, 32)           4128      \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 99, 32)           0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 99, 64)           16640     \n",
            " l)                                                              \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 6336)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                405568    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 3)                 195       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 490,531\n",
            "Trainable params: 490,531\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "from keras.layers import Embedding, SpatialDropout1D, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dropout, Flatten, Dense\n",
        "from keras.models import Sequential\n",
        "\n",
        "keras.backend.clear_session()\n",
        "\n",
        "max_length = 200\n",
        "embedding_dim = 64\n",
        "number_of_classes = 3\n",
        "\n",
        "# Define CNN + LSTM model\n",
        "model_cl1 = Sequential()\n",
        "model_cl1.add(Embedding(max_words, embedding_dim, input_length=max_length, trainable=True))\n",
        "#model_cl.add(SpatialDropout1D(0.8))\n",
        "#model_cl.add(Conv1D(filters=64, kernel_size=6, padding='same', activation='relu'))\n",
        "#model_cl.add(MaxPooling1D(pool_size=2))\n",
        "model_cl1.add(Conv1D(filters=32, kernel_size=2, activation='relu'))\n",
        "model_cl1.add(MaxPooling1D(pool_size=2))\n",
        "model_cl1.add(Bidirectional(LSTM(32, dropout=0.5, recurrent_dropout=0.5, return_sequences=True)))\n",
        "#model_cl.add(Dropout(0.5))\n",
        "#model_cl.add(Bidirectional(LSTM(400, dropout=0.5, recurrent_dropout=0.5)))\n",
        "#model_cl.add(Dropout(0.5))\n",
        "model_cl1.add(Flatten())\n",
        "model_cl1.add(Dense(64, activation='relu'))\n",
        "#model_cl.add(Dropout(0.5))\n",
        "model_cl1.add(Dense(number_of_classes, activation='softmax'))\n",
        "model_cl1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "print(model_cl1.summary())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_cl1.compile(optimizer = 'adam',\n",
        "                      loss = 'sparse_categorical_crossentropy',\n",
        "                      metrics =['accuracy'])\n"
      ],
      "metadata": {
        "id": "PVkTibn8IpZw"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "3-DMDqXjPGN3"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Convert train_labels and val_labels to strings\n",
        "train_labels = [str(label) for label in train_labels]\n",
        "val_labels = [str(label) for label in val_labels]\n",
        "\n",
        "# Create an instance of LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit label encoder on the combined labels\n",
        "combined_labels = train_labels + val_labels\n",
        "label_encoder.fit(combined_labels)\n",
        "\n",
        "# Transform the training labels\n",
        "train_labels_encoded = label_encoder.transform(train_labels)\n",
        "\n",
        "# Transform the validation labels\n",
        "val_labels_encoded = label_encoder.transform(val_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "5hhFbrNqPzJ1"
      },
      "outputs": [],
      "source": [
        "\n",
        "''' Callbacks with Checkpoint'''\n",
        "\n",
        "accuracy_threshold1 = 0.99\n",
        "\n",
        "class myCallback1(keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "      if(logs.get('accuracy')>accuracy_threshold1):\n",
        "        print(\"\\nReached %2.2f%% accuracy so we will stop trianing\" % (accuracy_threshold1*100))\n",
        "        self.model.stop_training = True\n",
        "\n",
        "acc_callback2 = myCallback1()\n",
        "  # Saved the Best Model\n",
        "filepath = \"mymodel_kell.h5\"\n",
        "checkpoint2 = keras.callbacks.ModelCheckpoint(filepath, monitor='val_accuracy', verbose=2, save_best_only=True,\n",
        "                                             save_weights_only=False, mode='max')\n",
        "  # callback list\n",
        "callback_list1= [acc_callback2, checkpoint2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rV8D6lTqPzLm",
        "outputId": "b8e3ce38-6d7d-4518-dd6b-dfd42be1f502"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.8987 - accuracy: 0.5789\n",
            "Epoch 1: val_accuracy improved from -inf to 0.65948, saving model to mymodel_kell.h5\n",
            "226/226 [==============================] - 72s 287ms/step - loss: 0.8987 - accuracy: 0.5789 - val_loss: 0.7853 - val_accuracy: 0.6595\n",
            "Epoch 2/10\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.8251 - accuracy: 0.6142\n",
            "Epoch 2: val_accuracy did not improve from 0.65948\n",
            "226/226 [==============================] - 71s 314ms/step - loss: 0.8251 - accuracy: 0.6142 - val_loss: 0.9486 - val_accuracy: 0.5461\n",
            "Epoch 3/10\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.7558 - accuracy: 0.6484\n",
            "Epoch 3: val_accuracy did not improve from 0.65948\n",
            "226/226 [==============================] - 61s 270ms/step - loss: 0.7558 - accuracy: 0.6484 - val_loss: 0.9277 - val_accuracy: 0.4915\n",
            "Epoch 4/10\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.6829 - accuracy: 0.6841\n",
            "Epoch 4: val_accuracy did not improve from 0.65948\n",
            "226/226 [==============================] - 61s 270ms/step - loss: 0.6829 - accuracy: 0.6841 - val_loss: 0.8191 - val_accuracy: 0.6136\n",
            "Epoch 5/10\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.6208 - accuracy: 0.7188\n",
            "Epoch 5: val_accuracy did not improve from 0.65948\n",
            "226/226 [==============================] - 64s 286ms/step - loss: 0.6208 - accuracy: 0.7188 - val_loss: 0.8831 - val_accuracy: 0.5988\n",
            "Epoch 6/10\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.5776 - accuracy: 0.7492\n",
            "Epoch 6: val_accuracy did not improve from 0.65948\n",
            "226/226 [==============================] - 78s 347ms/step - loss: 0.5776 - accuracy: 0.7492 - val_loss: 0.8872 - val_accuracy: 0.5985\n",
            "Epoch 7/10\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.5023 - accuracy: 0.7831\n",
            "Epoch 7: val_accuracy did not improve from 0.65948\n",
            "226/226 [==============================] - 62s 273ms/step - loss: 0.5023 - accuracy: 0.7831 - val_loss: 0.9443 - val_accuracy: 0.6049\n",
            "Epoch 8/10\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.4593 - accuracy: 0.8006\n",
            "Epoch 8: val_accuracy did not improve from 0.65948\n",
            "226/226 [==============================] - 62s 273ms/step - loss: 0.4593 - accuracy: 0.8006 - val_loss: 1.2238 - val_accuracy: 0.5088\n",
            "Epoch 9/10\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.4425 - accuracy: 0.8134\n",
            "Epoch 9: val_accuracy did not improve from 0.65948\n",
            "226/226 [==============================] - 64s 282ms/step - loss: 0.4425 - accuracy: 0.8134 - val_loss: 1.0270 - val_accuracy: 0.5951\n",
            "Epoch 10/10\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.3792 - accuracy: 0.8382\n",
            "Epoch 10: val_accuracy did not improve from 0.65948\n",
            "226/226 [==============================] - 62s 272ms/step - loss: 0.3792 - accuracy: 0.8382 - val_loss: 1.0916 - val_accuracy: 0.5855\n"
          ]
        }
      ],
      "source": [
        "history1= model_cl1.fit(train_pad, train_labels_encoded, epochs=10, batch_size=32, verbose=1, validation_data=(val_pad, val_labels_encoded), callbacks=callback_list1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "jpX3w4p8Qase"
      },
      "outputs": [],
      "source": [
        "model6 = load_model(\"mymodel_kell.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9PdokGCQa5X",
        "outputId": "c68ce0a0-5842-4caf-cad5-3b7aba3f5fe2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16/16 [==============================] - 1s 29ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,\n",
              "       0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,\n",
              "       0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,\n",
              "       1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,\n",
              "       0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
              "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "y_pred6 = np.argmax(model6.predict(test_pad), axis=-1)\n",
        "y_pred6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GD78rZEQa7j",
        "outputId": "e1a4fa71-accf-48b3-9ac1-acb20174f0bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.81      0.66       275\n",
            "           1       0.38      0.29      0.33       135\n",
            "           2       0.00      0.00      0.00        89\n",
            "\n",
            "    accuracy                           0.53       499\n",
            "   macro avg       0.31      0.37      0.33       499\n",
            "weighted avg       0.41      0.53      0.45       499\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(test_labels_encoded,y_pred6))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfy-8_LLBiLY"
      },
      "source": [
        "# BiLSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "VavaJ0sUenFO"
      },
      "outputs": [],
      "source": [
        "\n",
        "''' Callbacks with Checkpoint'''\n",
        "\n",
        "accuracy_threshold1 = 0.99\n",
        "\n",
        "class myCallback(keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "      if(logs.get('accuracy')>accuracy_threshold1):\n",
        "        print(\"\\nReached %2.2f%% accuracy so we will stop trianing\" % (accuracy_threshold1*100))\n",
        "        self.model.stop_training = True\n",
        "\n",
        "acc_callback0 = myCallback()\n",
        "  # Saved the Best Model\n",
        "filepath = \"mymodel000.h5\"\n",
        "checkpoint0 = keras.callbacks.ModelCheckpoint(filepath, monitor='val_accuracy', verbose=2, save_best_only=True,\n",
        "                                             save_weights_only=False, mode='max')\n",
        "  # callback list\n",
        "callback_list0= [acc_callback0, checkpoint0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xf43VMsIPzQK",
        "outputId": "3d354544-d99e-43cf-fbe9-c71b66e17bcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 200)]             0         \n",
            "                                                                 \n",
            " embedding_1 (Embedding)     (None, 200, 64)           64000     \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, 64)               24832     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 3)                 195       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 89,027\n",
            "Trainable params: 89,027\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D, Bidirectional, LSTM, GRU\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Embedding\n",
        "import keras.callbacks\n",
        "\n",
        "max_length = 200\n",
        "embedding_dim = 64\n",
        "number_of_classes = 3\n",
        "\n",
        "# Define BiLSTM model\n",
        "def lstm():\n",
        "    bi_text_inputs = Input(shape=(max_length,))\n",
        "    bi_embedding_layer = Embedding(max_words, embedding_dim, trainable=True, input_length=max_length)(bi_text_inputs)\n",
        "    LSTM_Layer_1 = Bidirectional(LSTM(32))(bi_embedding_layer)\n",
        "    bi_dense_layer_1 = Dense(number_of_classes, activation='softmax')(LSTM_Layer_1)\n",
        "    bilstm_model = Model(inputs=bi_text_inputs, outputs=bi_dense_layer_1)\n",
        "    return bilstm_model\n",
        "\n",
        "# Create the BiLSTM model with the embedding matrix\n",
        "lstm_model = lstm()\n",
        "\n",
        "# Print the model summary\n",
        "lstm_model.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "pgkj8-6yPzSP"
      },
      "outputs": [],
      "source": [
        "lstm_model.compile(optimizer = 'adam',\n",
        "                      loss = 'sparse_categorical_crossentropy',\n",
        "                      metrics =['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2zpFOj_eC-p",
        "outputId": "1a170c60-7bce-448a-e99f-eae3ef7d575f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.9085 - accuracy: 0.5663\n",
            "Epoch 1: val_accuracy improved from -inf to 0.59353, saving model to mymodel000.h5\n",
            "226/226 [==============================] - 57s 226ms/step - loss: 0.9085 - accuracy: 0.5663 - val_loss: 0.8509 - val_accuracy: 0.5935\n",
            "Epoch 2/10\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.8222 - accuracy: 0.6176\n",
            "Epoch 2: val_accuracy did not improve from 0.59353\n",
            "226/226 [==============================] - 49s 216ms/step - loss: 0.8222 - accuracy: 0.6176 - val_loss: 0.8388 - val_accuracy: 0.5852\n",
            "Epoch 3/10\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.7569 - accuracy: 0.6569\n",
            "Epoch 3: val_accuracy improved from 0.59353 to 0.61757, saving model to mymodel000.h5\n",
            "226/226 [==============================] - 46s 203ms/step - loss: 0.7569 - accuracy: 0.6569 - val_loss: 0.7906 - val_accuracy: 0.6176\n",
            "Epoch 4/10\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.7117 - accuracy: 0.6830\n",
            "Epoch 4: val_accuracy did not improve from 0.61757\n",
            "226/226 [==============================] - 44s 193ms/step - loss: 0.7117 - accuracy: 0.6830 - val_loss: 0.8284 - val_accuracy: 0.6117\n",
            "Epoch 5/10\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.6728 - accuracy: 0.7055\n",
            "Epoch 5: val_accuracy did not improve from 0.61757\n",
            "226/226 [==============================] - 46s 206ms/step - loss: 0.6728 - accuracy: 0.7055 - val_loss: 0.8640 - val_accuracy: 0.5889\n",
            "Epoch 6/10\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.6521 - accuracy: 0.7160\n",
            "Epoch 6: val_accuracy did not improve from 0.61757\n",
            "226/226 [==============================] - 47s 209ms/step - loss: 0.6521 - accuracy: 0.7160 - val_loss: 0.8734 - val_accuracy: 0.5914\n",
            "Epoch 7/10\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.6122 - accuracy: 0.7343\n",
            "Epoch 7: val_accuracy did not improve from 0.61757\n",
            "226/226 [==============================] - 50s 221ms/step - loss: 0.6122 - accuracy: 0.7343 - val_loss: 0.8802 - val_accuracy: 0.5895\n",
            "Epoch 8/10\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.5809 - accuracy: 0.7486\n",
            "Epoch 8: val_accuracy did not improve from 0.61757\n",
            "226/226 [==============================] - 45s 199ms/step - loss: 0.5809 - accuracy: 0.7486 - val_loss: 0.9723 - val_accuracy: 0.5661\n",
            "Epoch 9/10\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.5548 - accuracy: 0.7617\n",
            "Epoch 9: val_accuracy did not improve from 0.61757\n",
            "226/226 [==============================] - 44s 195ms/step - loss: 0.5548 - accuracy: 0.7617 - val_loss: 0.9370 - val_accuracy: 0.6117\n",
            "Epoch 10/10\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.5202 - accuracy: 0.7827\n",
            "Epoch 10: val_accuracy did not improve from 0.61757\n",
            "226/226 [==============================] - 46s 204ms/step - loss: 0.5202 - accuracy: 0.7827 - val_loss: 1.0473 - val_accuracy: 0.5646\n",
            "CPU times: user 11min 30s, sys: 3.59 s, total: 11min 33s\n",
            "Wall time: 8min 27s\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%time\n",
        "history4 = lstm_model.fit(train_pad,\n",
        "                              train_labels_encoded,\n",
        "                              epochs = 10,\n",
        "                              batch_size = 32,\n",
        "                              verbose = 1,\n",
        "                              validation_data=(val_pad, val_labels_encoded),\n",
        "                              callbacks = callback_list0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "k8dJuhE7eKSs"
      },
      "outputs": [],
      "source": [
        "model4 = load_model(\"mymodel000.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grZmdCcYeKUy",
        "outputId": "e7c9e45a-1eae-47e1-e804-baf2809e468d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16/16 [==============================] - 2s 57ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 2,\n",
              "       1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 2, 0, 1, 1, 1,\n",
              "       2, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 2, 1, 1, 1, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,\n",
              "       0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 1, 0, 1, 0, 0,\n",
              "       0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,\n",
              "       1, 0, 0, 1, 2, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
              "       0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,\n",
              "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 2, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
              "       0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n",
              "       0, 0, 0, 1, 1, 0, 1, 2, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,\n",
              "       1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 2,\n",
              "       0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,\n",
              "       1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,\n",
              "       1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,\n",
              "       1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,\n",
              "       0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,\n",
              "       0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,\n",
              "       0, 1, 0, 1, 1, 1, 1, 1, 2, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
              "       0, 1, 2, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "y_pred4 = np.argmax(model4.predict(test_pad), axis=-1)\n",
        "y_pred4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VE6kQFHVfgLD",
        "outputId": "c2bab0d8-d418-4f8c-c753-807c4ed3b11b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.60      0.57       275\n",
            "           1       0.35      0.47      0.40       135\n",
            "           2       0.43      0.07      0.12        89\n",
            "\n",
            "    accuracy                           0.47       499\n",
            "   macro avg       0.44      0.38      0.36       499\n",
            "weighted avg       0.47      0.47      0.44       499\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(test_labels_encoded,y_pred4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpVQo5SJL0zY"
      },
      "source": [
        "CNN with pretrained Glove Word Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pH06jPaLzaT",
        "outputId": "fe48f827-d64a-4f5a-9301-616306d20897"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-15 17:17:40--  https://www.dropbox.com/s/l70hvnely4y0pbm/bn_glove.39M.300d.zip?dl=\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.6.18, 2620:100:601c:18::a27d:612\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.6.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/raw/l70hvnely4y0pbm/bn_glove.39M.300d.zip [following]\n",
            "--2023-08-15 17:17:40--  https://www.dropbox.com/s/raw/l70hvnely4y0pbm/bn_glove.39M.300d.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucb02c4747ccfc1b9540ab8f960b.dl.dropboxusercontent.com/cd/0/inline/CB2yIVu4u3gFQ4d1jUEO-tSTuyqO7J3zAUBHaPTRtZJhdvcIlKoo5i4Cyrcapf1QqMciiHLarTSton68p_All6G98Zp4tz_UH-yoMABaUfpX9hL5VudgkpyVqXHbSolYJZMSUoztxzGmFq2UvMm-ZB2C/file# [following]\n",
            "--2023-08-15 17:17:41--  https://ucb02c4747ccfc1b9540ab8f960b.dl.dropboxusercontent.com/cd/0/inline/CB2yIVu4u3gFQ4d1jUEO-tSTuyqO7J3zAUBHaPTRtZJhdvcIlKoo5i4Cyrcapf1QqMciiHLarTSton68p_All6G98Zp4tz_UH-yoMABaUfpX9hL5VudgkpyVqXHbSolYJZMSUoztxzGmFq2UvMm-ZB2C/file\n",
            "Resolving ucb02c4747ccfc1b9540ab8f960b.dl.dropboxusercontent.com (ucb02c4747ccfc1b9540ab8f960b.dl.dropboxusercontent.com)... 162.125.6.15, 2620:100:601c:15::a27d:60f\n",
            "Connecting to ucb02c4747ccfc1b9540ab8f960b.dl.dropboxusercontent.com (ucb02c4747ccfc1b9540ab8f960b.dl.dropboxusercontent.com)|162.125.6.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/CB1i3KPooI1HwtkUHXTMZStHt5OPZhcrLW09lnpakzhTTgGxOgCmXnP_5_9MkqOVFfom0BdBZU_LLK3B97Ifq-JMEt-Q0w6430ztmbU1Ha3OFDKClxVYPWxw8_8R80D-e22ohwCEyK79f_5zcqbOVRqxAoT1g_qLKVdpUyeYcdfJzh549ziqezn8f0OgNAc3mnnUTHVEqCVj_iG6ANMYQTVXBQP22gA8UtHMH1M8aE0xYxkKCz0oBIT3g422ywyQC42gNax4ijbAn1A7bLVtKIPOKjN4MA-Db0uX_yk_oL9dCiUbqy_Kmd6yCHu5-8M03gE_ezaaahDVvG6-q3vBYmmbKIPVmztodSPLUGo67_-x2vW9rZ4K8vgVhII2FQFErPw/file [following]\n",
            "--2023-08-15 17:17:41--  https://ucb02c4747ccfc1b9540ab8f960b.dl.dropboxusercontent.com/cd/0/inline2/CB1i3KPooI1HwtkUHXTMZStHt5OPZhcrLW09lnpakzhTTgGxOgCmXnP_5_9MkqOVFfom0BdBZU_LLK3B97Ifq-JMEt-Q0w6430ztmbU1Ha3OFDKClxVYPWxw8_8R80D-e22ohwCEyK79f_5zcqbOVRqxAoT1g_qLKVdpUyeYcdfJzh549ziqezn8f0OgNAc3mnnUTHVEqCVj_iG6ANMYQTVXBQP22gA8UtHMH1M8aE0xYxkKCz0oBIT3g422ywyQC42gNax4ijbAn1A7bLVtKIPOKjN4MA-Db0uX_yk_oL9dCiUbqy_Kmd6yCHu5-8M03gE_ezaaahDVvG6-q3vBYmmbKIPVmztodSPLUGo67_-x2vW9rZ4K8vgVhII2FQFErPw/file\n",
            "Reusing existing connection to ucb02c4747ccfc1b9540ab8f960b.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 195374616 (186M) [application/zip]\n",
            "Saving to: ‘bangla-glove-300d.zip’\n",
            "\n",
            "bangla-glove-300d.z 100%[===================>] 186.32M  88.3MB/s    in 2.1s    \n",
            "\n",
            "2023-08-15 17:17:44 (88.3 MB/s) - ‘bangla-glove-300d.zip’ saved [195374616/195374616]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# download pretrained glove vectors\n",
        "!wget -O bangla-glove-300d.zip https://www.dropbox.com/s/l70hvnely4y0pbm/bn_glove.39M.300d.zip?dl="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "73ddXZkvL8in"
      },
      "outputs": [],
      "source": [
        "# extract the zip file and save the files into bangla-digit folder\n",
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile(\"bangla-glove-300d.zip\", 'r')\n",
        "zip_ref.extractall(\"\")\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZufQRBzzMAgK",
        "outputId": "79c83af2-0a28-4bf4-9b3a-8377749cbb35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-18fbf809732b>:2: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
            "  glove2word2vec(glove_input_file='bn_glove.39M.300d.txt', word2vec_output_file=\"gensim_glove_vectors.txt\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(178153, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "glove2word2vec(glove_input_file='bn_glove.39M.300d.txt', word2vec_output_file=\"gensim_glove_vectors.txt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "VwoE2dxtMD3g"
      },
      "outputs": [],
      "source": [
        "from gensim.models import KeyedVectors\n",
        "glove_model = KeyedVectors.load_word2vec_format(\"gensim_glove_vectors.txt\", binary=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbiQrN-MMD9p",
        "outputId": "5fcaacbd-d129-4a66-8629-efb215a35f5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of words not in the vocabulary:  21626\n",
            "Percentage of words not in the vocabulary:  21.626\n"
          ]
        }
      ],
      "source": [
        "#Creating Embeding Matrix\n",
        "max_words = 100000 # Most frequent 100000 words (It should be initialize during tokenization process)\n",
        "embedding_dim = 300 # embedding dimension of the model\n",
        "\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim), dtype = float)\n",
        "\n",
        "c = 0\n",
        "word_to_vec_map = {}\n",
        "for word, i in word_index.items():\n",
        "    if i < max_words:\n",
        "      embedding_vector = word_to_vec_map.get(word)\n",
        "      if embedding_vector is None:\n",
        "          c +=1;\n",
        "          #print(word)\n",
        "      if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "# Words not found in the pretrained model is assigned as zeros\n",
        "print(\"Number of words not in the vocabulary: \", c)\n",
        "print(\"Percentage of words not in the vocabulary: \", (c/max_words)*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "ydvySHF3MEAC"
      },
      "outputs": [],
      "source": [
        "\n",
        "''' Callbacks with Checkpoint'''\n",
        "\n",
        "accuracy_threshold = 0.99\n",
        "\n",
        "class myCallback(keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "      if(logs.get('accuracy')>accuracy_threshold):\n",
        "        print(\"\\nReached %2.2f%% accuracy so we will stop trianing\" % (accuracy_threshold*100))\n",
        "        self.model.stop_training = True\n",
        "\n",
        "acc_callback = myCallback()\n",
        "  # Saved the Best Model\n",
        "filepath = \"mymodel.h5\"\n",
        "checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_accuracy', verbose=2, save_best_only=True,\n",
        "                                             save_weights_only=False, mode='max')\n",
        "  # callback list\n",
        "callback_list = [acc_callback, checkpoint]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J99JjaoHMEDZ",
        "outputId": "e15b263a-fc79-4374-af61-374dd3154829"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 200)]             0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 200, 300)          30000000  \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 199, 32)           19232     \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 99, 32)           0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 3168)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 8)                 25352     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 30,044,584\n",
            "Trainable params: 44,584\n",
            "Non-trainable params: 30,000,000\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "#the Model\n",
        "\n",
        "keras.backend.clear_session()\n",
        "\n",
        "max_length = 200\n",
        "embedding_dim = 300\n",
        "number_of_classes = 8\n",
        "# define CNN model\n",
        "\n",
        "def CNN(embedding_matrix):\n",
        "  input = Input(shape=(max_length,))\n",
        "  embedding = Embedding(max_words, embedding_dim, input_length=200, weights=[embedding_matrix], trainable=False)(input)\n",
        "  conv1 = Conv1D(32, 2, activation='relu')(embedding)\n",
        "  pool1 = MaxPooling1D(2)(conv1)\n",
        "  flat = Flatten()(pool1)\n",
        "  output_layer = Dense(number_of_classes, activation='softmax')(flat)\n",
        "  model = Model(inputs=input, outputs=output_layer)\n",
        "  return model\n",
        "\n",
        "\n",
        "# call the model\n",
        "cnn_model_pretrained = CNN(embedding_matrix)\n",
        "\n",
        "cnn_model_pretrained.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "1hZ-oFClMEFh"
      },
      "outputs": [],
      "source": [
        "cnn_model_pretrained.compile(optimizer = 'adam',\n",
        "                      loss = 'sparse_categorical_crossentropy',\n",
        "                      metrics =['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gefV7-TDMEJY",
        "outputId": "263b3027-9fa2-4c53-c175-f28893620ff2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "202/203 [============================>.] - ETA: 0s - loss: 1.9514 - accuracy: 0.5678\n",
            "Epoch 1: val_accuracy improved from -inf to 0.00000, saving model to mymodel.h5\n",
            "203/203 [==============================] - 14s 62ms/step - loss: 1.9513 - accuracy: 0.5676 - val_loss: 1.9848 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/10\n",
            "203/203 [==============================] - ETA: 0s - loss: 1.7231 - accuracy: 0.5676\n",
            "Epoch 2: val_accuracy did not improve from 0.00000\n",
            "203/203 [==============================] - 8s 38ms/step - loss: 1.7231 - accuracy: 0.5676 - val_loss: 1.9231 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/10\n",
            "201/203 [============================>.] - ETA: 0s - loss: 1.5404 - accuracy: 0.5683\n",
            "Epoch 3: val_accuracy did not improve from 0.00000\n",
            "203/203 [==============================] - 6s 31ms/step - loss: 1.5400 - accuracy: 0.5676 - val_loss: 1.8917 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/10\n",
            "201/203 [============================>.] - ETA: 0s - loss: 1.3975 - accuracy: 0.5679\n",
            "Epoch 4: val_accuracy did not improve from 0.00000\n",
            "203/203 [==============================] - 6s 28ms/step - loss: 1.3969 - accuracy: 0.5676 - val_loss: 1.8806 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/10\n",
            "202/203 [============================>.] - ETA: 0s - loss: 1.2871 - accuracy: 0.5676\n",
            "Epoch 5: val_accuracy did not improve from 0.00000\n",
            "203/203 [==============================] - 8s 40ms/step - loss: 1.2870 - accuracy: 0.5676 - val_loss: 1.8851 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/10\n",
            "202/203 [============================>.] - ETA: 0s - loss: 1.2033 - accuracy: 0.5675\n",
            "Epoch 6: val_accuracy did not improve from 0.00000\n",
            "203/203 [==============================] - 6s 27ms/step - loss: 1.2030 - accuracy: 0.5676 - val_loss: 1.9002 - val_accuracy: 0.0000e+00\n",
            "Epoch 7/10\n",
            "203/203 [==============================] - ETA: 0s - loss: 1.1389 - accuracy: 0.5676\n",
            "Epoch 7: val_accuracy did not improve from 0.00000\n",
            "203/203 [==============================] - 8s 39ms/step - loss: 1.1389 - accuracy: 0.5676 - val_loss: 1.9191 - val_accuracy: 0.0000e+00\n",
            "Epoch 8/10\n",
            "203/203 [==============================] - ETA: 0s - loss: 1.0896 - accuracy: 0.5676\n",
            "Epoch 8: val_accuracy did not improve from 0.00000\n",
            "203/203 [==============================] - 6s 29ms/step - loss: 1.0896 - accuracy: 0.5676 - val_loss: 1.9348 - val_accuracy: 0.0000e+00\n",
            "Epoch 9/10\n",
            "203/203 [==============================] - ETA: 0s - loss: 1.0515 - accuracy: 0.5676\n",
            "Epoch 9: val_accuracy did not improve from 0.00000\n",
            "203/203 [==============================] - 6s 31ms/step - loss: 1.0515 - accuracy: 0.5676 - val_loss: 1.9471 - val_accuracy: 0.0000e+00\n",
            "Epoch 10/10\n",
            "202/203 [============================>.] - ETA: 0s - loss: 1.0215 - accuracy: 0.5681\n",
            "Epoch 10: val_accuracy did not improve from 0.00000\n",
            "203/203 [==============================] - 10s 50ms/step - loss: 1.0217 - accuracy: 0.5676 - val_loss: 1.9607 - val_accuracy: 0.0000e+00\n",
            "CPU times: user 1min 31s, sys: 2.38 s, total: 1min 33s\n",
            "Wall time: 1min 23s\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%time\n",
        "history = cnn_model_pretrained.fit(train_pad,\n",
        "                              train_labels_encoded,\n",
        "                              epochs = 10,\n",
        "                              batch_size = 32,\n",
        "                              verbose = 1,\n",
        "                              validation_split =0.1,\n",
        "                              callbacks = callback_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "olnZCa0_Matu"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Recreate the exact same model purely from the file:\n",
        "model = load_model(\"mymodel.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKVCkHy7Mavp",
        "outputId": "7dc11866-c8eb-461b-f31d-a4c47fc776c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16/16 [==============================] - 0s 10ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "y_pred = np.argmax(model.predict(test_pad), axis=-1)\n",
        "y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClroFB-MMaz9",
        "outputId": "8e30ea15-6507-4c99-dcf1-0f0bf1b6fdc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      1.00      0.71       275\n",
            "           1       0.00      0.00      0.00       135\n",
            "           2       0.00      0.00      0.00        89\n",
            "\n",
            "    accuracy                           0.55       499\n",
            "   macro avg       0.18      0.33      0.24       499\n",
            "weighted avg       0.30      0.55      0.39       499\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(classification_report(test_labels_encoded,y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "yvJ-1c2TMa3j",
        "outputId": "d91fe102-4d96-42b1-e04f-af10a7bba0d2"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-d08744459582>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_labels_encoded\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m     \"\"\"\n\u001b[0;32m--> 317\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not supported\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \"\"\"\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y_true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y_pred\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    398\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [3245, 499]"
          ]
        }
      ],
      "source": [
        "\n",
        "confusion_matrix(val_labels_encoded,y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZV6d2BVGkpbK"
      },
      "source": [
        "#BiLSTM with glove"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "zHHl89MfktNp"
      },
      "outputs": [],
      "source": [
        "''' Callbacks with Checkpoint'''\n",
        "\n",
        "accuracy_threshold1 = 0.99\n",
        "\n",
        "class myCallback(keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "      if(logs.get('accuracy')>accuracy_threshold1):\n",
        "        print(\"\\nReached %2.2f%% accuracy so we will stop trianing\" % (accuracy_threshold1*100))\n",
        "        self.model.stop_training = True\n",
        "\n",
        "acc_callback0 = myCallback()\n",
        "  # Saved the Best Model\n",
        "filepath = \"mymodel000.h5\"\n",
        "checkpoint0 = keras.callbacks.ModelCheckpoint(filepath, monitor='val_accuracy', verbose=2, save_best_only=True,\n",
        "                                             save_weights_only=False, mode='max')\n",
        "  # callback list\n",
        "callback_list0= [acc_callback0, checkpoint0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2l7artWaknnq",
        "outputId": "216c83e5-bb3a-453e-a008-41de82ebbcee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 200)]             0         \n",
            "                                                                 \n",
            " embedding_1 (Embedding)     (None, 200, 64)           6400000   \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 64)               24832     \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 3)                 195       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,425,027\n",
            "Trainable params: 6,425,027\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D, Bidirectional, LSTM, GRU\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Embedding\n",
        "import keras.callbacks\n",
        "\n",
        "max_length = 200\n",
        "embedding_dim = 64\n",
        "number_of_classes = 3\n",
        "\n",
        "# Define BiLSTM model\n",
        "def lstm():\n",
        "    bi_text_inputs = Input(shape=(max_length,))\n",
        "    bi_embedding_layer = Embedding(max_words, embedding_dim, trainable=True, input_length=max_length)(bi_text_inputs)\n",
        "    LSTM_Layer_1 = Bidirectional(LSTM(32))(bi_embedding_layer)\n",
        "    bi_dense_layer_1 = Dense(number_of_classes, activation='softmax')(LSTM_Layer_1)\n",
        "    bilstm_model = Model(inputs=bi_text_inputs, outputs=bi_dense_layer_1)\n",
        "    return bilstm_model\n",
        "\n",
        "# Create the BiLSTM model with the embedding matrix\n",
        "lstm_model = lstm()\n",
        "\n",
        "# Print the model summary\n",
        "lstm_model.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "fCKuE-p4k7C5"
      },
      "outputs": [],
      "source": [
        "lstm_model.compile(optimizer = 'adam',\n",
        "                      loss = 'sparse_categorical_crossentropy',\n",
        "                      metrics =['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOIbulk8lAyx",
        "outputId": "3bc6ff42-7f32-4dfc-f81c-8798b2e88edc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.9089 - accuracy: 0.5623\n",
            "Epoch 1: val_accuracy improved from -inf to 0.65701, saving model to mymodel000.h5\n",
            "226/226 [==============================] - 82s 338ms/step - loss: 0.9089 - accuracy: 0.5623 - val_loss: 0.7876 - val_accuracy: 0.6570\n",
            "Epoch 2/10\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.8007 - accuracy: 0.6269\n",
            "Epoch 2: val_accuracy improved from 0.65701 to 0.66133, saving model to mymodel000.h5\n",
            "226/226 [==============================] - 71s 315ms/step - loss: 0.8007 - accuracy: 0.6269 - val_loss: 0.7618 - val_accuracy: 0.6613\n",
            "Epoch 3/10\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.7480 - accuracy: 0.6617\n",
            "Epoch 3: val_accuracy did not improve from 0.66133\n",
            "226/226 [==============================] - 63s 280ms/step - loss: 0.7480 - accuracy: 0.6617 - val_loss: 0.8890 - val_accuracy: 0.5451\n",
            "Epoch 4/10\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.7146 - accuracy: 0.6763\n",
            "Epoch 4: val_accuracy did not improve from 0.66133\n",
            "226/226 [==============================] - 68s 300ms/step - loss: 0.7146 - accuracy: 0.6763 - val_loss: 0.8550 - val_accuracy: 0.5701\n",
            "Epoch 5/10\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.6773 - accuracy: 0.7005\n",
            "Epoch 5: val_accuracy did not improve from 0.66133\n",
            "226/226 [==============================] - 67s 297ms/step - loss: 0.6773 - accuracy: 0.7005 - val_loss: 0.8257 - val_accuracy: 0.6102\n",
            "Epoch 6/10\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.6468 - accuracy: 0.7195\n",
            "Epoch 6: val_accuracy did not improve from 0.66133\n",
            "226/226 [==============================] - 66s 294ms/step - loss: 0.6468 - accuracy: 0.7195 - val_loss: 0.8732 - val_accuracy: 0.5877\n",
            "Epoch 7/10\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.6098 - accuracy: 0.7382\n",
            "Epoch 7: val_accuracy did not improve from 0.66133\n",
            "226/226 [==============================] - 64s 285ms/step - loss: 0.6098 - accuracy: 0.7382 - val_loss: 0.8874 - val_accuracy: 0.5951\n",
            "Epoch 8/10\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.5714 - accuracy: 0.7561\n",
            "Epoch 8: val_accuracy did not improve from 0.66133\n",
            "226/226 [==============================] - 64s 283ms/step - loss: 0.5714 - accuracy: 0.7561 - val_loss: 0.9319 - val_accuracy: 0.5710\n",
            "Epoch 9/10\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.5438 - accuracy: 0.7716\n",
            "Epoch 9: val_accuracy did not improve from 0.66133\n",
            "226/226 [==============================] - 72s 317ms/step - loss: 0.5438 - accuracy: 0.7716 - val_loss: 0.9837 - val_accuracy: 0.5852\n",
            "Epoch 10/10\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.5450 - accuracy: 0.7763\n",
            "Epoch 10: val_accuracy did not improve from 0.66133\n",
            "226/226 [==============================] - 64s 285ms/step - loss: 0.5450 - accuracy: 0.7763 - val_loss: 1.0520 - val_accuracy: 0.5572\n",
            "CPU times: user 16min, sys: 20.5 s, total: 16min 20s\n",
            "Wall time: 11min 22s\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%time\n",
        "history4 = lstm_model.fit(train_pad,\n",
        "                              train_labels_encoded,\n",
        "                              epochs = 10,\n",
        "                              batch_size = 32,\n",
        "                              verbose = 1,\n",
        "                              validation_data=(val_pad, val_labels_encoded),\n",
        "                              callbacks = callback_list0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "vw8ZXQRglBRp"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Recreate the exact same model purely from the file:\n",
        "model4 = load_model(\"mymodel000.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgiHyNnTlX8B",
        "outputId": "bc1f3b7b-7ee7-4919-b22a-f85a15704f66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16/16 [==============================] - 1s 32ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 2, 0, 1, 1, 0,\n",
              "       0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 1, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 2,\n",
              "       0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,\n",
              "       1, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
              "       0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ],
      "source": [
        "y_pred4 = np.argmax(model4.predict(test_pad), axis=-1)\n",
        "y_pred4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1bdd0BYlicR",
        "outputId": "f5cbb413-b5b3-4d52-9381-1f118ffcba54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      1.00      0.71       275\n",
            "           1       0.00      0.00      0.00       135\n",
            "           2       0.00      0.00      0.00        89\n",
            "\n",
            "    accuracy                           0.55       499\n",
            "   macro avg       0.18      0.33      0.24       499\n",
            "weighted avg       0.30      0.55      0.39       499\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(test_labels_encoded,y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDf4fr2NoZZY"
      },
      "source": [
        "#CNN+BiLSTM with glove"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGEuKvjlocr4",
        "outputId": "1f1d2928-3fa2-4477-8cf7-af84268c0d71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 200, 64)           6400000   \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 199, 32)           4128      \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 99, 32)           0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 99, 64)           16640     \n",
            " l)                                                              \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 6336)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                405568    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 3)                 195       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,826,531\n",
            "Trainable params: 6,826,531\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "keras.backend.clear_session()\n",
        "\n",
        "max_length = 200\n",
        "embedding_dim = 64\n",
        "number_of_classes = 3\n",
        "\n",
        "# Define CNN + LSTM model\n",
        "model_cl1 = Sequential()\n",
        "model_cl1.add(Embedding(max_words, embedding_dim, input_length=max_length, trainable=True))\n",
        "#model_cl.add(SpatialDropout1D(0.8))\n",
        "#model_cl.add(Conv1D(filters=64, kernel_size=6, padding='same', activation='relu'))\n",
        "#model_cl.add(MaxPooling1D(pool_size=2))\n",
        "model_cl1.add(Conv1D(filters=32, kernel_size=2, activation='relu'))\n",
        "model_cl1.add(MaxPooling1D(pool_size=2))\n",
        "model_cl1.add(Bidirectional(LSTM(32, dropout=0.5, recurrent_dropout=0.5, return_sequences=True)))\n",
        "#model_cl.add(Dropout(0.5))\n",
        "#model_cl.add(Bidirectional(LSTM(400, dropout=0.5, recurrent_dropout=0.5)))\n",
        "#model_cl.add(Dropout(0.5))\n",
        "model_cl1.add(Flatten())\n",
        "model_cl1.add(Dense(64, activation='relu'))\n",
        "#model_cl.add(Dropout(0.5))\n",
        "model_cl1.add(Dense(number_of_classes, activation='softmax'))\n",
        "model_cl1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "print(model_cl1.summary())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "0BXbwFwN2rYN"
      },
      "outputs": [],
      "source": [
        "model_cl1.compile(optimizer = 'adam',\n",
        "                      loss = 'sparse_categorical_crossentropy',\n",
        "                      metrics =['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "HSql-C-33U1A"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Convert train_labels and val_labels to strings\n",
        "train_labels = [str(label) for label in train_labels]\n",
        "val_labels = [str(label) for label in val_labels]\n",
        "\n",
        "# Create an instance of LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit label encoder on the combined labels\n",
        "combined_labels = train_labels + val_labels\n",
        "label_encoder.fit(combined_labels)\n",
        "\n",
        "# Transform the training labels\n",
        "train_labels_encoded = label_encoder.transform(train_labels)\n",
        "\n",
        "# Transform the validation labels\n",
        "val_labels_encoded = label_encoder.transform(val_labels)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "6HG1ByWw3eH1"
      },
      "outputs": [],
      "source": [
        "\n",
        "''' Callbacks with Checkpoint'''\n",
        "\n",
        "accuracy_threshold1 = 0.99\n",
        "\n",
        "class myCallback1(keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "      if(logs.get('accuracy')>accuracy_threshold1):\n",
        "        print(\"\\nReached %2.2f%% accuracy so we will stop trianing\" % (accuracy_threshold1*100))\n",
        "        self.model.stop_training = True\n",
        "\n",
        "acc_callback2 = myCallback1()\n",
        "  # Saved the Best Model\n",
        "filepath = \"mymodel_kell.h5\"\n",
        "checkpoint2 = keras.callbacks.ModelCheckpoint(filepath, monitor='val_accuracy', verbose=2, save_best_only=True,\n",
        "                                             save_weights_only=False, mode='max')\n",
        "  # callback list\n",
        "callback_list1= [acc_callback2, checkpoint2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5wf79pr3ujV",
        "outputId": "e262c3f3-91ae-40bd-c12b-662bbfa02723"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.8906 - accuracy: 0.5803\n",
            "Epoch 1: val_accuracy improved from -inf to 0.61541, saving model to mymodel_kell.h5\n",
            "226/226 [==============================] - 99s 405ms/step - loss: 0.8906 - accuracy: 0.5803 - val_loss: 0.8305 - val_accuracy: 0.6154\n",
            "Epoch 2/10\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.8161 - accuracy: 0.6126\n",
            "Epoch 2: val_accuracy improved from 0.61541 to 0.62157, saving model to mymodel_kell.h5\n",
            "226/226 [==============================] - 90s 399ms/step - loss: 0.8161 - accuracy: 0.6126 - val_loss: 0.7910 - val_accuracy: 0.6216\n",
            "Epoch 3/10\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.7338 - accuracy: 0.6648\n",
            "Epoch 3: val_accuracy did not improve from 0.62157\n",
            "226/226 [==============================] - 82s 363ms/step - loss: 0.7338 - accuracy: 0.6648 - val_loss: 0.9523 - val_accuracy: 0.4955\n",
            "Epoch 4/10\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.6849 - accuracy: 0.6864\n",
            "Epoch 4: val_accuracy did not improve from 0.62157\n",
            "226/226 [==============================] - 87s 386ms/step - loss: 0.6849 - accuracy: 0.6864 - val_loss: 0.8970 - val_accuracy: 0.5692\n",
            "Epoch 5/10\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.6200 - accuracy: 0.7202\n",
            "Epoch 5: val_accuracy did not improve from 0.62157\n",
            "226/226 [==============================] - 82s 361ms/step - loss: 0.6200 - accuracy: 0.7202 - val_loss: 0.9100 - val_accuracy: 0.5683\n",
            "Epoch 6/10\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.5593 - accuracy: 0.7510\n",
            "Epoch 6: val_accuracy did not improve from 0.62157\n",
            "226/226 [==============================] - 82s 364ms/step - loss: 0.5593 - accuracy: 0.7510 - val_loss: 0.9101 - val_accuracy: 0.5864\n",
            "Epoch 7/10\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.5002 - accuracy: 0.7861\n",
            "Epoch 7: val_accuracy did not improve from 0.62157\n",
            "226/226 [==============================] - 90s 400ms/step - loss: 0.5002 - accuracy: 0.7861 - val_loss: 0.9873 - val_accuracy: 0.6025\n",
            "Epoch 8/10\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.4505 - accuracy: 0.8038\n",
            "Epoch 8: val_accuracy did not improve from 0.62157\n",
            "226/226 [==============================] - 86s 381ms/step - loss: 0.4505 - accuracy: 0.8038 - val_loss: 1.1785 - val_accuracy: 0.5002\n",
            "Epoch 9/10\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.4171 - accuracy: 0.8228\n",
            "Epoch 9: val_accuracy did not improve from 0.62157\n",
            "226/226 [==============================] - 89s 396ms/step - loss: 0.4171 - accuracy: 0.8228 - val_loss: 1.2706 - val_accuracy: 0.5273\n",
            "Epoch 10/10\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.3654 - accuracy: 0.8510\n",
            "Epoch 10: val_accuracy did not improve from 0.62157\n",
            "226/226 [==============================] - 83s 369ms/step - loss: 0.3654 - accuracy: 0.8510 - val_loss: 1.2827 - val_accuracy: 0.5670\n"
          ]
        }
      ],
      "source": [
        "history1= model_cl1.fit(train_pad, train_labels_encoded, epochs=10, batch_size=32, verbose=1, validation_data=(val_pad, val_labels_encoded), callbacks=callback_list1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "G1Plmgeb3wFV"
      },
      "outputs": [],
      "source": [
        "model6 = load_model(\"mymodel_kell.h5\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpRBy0qP4CUH",
        "outputId": "ba7ededd-50d3-4c37-cb5c-042fd38a2a4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16/16 [==============================] - 1s 30ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
              "       0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,\n",
              "       0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,\n",
              "       0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,\n",
              "       0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,\n",
              "       0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
              "       0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
              "       0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,\n",
              "       1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
              "       1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
              "       0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,\n",
              "       0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,\n",
              "       0, 0, 0, 1, 1, 0, 1, 2, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,\n",
              "       1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,\n",
              "       1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,\n",
              "       0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,\n",
              "       1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,\n",
              "       0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,\n",
              "       0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,\n",
              "       0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ],
      "source": [
        "y_pred6 = np.argmax(model6.predict(test_pad), axis=-1)\n",
        "y_pred6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nTVwRXl4Cml",
        "outputId": "41a9f903-69a8-45a5-8c9a-da3fea6ef74f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.51      0.52       275\n",
            "           1       0.34      0.59      0.43       135\n",
            "           2       1.00      0.01      0.02        89\n",
            "\n",
            "    accuracy                           0.44       499\n",
            "   macro avg       0.63      0.37      0.33       499\n",
            "weighted avg       0.57      0.44      0.41       499\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(test_labels_encoded,y_pred6))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWiPSZvGMrtU"
      },
      "source": [
        "#CNN with pretrained Fastext Word Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "OpMD3kDPMnnf"
      },
      "outputs": [],
      "source": [
        "from urllib.request import urlopen\n",
        "import gzip\n",
        "\n",
        "# get the vectors\n",
        "file = gzip.open(urlopen('https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.bn.300.vec.gz'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "ZPyiLE1WMnqo"
      },
      "outputs": [],
      "source": [
        "vocab_and_vectors = {}\n",
        "# put words as dict indexes and vectors as words values\n",
        "for line in file:\n",
        "  values = line.split()\n",
        "  word = values [0].decode('utf-8')\n",
        "  vector = np.asarray(values[1:], dtype='float32')\n",
        "  vocab_and_vectors[word] = vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkrxmgufMn_H",
        "outputId": "c9c1b9a7-fd6a-416c-d134-ce05b020d1fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "—mostly\n",
            "writing—\n",
            "works”\n",
            "arm’s\n",
            "restaurant’s\n",
            "stepfamily\n",
            "🥳🎉\n",
            "emotional”\n",
            "swettingbehind\n",
            "spasms\n",
            "imemdiately\n",
            "oxygyn\n",
            "speedpaint\n",
            "9am\n",
            "nemours\n",
            "pothead\n",
            "dean's\n",
            "“witness\n",
            "dorms”\n",
            "surveill\n",
            "aerea\n",
            "compulsively\n",
            "jacked\n",
            "deadlift\n",
            "tump\n",
            "'pricky'\n",
            "restate\n",
            "'screaming\n",
            "class's\n",
            "unpleasantness\n",
            "complexion\n",
            "matriculate\n",
            "tuitions\n",
            "promiscuous\n",
            "opulent\n",
            "allergies\n",
            "hoarders\n",
            "crush's\n",
            "dehumanizing\n",
            "braze\n",
            "“follow\n",
            "gut”\n",
            "wiping\n",
            "groupchats\n",
            "unconfortable\n",
            "bumbling\n",
            "dismorphia\n",
            "effervescent\n",
            "calloused\n",
            "simmer\n",
            "singleness\n",
            "pinches\n",
            "t”\n",
            "rejoined\n",
            "“revising\n",
            "alluding\n",
            "contorted\n",
            "pointy\n",
            "beady\n",
            "cools\n",
            "contort\n",
            "“hoe”\n",
            "crappier\n",
            "socioeconomic\n",
            "glover\n",
            "vindicators\n",
            "vapes\n",
            "responsed\n",
            "cowardice\n",
            "•absolutely\n",
            "•constant\n",
            "•loss\n",
            "snuffed\n",
            "•trouble\n",
            "•often\n",
            "ledges\n",
            "desolated\n",
            "pennsylvania\n",
            "gamecube\n",
            "1900's\n",
            "wanderer\n",
            "tampering\n",
            "cherishing\n",
            "numbly\n",
            "reappearing\n",
            "move”\n",
            "jobwhere\n",
            "heaping\n",
            "bestest\n",
            "bcecause\n",
            "“hang\n",
            "there”\n",
            "progres\n",
            "celxa\n",
            "durt\n",
            "befor\n",
            "whiney\n",
            "fuckfuckfuckfuckfuck\n",
            "unappreciative\n",
            "btec\n",
            "warhammer\n",
            "circlejerk\n",
            "egomaniacal\n",
            "“ehh\n",
            "catered\n",
            "busy”\n",
            "“i’ll\n",
            "coincidental\n",
            "circlejerks\n",
            "cayden\n",
            "oakland\n",
            "anthonys\n",
            "shithead\n",
            "12years\n",
            "𝙩𝙝𝙖𝙩\n",
            "phenomenally\n",
            "beens\n",
            "fuelling\n",
            "'dust\n",
            "bricking\n",
            "ughhhhh\n",
            "puddle\n",
            "anyone would\n",
            "'teenage\n",
            "bs'\n",
            "one”\n",
            "fbi\n",
            "snipes\n",
            "putted\n",
            "unuckily\n",
            "togheter\n",
            "intelligen\n",
            "obvusly\n",
            "recive\n",
            "'nother\n",
            "visted\n",
            "between”\n",
            "definitve\n",
            "talking”\n",
            "moreso\n",
            "intrusively\n",
            "frontlines\n",
            "predisposes\n",
            "slowlydrift\n",
            "disobey\n",
            "extort\n",
            "snowed\n",
            "unsteady\n",
            "motovation\n",
            "angrier\n",
            "“then\n",
            "seconds”\n",
            "impossible”\n",
            "wise”\n",
            "prioritise\n",
            "evaporate\n",
            "cooping\n",
            "oculus\n",
            "worryingly\n",
            "emotionaly\n",
            "scrupulosity\n",
            "bareky\n",
            "whoi\n",
            "hatw\n",
            "wwnt\n",
            "loasing\n",
            "soemthing\n",
            "throath\n",
            "dafuq\n",
            "cloudiness\n",
            "houston\n",
            "« nah\n",
            "lmao »\n",
            "victimise\n",
            "victimisation\n",
            "debunk\n",
            "tramatic\n",
            "dreamy\n",
            "gofundme\n",
            "aaaaaaa\n",
            "m17\n",
            "ffuuuck\n",
            "ffuuck\n",
            "ffuck\n",
            "plllease\n",
            "slowe\n",
            "moderatly\n",
            "mended\n",
            "intertwines\n",
            "freddy\n",
            "van’t\n",
            "uncomftrable\n",
            "doig\n",
            "intervene\n",
            "beetween\n",
            "frainds\n",
            "killjoy\n",
            "exagerating\n",
            "clincal\n",
            "mornin\n",
            "hesitating\n",
            "pestered\n",
            "tifu\n",
            "suffocates\n",
            "😥\n",
            "170cm\n",
            "nothin\n",
            "strange’s\n",
            "m26\n",
            "leprechaun\n",
            "pounce\n",
            "fortuna\n",
            "caging\n",
            "victimising\n",
            "affirmation\n",
            "sparkling\n",
            "optimising\n",
            "changement\n",
            "electronical\n",
            "greetings\n",
            "bets\n",
            "schoolmate\n",
            "dide\n",
            "imbecile\n",
            "betterment\n",
            "tadario\n",
            "“thoughts”\n",
            "illness's\n",
            "unaccomplished\n",
            "do😔\n",
            "gravol\n",
            "“highs\n",
            "lows”\n",
            "head…\n",
            "“wake\n",
            "mom”\n",
            "“am\n",
            "”what’s\n",
            "disguising\n",
            "falll\n",
            "👏kill👏me👏please👏\n",
            "uneless\n",
            "trird\n",
            "aurelius\n",
            "callum\n",
            "well'\n",
            "snotting\n",
            "today”\n",
            "amirite\n",
            "dirtball\n",
            "300kms\n",
            "placate\n",
            "huckster\n",
            "bathrobe\n",
            "flud\n",
            "infuriates\n",
            "exploratory\n",
            "numerable\n",
            "metaphore\n",
            "bandaging\n",
            "trimesters\n",
            "dishing\n",
            "old'\n",
            "redit\n",
            "absoulte\n",
            "“find\n",
            "inevitability\n",
            "remember—\n",
            "family—\n",
            "noosa\n",
            "hense\n",
            "'low'\n",
            "limping\n",
            "vouched\n",
            "dinnner\n",
            "pitt\n",
            "aweful\n",
            "reoccurring\n",
            "00003\n",
            "flushed\n",
            "inexplicable\n",
            "acutane\n",
            "sinned\n",
            "f26\n",
            "party”\n",
            "loliness\n",
            "14y\n",
            "poof\n",
            "liven\n",
            "nighter\n",
            "saul\n",
            "anime’s\n",
            "mundaneness\n",
            "geez\n",
            "echos\n",
            "18years\n",
            "appartement\n",
            "spontaneus\n",
            "stupidiest\n",
            "m'y\n",
            "do™️\n",
            "inebriated\n",
            "insicurities\n",
            "razors\n",
            "whiplash\n",
            "''at\n",
            "home''\n",
            "earbuds\n",
            "nourish\n",
            "cradled\n",
            "goddamnit\n",
            "applaud\n",
            "braindead\n",
            "flutter\n",
            "survivalist\n",
            "theur\n",
            "beastly\n",
            "😘✌lol\n",
            "ahaha\n",
            "lovelies\n",
            "💗💗\n",
            "bcits\n",
            "frickin\n",
            "figment\n",
            "yunno\n",
            "verbalize\n",
            "giggly\n",
            "normatively\n",
            "peering\n",
            "cliff's\n",
            "“executive\n",
            "drs’\n",
            "5150’d\n",
            "underpass\n",
            "🤷🏽‍♂️\n",
            "best”\n",
            "“stick\n",
            "overbite\n",
            "reawakened\n",
            "tolls\n",
            "27kg\n",
            "rope's\n",
            "muchly\n",
            "neverland\n",
            "showgirls\n",
            "vhs\n",
            "“have”\n",
            "hookers\n",
            "“accepted”\n",
            "rockstar\n",
            "atlanta\n",
            "vh1\n",
            "devious\n",
            "roosh\n",
            "🥜\n",
            "“understand\n",
            "“rebelled”\n",
            "shepard\n",
            "nevadan\n",
            "stds\n",
            "weight”\n",
            "boy”\n",
            "abysmal\n",
            "precipice\n",
            "elusive\n",
            "mince\n",
            "dovey\n",
            "erode\n",
            "numbingly\n",
            "fells\n",
            "'how\n",
            "aday\n",
            "“yeah”\n",
            "“nah”\n",
            "this'll\n",
            "5'9\n",
            "grindr\n",
            "thati\n",
            "piercings\n",
            "reproach\n",
            "•im\n",
            "•my\n",
            "“helping”\n",
            "azel04\n",
            "rustling\n",
            "gust\n",
            "hows\n",
            "guys❤\n",
            "iforgot\n",
            "🌲\n",
            "cognizant\n",
            "postured\n",
            "year'\n",
            "m18\n",
            "feeling”\n",
            "anybody’s\n",
            "cyrlqtvunbg1lsjvdvsqsy\n",
            "2020❤️\n",
            "sieze\n",
            "shaw324\n",
            "одиночество\n",
            "всем\n",
            "привет\n",
            "казахстана\n",
            "зовут\n",
            "виталя\n",
            "мне\n",
            "года\n",
            "сегодня\n",
            "новый\n",
            "получил\n",
            "одной\n",
            "смс\n",
            "звонка\n",
            "поздравлениями\n",
            "родители\n",
            "братья\n",
            "поздравили\n",
            "ненавижу\n",
            "праздники\n",
            "очередной\n",
            "раз\n",
            "показали\n",
            "весь\n",
            "погружен\n",
            "работу\n",
            "выходной\n",
            "праздничный\n",
            "день\n",
            "понимаешь\n",
            "один\n",
            "друзей\n",
            "родственникам\n",
            "сейчас\n",
            "хочу\n",
            "набить\n",
            "надписью\n",
            "хочется\n",
            "буду\n",
            "смотреть\n",
            "отгонять\n",
            "мысли\n",
            "напишите\n",
            "пожалуйста\n",
            "нибудь\n",
            "комментариях\n",
            "you❤️\n",
            "snitched\n",
            "hjahahahahah\n",
            "soko\n",
            "voku\n",
            "verizon\n",
            "terpin\n",
            "900k\n",
            "problenms\n",
            "porpuse\n",
            "recompense\n",
            "ralph\n",
            "waldo\n",
            "✨\n",
            "peterson\n",
            "overlord\n",
            "kermit\n",
            "nick's\n",
            "applepodcasts\n",
            "2fpodcasts\n",
            "2fus\n",
            "2fpodcast\n",
            "2fnicks\n",
            "2fid1450771426\n",
            "2fsoundcloud\n",
            "2fuser\n",
            "2ftracks\n",
            "osama\n",
            "payback\n",
            "it🙏\n",
            "5cbjcqv8cud816vnx5omdx\n",
            "umpm1dq\n",
            "sulkiwy7bhbq\n",
            "✌️\n",
            "hings\n",
            "😪\n",
            "cka\n",
            "egross\n",
            "qnbn6epqzrk\n",
            "idkrb\n",
            "cutee\n",
            "imaa\n",
            "fishhhh\n",
            "thooo\n",
            "yayy\n",
            "devasted\n",
            "gratefully\n",
            "livestream\n",
            "imprinted\n",
            "resonating\n",
            "absorbing\n",
            "sleepaffirmations\n",
            "wealthaffirmations\n",
            "“there\n",
            "enjoy”\n",
            "maybeeeee\n",
            "neighbor’s\n",
            "outlive\n",
            "afforfable\n",
            "overcrowding\n",
            "workthile\n",
            "esteem's\n",
            "conparison\n",
            "unscrutonized\n",
            "goldy\n",
            "peacefull\n",
            "theese\n",
            "0 \n",
            "shouldnt \n",
            "stoicism\n",
            "joust\n",
            "fullfilled\n",
            "terrorized\n",
            "neuroplasticity\n",
            "percieve\n",
            "ceaser's\n",
            "50℅\n",
            "exprctation\n",
            "rearange\n",
            "surive\n",
            "conplexity\n",
            "hedonic\n",
            "neuroplasticty\n",
            "missunderestood\n",
            "necesarilly\n",
            "yourself❤️\n",
            "applys\n",
            "ereased\n",
            "fun“\n",
            "clayton\n",
            "whens\n",
            "streamers\n",
            "polygamous\n",
            "alway\n",
            "kg’s\n",
            "unibrow\n",
            "holdiays\n",
            "conver\n",
            "cashier\n",
            "gaddamn\n",
            "fav\n",
            "joji\n",
            "outpouring\n",
            "frustrations\n",
            "advantaged\n",
            "12hr\n",
            "appt\n",
            "altho\n",
            "poemhunter\n",
            "sloes\n",
            "compartmentalizing\n",
            "kinks\n",
            "avery\n",
            "lynchpin\n",
            "housemate's\n",
            "unfollow\n",
            "bloomer\n",
            "70's\n",
            "whatsapp's\n",
            "snotted\n",
            "millionare\n",
            "smitten\n",
            "hapy\n",
            "mykife\n",
            "“friendship”\n",
            "suuuper\n",
            "others”\n",
            "morphed\n",
            "frosting\n",
            "this💜☮️\n",
            "exfoliate\n",
            "vintages\n",
            "croix\n",
            "ozeruga\n",
            "e0dud\n",
            "kudos\n",
            "“effort\n",
            "energy”\n",
            "voice”\n",
            "dog's\n",
            "flow'\n",
            "baity\n",
            "heafy\n",
            "trivium\n",
            "ravenous\n",
            "hollowed\n",
            "raviolis\n",
            "vouchers\n",
            "body is heavy\n",
            "looks like\n",
            "is busy\n",
            "“tomorrow”\n",
            "“tomorrow\n",
            "extravagant\n",
            "hopefulness\n",
            "fret\n",
            "uplifted\n",
            "groomed\n",
            "puncture\n",
            "t11nani\n",
            "professoroak11\n",
            "chuffed\n",
            "platonically\n",
            "🙁\n",
            "healthiest\n",
            "keller\n",
            "everyhting\n",
            "appologized\n",
            "helpess\n",
            "illess\n",
            "irs\n",
            "pretty¹\n",
            "tormented\n",
            "shootings\n",
            "dressers\n",
            "whoever’s\n",
            "“my\n",
            "month”\n",
            "difficultly\n",
            "reture\n",
            "stead\n",
            "perpetuates\n",
            "crawler\n",
            "tangental\n",
            "subconcsious\n",
            "hittin\n",
            "eww\n",
            "purposeful\n",
            "'18\n",
            "mustang\n",
            "tyring\n",
            "tampa\n",
            "hashimotos\n",
            "allot\n",
            "adk\n",
            "inconvenienced\n",
            "shittiness\n",
            "lense\n",
            "perfecting\n",
            "“sorry\n",
            "ahhhh\n",
            "💪\n",
            "jugded\n",
            "soonish\n",
            "“work\n",
            "live”\n",
            "dawgs\n",
            "fipwb2ymve8\n",
            "ridicoudly\n",
            "redid\n",
            "student’s\n",
            "weirdness\n",
            "warcraft\n",
            "“fire\n",
            "valedictorian\n",
            "waffles\n",
            "whisked\n",
            "comitted\n",
            "gritted\n",
            "looned\n",
            "pipils\n",
            "bared\n",
            "askew\n",
            "banal\n",
            "lenient\n",
            "shins\n",
            "misstakes\n",
            "misshaps\n",
            "asbergers\n",
            "carshop\n",
            "wonderfull\n",
            "incompatabile\n",
            "blackhead\n",
            "manboobs\n",
            "overstepping\n",
            "“pray”\n",
            "adored\n",
            "special”\n",
            "pitchforks\n",
            "day”\n",
            "unconnected\n",
            "5yrs\n",
            "18yrs\n",
            "molsters\n",
            "famiy\n",
            "fufilled\n",
            "desensitising\n",
            "gasped\n",
            "whateveryouare\n",
            "immortalise\n",
            "else \n",
            "trauma \n",
            "it \n",
            "changed  \n",
            "blossomed\n",
            "vowing\n",
            "'never\n",
            "bed'\n",
            "outrun\n",
            "daytimes\n",
            "'functional'\n",
            "own¿\n",
            "stéphane's\n",
            "scoffed\n",
            "craig\n",
            "moronic\n",
            "gcse's\n",
            "trynna\n",
            "skint\n",
            "bustles\n",
            "rusted\n",
            "rebelliousness\n",
            "maticulously\n",
            "timebomb\n",
            "prediabetic\n",
            "bloaad\n",
            "unliveable\n",
            "passangers\n",
            "weathers\n",
            "stroll\n",
            "chester\n",
            "bennington\n",
            "sandler\n",
            "studyings\n",
            "🤔\n",
            "shisha\n",
            "suppiorted\n",
            "decaf\n",
            "achiecved\n",
            "pepole\n",
            "snob\n",
            "fared\n",
            "hyping\n",
            "claustrophobic\n",
            "spats\n",
            "‘emotional\n",
            "rock’\n",
            "parter\n",
            "‘you\n",
            "everyday’\n",
            "plan's\n",
            "“reward”\n",
            "recouperate\n",
            "begone\n",
            "“phobia\n",
            "trust”\n",
            "herself”\n",
            "bscs\n",
            "sportsman\n",
            "quetions\n",
            "earshot\n",
            "clientele\n",
            "dirtbag\n",
            "motivator\n",
            "aquaintences\n",
            "spitballed\n",
            "oddest\n",
            "joyfulness\n",
            "justbgive\n",
            "sleepovers\n",
            "‘different’\n",
            "head’s\n",
            "spotless\n",
            "brita\n",
            "“skinny\n",
            "stomach”\n",
            "“by\n",
            "‘wrong’\n",
            "3months\n",
            "500km\n",
            "themseleves\n",
            "glancing\n",
            "pharmacymedications\n",
            "fortariel\n",
            "“reinvent”\n",
            "teammates\n",
            "3×\n",
            "proctor\n",
            "12547225362\n",
            "confidence—well\n",
            "ooh\n",
            "snitching\n",
            "poisioning\n",
            "pocked\n",
            "remmember\n",
            "moped\n",
            "depriving\n",
            "ya'\n",
            "“job”\n",
            "overcrowded\n",
            "“comforted\n",
            "“apologized”\n",
            "wronged\n",
            "assertively\n",
            "her”\n",
            "emasculating\n",
            "“open\n",
            "ludicrous\n",
            "remarriage\n",
            "swooning\n",
            "dreamily\n",
            "mirin\n",
            "homocide\n",
            "5elboykb4xyjgys5zgukao\n",
            "jtbjbj8creojgnjop6\n",
            "uzg\n",
            "anymore…\n",
            "foh\n",
            "microdoses\n",
            "unclean\n",
            "germaphobia\n",
            "vacuuming\n",
            "“treat\n",
            "“lie\n",
            "“cheat\n",
            "interventionist\n",
            "endocrinologist\n",
            "hashimoto\n",
            "metabalism\n",
            "roaches\n",
            "fumigation\n",
            "exterminator\n",
            "bugged\n",
            "fufiling\n",
            "fealing\n",
            "succeced\n",
            "perserving\n",
            "firey\n",
            "pretanious\n",
            "arthouse\n",
            "ascencion\n",
            "h1n1\n",
            "vise\n",
            "alllll\n",
            "140something\n",
            "loveable\n",
            "donates\n",
            "adobt\n",
            "sunway\n",
            "cravings\n",
            "despiteful\n",
            "neger\n",
            "barbarian\n",
            "“3\n",
            "blessings”\n",
            "m24\n",
            "downpour\n",
            "wiggled\n",
            "snuggling\n",
            "qthan\n",
            "hecked\n",
            "aaaah\n",
            "inquisitive\n",
            "aggressiv\n",
            "singel\n",
            "readed\n",
            "comprehended\n",
            "conscient\n",
            "everyone👋\n",
            "unlikable\n",
            "“ï\n",
            "penis”\n",
            "fathomed\n",
            "breyon\n",
            "4debe598cdf8\n",
            "fingernails\n",
            "“clingy”\n",
            "“nope\n",
            "want's\n",
            "unwelcomed\n",
            "scrapbook\n",
            "dinners\n",
            "vocalize\n",
            "skillet\n",
            "alchahol\n",
            "rnqxr88nczq\n",
            "errand\n",
            "stickie\n",
            "marxism\n",
            "“embarrased”\n",
            "dredge\n",
            "meeeeeeeeeee\n",
            "jacksepticeye’s\n",
            "it❤️\n",
            "myself'\n",
            "trays\n",
            "uoutself\n",
            "😖\n",
            "attention”\n",
            "“tired\n",
            "writhing\n",
            "dysmorphia\n",
            "pretty”\n",
            "viens\n",
            "dumshit\n",
            "injustices\n",
            "conclusions—ones\n",
            "pathologize\n",
            "caretaking\n",
            "deflecting\n",
            "insinuated\n",
            "outliving\n",
            "hopefullness\n",
            "i'ma\n",
            "leave”\n",
            "harrassment\n",
            "syrians\n",
            "silverlining\n",
            "ecofascist\n",
            "retweeted\n",
            "dogpile\n",
            "genocidal\n",
            "dogpiled\n",
            "‘hug\n",
            "boyfriend’\n",
            "sucsefully\n",
            "energia\n",
            "thougs\n",
            "toothache\n",
            "gobs\n",
            "externalize\n",
            "scraed\n",
            "11yo\n",
            "stabilizers\n",
            "😀\n",
            "worksheets\n",
            "cernusco\n",
            "naviglio\n",
            "there´s\n",
            "worsts\n",
            "symptons\n",
            "surgerys\n",
            "wouldn´t\n",
            "sega\n",
            "bullyng\n",
            "recesses\n",
            "shitposts\n",
            "superbad\n",
            "ferris\n",
            "bueller's\n",
            "clinicians\n",
            "'done'\n",
            "revolve\n",
            "180mg\n",
            "gassy\n",
            "undertone\n",
            "brainwashed\n",
            "asshat\n",
            "beca\n",
            "michele\n",
            "pompano\n",
            "dpression\n",
            "nuise\n",
            "streamer\n",
            "“empty”\n",
            "throat”\n",
            "45mins\n",
            "“bubble”away\n",
            "unwind\n",
            "expres\n",
            "argh\n",
            "diminishes\n",
            "electroconvulsive\n",
            "😁🌞\n",
            "flys\n",
            "heartcharming\n",
            "filmaffinity\n",
            "karakai\n",
            "jouzu\n",
            "354339679\n",
            "mmed\n",
            "ethernet\n",
            "dingle\n",
            "titanfall\n",
            "hellblade\n",
            "senua's\n",
            "revisiting\n",
            "'why\n",
            "job'\n",
            "'go\n",
            "people'\n",
            "tole\n",
            "that’d\n",
            "altruistic\n",
            "dissociated\n",
            "amaze\n",
            "disowns\n",
            "'good\n",
            "news'\n",
            "nadda\n",
            "'brighter\n",
            "days'\n",
            "'stroke\n",
            "luck'\n",
            "'better\n",
            "'need'\n",
            "'easier'\n",
            "'yes\n",
            "'found'\n",
            "14yo\n",
            "unthoughtful\n",
            "adventage\n",
            "attencion\n",
            "40lbs\n",
            "fibromyalgia\n",
            "cherishes\n",
            "adrenalin\n",
            "worseto\n",
            "forhead\n",
            "sensivity\n",
            "whattit\n",
            "justtthe\n",
            "avout\n",
            "abnormality\n",
            "underaged\n",
            "“memento”\n",
            "“string”\n",
            "comjng\n",
            "obsessiveness\n",
            "sleepity\n",
            "nught\n",
            "quacks\n",
            "“lightest”\n",
            "morbidlyreality\n",
            "army”\n",
            "protestor\n",
            "canister\n",
            "“free\n",
            "hk”\n",
            "untethered\n",
            "41m\n",
            "fmily\n",
            "jackknife\n",
            "empatheticz\n",
            "wanta\n",
            "thoigh\n",
            "doesent\n",
            "hapoened\n",
            "empathitic\n",
            "și\n",
            "amenities\n",
            "✌🏼\n",
            "unberable\n",
            "biotech\n",
            "“thoughts\n",
            "800mg\n",
            "faillure\n",
            "medschool\n",
            "fullfill\n",
            "sundress\n",
            "17ish\n",
            "pinpoints\n",
            "“attacks”\n",
            "melancholic\n",
            "sadden\n",
            "intimal\n",
            "caress\n",
            "subbing\n",
            "bodyweight\n",
            "theorize\n",
            "destructed\n",
            "amygdala\n",
            "adept\n",
            "inadequacies\n",
            "gnaws\n",
            "unhealthily\n",
            "gues\n",
            "31th\n",
            "paranthesis\n",
            "antiquated\n",
            "doimg\n",
            "refocused\n",
            "doctprs\n",
            "bavk\n",
            "stunted\n",
            "greener\n",
            "craigslist\n",
            "distrusting\n",
            "temporariness\n",
            "contextualise\n",
            "functional…\n",
            "sceptical\n",
            "“dimmed”\n",
            "“on\n",
            "demand”\n",
            "gaba\n",
            "distractibility\n",
            "“abused”\n",
            "blight\n",
            "laughable\n",
            "fluent'\n",
            "'radiating\n",
            "glow'\n",
            "fast'\n",
            "bleakly\n",
            "4as\n",
            "dumbfucks\n",
            "16yr\n",
            "gcse’s\n",
            "mysekf\n",
            "dependance\n",
            "overreacts\n",
            "overreach\n",
            "“gee\n",
            "“sigh\n",
            "metallothionein\n",
            "deal”\n",
            "stalked\n",
            "mindis\n",
            "depression’s\n",
            "t50\n",
            "personable\n",
            "forewent\n",
            "comparably\n",
            "regressed\n",
            "“following\n",
            "friends”\n",
            "“what’s”\n",
            "“hows\n",
            "uppers\n",
            "ndri\n",
            "offically\n",
            "wot\n",
            "keema\n",
            "exist”\n",
            "escape”\n",
            "withholding\n",
            "chiling\n",
            "bewildered\n",
            "'official'\n",
            "vraylar\n",
            "calibrate\n",
            "homecountry\n",
            "coasted\n",
            "riddenly\n",
            "definitively\n",
            "thyroxin\n",
            "allergys\n",
            "coffein\n",
            "antidepressiva\n",
            "bupripion\n",
            "reboxetine\n",
            "moclobemide\n",
            "tranylcypromine\n",
            "amisulpride\n",
            "tianeptine\n",
            "sulbutiamine\n",
            "piracetam\n",
            "aniracetam\n",
            "boast\n",
            "prestiq\n",
            "finasteride\n",
            "noice\n",
            "flexed\n",
            "finsta\n",
            "skimmed\n",
            "blase\n",
            "denationalization\n",
            "barelt\n",
            "shakier\n",
            "quazar3846\n",
            "councelling\n",
            "„friends“\n",
            "10years\n",
            "stressy\n",
            "„growing\n",
            "up“\n",
            "800km\n",
            "1year\n",
            "bloodstream\n",
            "fickle\n",
            "unbecoming\n",
            "vividness\n",
            "freshen\n",
            "egotism\n",
            "zooming\n",
            "gleans\n",
            "laugh…\n",
            "grasps\n",
            "vcr\n",
            "rewound\n",
            "jimedorje\n",
            "rethinking\n",
            "breathtakingly\n",
            "idealizing\n",
            "brunt\n",
            "tightrope\n",
            "okayish\n",
            "hellopoetry\n",
            "3711178\n",
            "inebriation\n",
            "siren’s\n",
            "misery’s\n",
            "ca't\n",
            "matter❤️\n",
            "'self\n",
            "defense'\n",
            "'man\n",
            "programs'\n",
            "i''m\n",
            "furaffinity\n",
            "hoping'\n",
            "lartely\n",
            "bundy\n",
            "jumpy\n",
            "nnightmares\n",
            "fainted\n",
            "lumps\n",
            "howdy\n",
            "concerta\n",
            "strattera\n",
            "focalin\n",
            "🤍\n",
            "distain\n",
            "indulgent\n",
            "selflessness\n",
            "misfire\n",
            "🥂\n",
            "topirimate\n",
            "exhaseted\n",
            "unemployable\n",
            "labouring\n",
            "amitriptalyne\n",
            "730pm\n",
            "neverrrrr\n",
            "misophonia\n",
            "spasming\n",
            "orchestrate\n",
            "'kill\n",
            "depressed'\n",
            "'given\n",
            "10kgs\n",
            "120kgs\n",
            "concoction\n",
            "24hr\n",
            "stigmas\n",
            "50mins\n",
            "panict\n",
            "phobias\n",
            "theee\n",
            "spontaneuous\n",
            "erections\n",
            "homelife\n",
            "unempoyed\n",
            "3weeks\n",
            "neeeeed\n",
            "scumbags\n",
            "molest\n",
            "panazep\n",
            "etizolam\n",
            "bartending\n",
            "meaninless\n",
            "festers\n",
            "deferring\n",
            "“need\n",
            "provide”\n",
            "gcse\n",
            "lobotomised\n",
            "imploding\n",
            "ssdi\n",
            "jusg\n",
            "genious\n",
            "illustartion\n",
            "ruts\n",
            "iceburg\n",
            "successfull\n",
            "reincarnate\n",
            "iqs\n",
            "blunders\n",
            "obgyn\n",
            "toughing\n",
            "ween\n",
            "tailspin\n",
            "help 🙃\n",
            "lot ❤️❤️\n",
            "125mg\n",
            "sacrified\n",
            "obliviously\n",
            "personnality\n",
            "unsurprisingly\n",
            "volounteer\n",
            "phychiatrists\n",
            "potty\n",
            "mrdd\n",
            "unsanitary\n",
            "tantrums\n",
            "“mutually”\n",
            "rollies\n",
            "resteraunt\n",
            "buproprion\n",
            "romanticized\n",
            "overemphasize\n",
            "cynicism\n",
            "steer\n",
            "seams\n",
            "breakouts\n",
            "rhynauds\n",
            "13ish\n",
            "negitive\n",
            "assertiveness\n",
            "depersonalizing\n",
            "decissions\n",
            "depersonalisation\n",
            "overreaction\n",
            "trainwreck\n",
            "„diseases“\n",
            "excactly\n",
            "inaquality\n",
            "“psychologically“\n",
            "psychiastry\n",
            "what‘s\n",
            "can‘t\n",
            "responsable\n",
            "untenable\n",
            "„ok“\n",
            "dasha\n",
            "taran\n",
            "eilish‘s\n",
            "eden‘s\n",
            "🌼tw🌼\n",
            "head's\n",
            "close—\n",
            "prepped\n",
            "biiiittttcchhhhhhhhhhhhhhhhhhhhhh\n",
            "control’s\n",
            "soooooooooooo\n",
            "everyfuckingwhere\n",
            "🤬\n",
            "repressing\n",
            "goin'\n",
            "strangulation\n",
            "waded\n",
            "acdc\n",
            "timid\n",
            "pomodoro\n",
            "usual”\n",
            "gluttonous\n",
            "lessened\n",
            "simptoms\n",
            "grappling\n",
            "payoff\n",
            "infusions\n",
            "shkreli\n",
            "execs\n",
            "rubble\n",
            "tranxene\n",
            "diligently\n",
            "phenergan\n",
            "unmedicated\n",
            "dissappearing\n",
            "anixety\n",
            "hypersomnia\n",
            "hydrobromide\n",
            "closed—online\n",
            "semester—due\n",
            "potus\n",
            "cancelled—commencement\n",
            "arts—theatre\n",
            "music—department\n",
            "disproportionally\n",
            "nauseous—still\n",
            "trauma’s\n",
            "swath\n",
            "seniors—i\n",
            "love—fuck\n",
            "students—the\n",
            "home—are\n",
            "craziest\n",
            "😞🏳️‍🌈❤️\n",
            "briviact\n",
            "sincerest\n",
            "genuinity\n",
            "“want”\n",
            "unwillingness\n",
            "psychatrist\n",
            "citaploram\n",
            "witam\n",
            "nazywam\n",
            "dzisiaj\n",
            "będę\n",
            "strzelał\n",
            "wszystkich\n",
            "moim\n",
            "mieszkaniowym\n",
            "zabiję\n",
            "moją\n",
            "rodzinę\n",
            "bronią\n",
            "ponieważ\n",
            "jestem\n",
            "zmęczony\n",
            "zycezie\n",
            "zyziez\n",
            "złski\n",
            "instigate\n",
            "chaperon\n",
            "stalls\n",
            "napkin\n",
            "uite\n",
            "story's\n",
            "uestions\n",
            "backtracked\n",
            "diagnosis'\n",
            "imbeciles\n",
            "seuence\n",
            "theraphy\n",
            "parents—one\n",
            "parkinson’s\n",
            "point—the\n",
            "noticed—it\n",
            "life—can’t\n",
            "job”\n",
            "'hum'\n",
            "corvid\n",
            "ifs'\n",
            "bandaid\n",
            "“there’s\n",
            "congregated\n",
            "aspired\n",
            "knowledgably\n",
            "engrossed\n",
            "renewing\n",
            "ellobarote\n",
            "refferred\n",
            "recieving\n",
            "bodys\n",
            "gastritis\n",
            "ativan\n",
            "trazedone\n",
            "frisium\n",
            "clobazam\n",
            "college’s\n",
            "crosswalk\n",
            "flicker\n",
            "memos\n",
            "perverts\n",
            "bagging\n",
            "functionings\n",
            "oxcarbazepine\n",
            "uninsured\n",
            "clonazepan\n",
            "vpa\n",
            "“useless”\n",
            "waight\n",
            "chid\n",
            "shakra\n",
            "zhineng\n",
            "qigong\n",
            "sabotages\n",
            "boiz\n",
            "bloomers\n",
            "tomorrows\n",
            "repay\n",
            "exspecially\n",
            "fuckicn\n",
            "bugzy\n",
            "ugliness\n",
            "sameness\n",
            "overpowers\n",
            "subbreddit\n",
            "usseles\n",
            "trowhing\n",
            "pshyco\n",
            "hitt\n",
            "afther\n",
            "whit\n",
            "suppoused\n",
            "everythime\n",
            "alchochol\n",
            "sitts\n",
            "sitt\n",
            "disaproving\n",
            "hatted\n",
            "horribles\n",
            "sneek\n",
            "tumble\n",
            "effer\n",
            "devestated\n",
            "accentuated\n",
            "guilts\n",
            "recapping\n",
            "greedyness\n",
            "bryant’s\n",
            "week’s\n",
            "pmed\n",
            "desserving\n",
            "fertalizer\n",
            "crying”\n",
            "lessens\n",
            "exwife\n",
            "stoked\n",
            "stubbed\n",
            "disassociates\n",
            "dramaqueen\n",
            "teared\n",
            "happy…\n",
            "‘smart’\n",
            "‘the\n",
            "geniuses’\n",
            "‘no\n",
            "assistance’\n",
            "depresaiom\n",
            "evolves\n",
            "hideouts\n",
            "severing\n",
            "idolization\n",
            "completelt\n",
            "figments\n",
            "“save\n",
            "reared\n",
            "amursk\n",
            "principal's\n",
            "immeasurable\n",
            "sats\n",
            "tiem\n",
            "nebraska\n",
            "fearfullness\n",
            "cosmos\n",
            "scratchiness\n",
            "beets\n",
            "wellington\n",
            "tauranga\n",
            "400km\n",
            "unattached\n",
            "workmates\n",
            "rediscover\n",
            "backsliding\n",
            "disarray\n",
            "'equipment\n",
            "manager'\n",
            "cor''\n",
            "'regrets'\n",
            "confuzzled\n",
            "“stylish”\n",
            "diseased\n",
            "reborn\n",
            "chihuahuas\n",
            "wvery\n",
            "inconveniently\n",
            "hootie\n",
            "blowfish\n",
            "especionaly\n",
            "sadenu\n",
            "regularily\n",
            "awoke\n",
            "poetically\n",
            "bombardments\n",
            "differienciates\n",
            "‘being\n",
            "bitch’\n",
            "yooo\n",
            "copped\n",
            "serengeti\n",
            "palpitations\n",
            "impatiently\n",
            "shingle\n",
            "reincarnated\n",
            "coronavirus””i\n",
            "””oh\n",
            "corona’s\n",
            "here””she’s\n",
            "ugly””ewww””she’s\n",
            "smart”teachers\n",
            "pet”ect\n",
            "self””\n",
            "‘friend’\n",
            "pese\n",
            "feeels\n",
            "soooooooooo\n",
            "overeagerness\n",
            "trully\n",
            "thibg\n",
            "brightside\n",
            "yorkie\n",
            "nala\n",
            "preventer\n",
            "hauiqjshsaiajejej\n",
            "genuinly\n",
            "motionless\n",
            "precariously\n",
            "jenga\n",
            "gruelling\n",
            "entretainment\n",
            "17—reconnected\n",
            "😏\n",
            "city’s\n",
            "‘relaxing’\n",
            "🐰\n",
            "whimp\n",
            "“weak\n",
            "faggot”\n",
            "“gay”\n",
            "“faggy”\n",
            "whimpy\n",
            "week’\n",
            "rn’\n",
            "‘if\n",
            "this’\n",
            "bestfriend’s\n",
            "4guys\n",
            "litreally\n",
            "aborting\n",
            "foresee\n",
            "“progress”\n",
            "“working”\n",
            "cohabitate\n",
            "skeevy\n",
            "innappropriate\n",
            "hotline”\n",
            "gabby\n",
            "“party\n",
            "bus”\n",
            "ex’s\n",
            "wreaking\n",
            "reoccur\n",
            "numbbb\n",
            "indulging\n",
            "dont'\n",
            "400lbs\n",
            "245lbs\n",
            "😓\n",
            "tanked\n",
            "googles\n",
            "“rather\n",
            "project”\n",
            "“daydreams”\n",
            "“understanding”\n",
            "dtarted\n",
            "efford\n",
            "browsering\n",
            "enfected\n",
            "‘vacation\n",
            "gtfo\n",
            "pityparty\n",
            "fbcdn\n",
            "q65\n",
            "88212694\n",
            "2599118326880690\n",
            "8960531162304872448\n",
            "85a577\n",
            "efg\n",
            "eyjpijoidcj9\n",
            "8fozkxxecmkax9zztvf\n",
            "dee47367c41d683a322c0f3230cb2b6e\n",
            "5e926081\n",
            "handsanitizer\n",
            "nonrefundable\n",
            "👏\n",
            "songbirds\n",
            "permeates\n",
            "hour on\n",
            "lecturers\n",
            "banish\n",
            "celebratory\n",
            "again😭😭😭😭😭😭😔😔👉💔💔💔💔💔\n",
            "bossing\n",
            "saddens\n",
            "pointform\n",
            "8weeks\n",
            "3cg1\n",
            "colorectal\n",
            "gf's\n",
            "spousal\n",
            "persistant\n",
            "skimped\n",
            "preconceived\n",
            "percy\n",
            "deplete\n",
            "similair\n",
            "weirder\n",
            "100€\n",
            "130€\n",
            "300€\n",
            "150€\n",
            "400€\n",
            "incubating\n",
            "snooze\n",
            "subservient\n",
            "succesfully\n",
            "shaming\n",
            "misreads\n",
            "hypothesizes\n",
            "dilapidated\n",
            "indecisiveness\n",
            "coldly\n",
            "5years\n",
            "lowly\n",
            "intensionally\n",
            "chirpiness\n",
            "doveey\n",
            "missed”\n",
            "“fallen”\n",
            "sedatives\n",
            "who´s\n",
            "won´t\n",
            "adversities\n",
            "stagnates\n",
            "let´s\n",
            "aren´t\n",
            "cuck\n",
            "unimpressive\n",
            "artitude\n",
            "wrighting\n",
            "wrighten\n",
            "mounths\n",
            "inactions\n",
            "retriggered\n",
            "creatine\n",
            "“religious”\n",
            "‘real’\n",
            "“conservative”\n",
            "classist\n",
            "mindfuck\n",
            "sharpening\n",
            "clog\n",
            "supervisors\n",
            "makw\n",
            "machinist\n",
            "immersively\n",
            "spiritually\n",
            "booz\n",
            "miniscule\n",
            "hallucinogenic\n",
            "robowalking\n",
            "heartrate\n",
            "supped\n",
            "dished\n",
            "regatta\n",
            "mondays\n",
            "wednesdays\n",
            "everygirl\n",
            "woulda\n",
            "ican\n",
            "thanksbrain\n",
            "endogenous\n",
            "gooder\n",
            "patronising\n",
            "understaffed\n",
            "ihop\n",
            "car's\n",
            "unsecure\n",
            "straggler\n",
            "trainers\n",
            "hanf\n",
            "þe\n",
            "banter\n",
            "rhe\n",
            "coronovirus\n",
            "gretchen\n",
            "hospitalises\n",
            "unheathly\n",
            "mistreat\n",
            "“crash”\n",
            "garbo\n",
            "habbibi\n",
            "ozzy\n",
            "osbourne\n",
            "harsher\n",
            "heige\n",
            "seil\n",
            "banished\n",
            "homecoming\n",
            "nerf\n",
            "andreas\n",
            "dewine\n",
            "mashed\n",
            "overqualified\n",
            "‘recovered’\n",
            "inkling\n",
            "everythinh\n",
            "ducking\n",
            "rockier\n",
            "scammer\n",
            "lierally\n",
            "“you”\n",
            "rat’s\n",
            "nastiest\n",
            "probs\n",
            "'picture\n",
            "girl'\n",
            "pedastool\n",
            "f17\n",
            "foreward\n",
            "thinigs\n",
            "suddently\n",
            "dissappear\n",
            "shit's\n",
            "crayons\n",
            "scrapes\n",
            "babysitter's\n",
            "illinois\n",
            "standoffish\n",
            "exomony\n",
            "creepily\n",
            "“let\n",
            "control”\n",
            "stench\n",
            "😫\n",
            "awry\n",
            "flailing\n",
            "uhhhhhhh\n",
            "unannounced\n",
            "demotivation\n",
            "🤞🏽\n",
            "stuggle\n",
            "tryn\n",
            "‘cured’\n",
            "nashville\n",
            "wotld\n",
            "flase\n",
            "“manipulative”\n",
            "pease\n",
            "suboxone\n",
            "“you’ll\n",
            "greyhound”\n",
            "retriever\n",
            "university’s\n",
            "jnow\n",
            "highshcool\n",
            "lagged\n",
            "’m\n",
            "“popular”\n",
            "friends😂\n",
            "alchool\n",
            "piggy\n",
            "fucko'\n",
            "zzzquil\n",
            "condescended\n",
            "salads\n",
            "hush\n",
            "congratulating\n",
            "burrito\n",
            "i’vs\n",
            "planks\n",
            "mattress’s\n",
            "jag\n",
            "enticing\n",
            "“forget”\n",
            "becoz\n",
            "nigger\n",
            "68m\n",
            "74m\n",
            "dabbling\n",
            "tramadol\n",
            "distrust\n",
            "wrenched\n",
            "jakfmfmgksi\n",
            "thole\n",
            "flared\n",
            "byee\n",
            "deja\n",
            "txt'd\n",
            "ad's\n",
            "ej\n",
            "s5rjt\n",
            "feeble\n",
            "firerocket\n",
            "zits\n",
            "sedentary\n",
            "upticked\n",
            "incrementally\n",
            "disvracefully\n",
            "ieave\n",
            "erge\n",
            "thoes\n",
            "nobodys\n",
            "“vice”\n",
            "disorganization\n",
            "4years\n",
            "firsth\n",
            "liv3\n",
            "remarrying\n",
            "son’s\n",
            "fanarts\n",
            "likers\n",
            "commiss\n",
            "aidan\n",
            "12”\n",
            "freak”\n",
            "canberra\n",
            "nikita’s\n",
            "lote\n",
            "clump\n",
            "sharemarket\n",
            "“choice”\n",
            "€30\n",
            "solicitor\n",
            "€3000\n",
            "digger\n",
            "obession\n",
            "excistance\n",
            "affraid\n",
            "humilty\n",
            "feelinges\n",
            "shoudn't\n",
            "forgoten\n",
            "desytroy\n",
            "doings\n",
            "perhapse\n",
            "goody\n",
            "gettening\n",
            "invovle\n",
            "savor\n",
            "traumatise\n",
            "avicii\n",
            "😘\n",
            "couldn‘t\n",
            "doesn‘t\n",
            "didn‘t\n",
            "deescalate\n",
            "liying\n",
            "it♡\n",
            "k’n\n",
            "❤️😔\n",
            "homesick\n",
            "shitloads\n",
            "7yrs\n",
            "4yrs\n",
            "thumbing\n",
            "loudest\n",
            "blubbering\n",
            "realisations\n",
            "strong”\n",
            "tocking\n",
            "boyfriend’s\n",
            "idly\n",
            "exterminate\n",
            "rudest\n",
            "rhythmically\n",
            "contently\n",
            "else—\n",
            "suprisingly\n",
            "oddysey\n",
            "peacefuly\n",
            "sucididal\n",
            "0's\n",
            "“reach\n",
            "somebody”\n",
            "“brutal\n",
            "truth”\n",
            "catoptrophobia\n",
            "💕\n",
            "whtas\n",
            "lame''\n",
            "uncurable\n",
            "m21\n",
            "intrests\n",
            "memorabiallia\n",
            "upsmanship\n",
            "xenophobic\n",
            "'me\n",
            "first'\n",
            "schmacked\n",
            "jeepney\n",
            "scratcher\n",
            "wailing\n",
            "courted\n",
            "execuse\n",
            "“luxury\n",
            "1br”\n",
            "“loves\n",
            "everywhere”\n",
            "waitts\n",
            "chime\n",
            "ammount\n",
            "😓😓\n",
            "🖤🖤🖤🖤\n",
            "councilors\n",
            "sturdy\n",
            "emphathy\n",
            "cleary\n",
            "biengs\n",
            "realisty\n",
            "ek0sabh\n",
            "9os\n",
            "“ready”\n",
            "1½\n",
            "smalltalk\n",
            "tidbit\n",
            "obsolutly\n",
            "daysss\n",
            "21yo\n",
            "reflexion\n",
            "“it’ll\n",
            "sorrys\n",
            "turture\n",
            "snapt\n",
            "episonde\n",
            "haved\n",
            "havend\n",
            "frome\n",
            "normaly\n",
            "wast\n",
            "dident\n",
            "struggeling\n",
            "persone\n",
            "anoy\n",
            "anoying\n",
            "anoyed\n",
            "horible\n",
            "socal\n",
            "clima\n",
            "foult\n",
            "weeked\n",
            "stopp\n",
            "depressiv´´\n",
            "onley\n",
            "earlyer\n",
            "exakly\n",
            "mabey\n",
            "depressiv\n",
            "litte\n",
            "wile\n",
            "laughting\n",
            "witheout\n",
            "cornelian\n",
            "endrant\n",
            "human”\n",
            "“ohh\n",
            "benefits”\n",
            "motiving\n",
            "slithering\n",
            "arounnd\n",
            "wouldn't´t\n",
            "isn´t\n",
            "autist\n",
            "whitewashed\n",
            "minecrafting\n",
            "postmates\n",
            "convinient\n",
            "intoduced\n",
            "frivolous\n",
            "persued\n",
            "entropic\n",
            "quintessentially\n",
            "ensue\n",
            "ballooned\n",
            "floaty\n",
            "myeslf\n",
            "“intellectual”\n",
            "obstinately\n",
            "euthanize\n",
            "lathered\n",
            "disposing\n",
            "anything—leave\n",
            "limp\n",
            "stucked\n",
            "ventilating\n",
            "subcame\n",
            "battering\n",
            "secondarys\n",
            "outcasted\n",
            "shoulda\n",
            "atone\n",
            "screech\n",
            "unfriended\n",
            "honeslty\n",
            "themeselves\n",
            "bohoo\n",
            "misersable\n",
            "nevermore\n",
            "ehy\n",
            "idipt\n",
            "idito\n",
            "idotk\n",
            "ndjdjdndbfbdbfvhd\n",
            "ingnored\n",
            "lonleny\n",
            "empy\n",
            "brokenness\n",
            "30yo\n",
            "rsd\n",
            "💔\n",
            "partum\n",
            "uptick\n",
            "refocus\n",
            "uncharacteristically\n",
            "outed\n",
            "quivering\n",
            "reengage\n",
            "finicky\n",
            "newfound\n",
            "now's\n",
            "unsatisfyingly\n",
            "8hrs\n",
            "5hrs\n",
            "6hrs\n",
            "loosened\n",
            "reengaging\n",
            "reframing\n",
            "reframed\n",
            "flipside\n",
            "facemask\n",
            "loony\n",
            "upticks\n",
            "trafficked\n",
            "'safe\n",
            "assume'\n",
            "clincally\n",
            "furlough\n",
            "serval\n",
            "severally\n",
            "everyone”\n",
            "spencer\n",
            "else”\n",
            "“nooo\n",
            "chaing\n",
            "sweatshirts\n",
            "nonbinary\n",
            "afab\n",
            "blindside\n",
            "“lack\n",
            "interest”\n",
            "uglier\n",
            "beethoven\n",
            "empties\n",
            "typa\n",
            "praeder\n",
            "willie\n",
            "checkmate\n",
            "negates\n",
            "‘well\n",
            "trying’\n",
            "shit'\n",
            "40’s\n",
            "anyways”\n",
            "n64\n",
            "ps3\n",
            "cheeriest\n",
            "trudge\n",
            "tenuous\n",
            "overprotective\n",
            "misbehaved\n",
            "shitton\n",
            "backstabbed\n",
            "irrelevancy\n",
            "herculean\n",
            "anobody\n",
            "inlesses\n",
            "introvertism\n",
            "paranoic\n",
            "pottencial\n",
            "hounts\n",
            "unbareble\n",
            "forat\n",
            "emptieness\n",
            "overexaggerateing\n",
            "lout\n",
            "shouldt\n",
            "“male”\n",
            "“verbal\n",
            "reasoning”\n",
            "“male\n",
            "typical”\n",
            "brazillians\n",
            "yknow\n",
            "ya’ll\n",
            "banger\n",
            "skyrocketed\n",
            "outgrown\n",
            "fuuuckk\n",
            "uugh\n",
            "actualized\n",
            "idea”\n",
            "dishonorable\n",
            "monhts\n",
            "alying\n",
            "suicde\n",
            "tomy\n",
            "enthusastic\n",
            "alternitve\n",
            "brattiest\n",
            "“introvert”\n",
            "sloppily\n",
            "fuckibg\n",
            "theyr\n",
            "experinced\n",
            "bonfire\n",
            "thirdly\n",
            "overcompensation\n",
            "'sadness'\n",
            "'relationship'\n",
            "'gender\n",
            "dysphoria'\n",
            "couse\n",
            "charlatans\n",
            "saddening\n",
            "pussying\n",
            "quaratine\n",
            "minnesota\n",
            "🙂\n",
            "oftenly\n",
            "“feel”\n",
            "“feeling”\n",
            "“ups”\n",
            "quesrions\n",
            "v2k\n",
            "'v2k'\n",
            "'chat\n",
            "bot'\n",
            "“attend”\n",
            "paused\n",
            "’and\n",
            "donf\n",
            "sikh\n",
            "cumulating\n",
            "fines\n",
            "unsusesfully\n",
            "retreated\n",
            "foreboding\n",
            "raleigh\n",
            "swat\n",
            "cuffs\n",
            "swatted\n",
            "1x4\n",
            "indubitably\n",
            "entrapped\n",
            "caseworker\n",
            "track’\n",
            "might’ve\n",
            "‘5\n",
            "boris\n",
            "calstates\n",
            "coroshit\n",
            "postponing\n",
            "postapocalyptic\n",
            "normalising\n",
            "mcat\n",
            "finical\n",
            "“hi\n",
            "pectus\n",
            "excavatum\n",
            "quarentine\n",
            "agistment\n",
            "course's\n",
            "godsend\n",
            "eyelash\n",
            "noucense\n",
            "tf2\n",
            "excersize\n",
            "misdemeanor\n",
            "likelyhood\n",
            "stupid”\n",
            "⚠️tw\n",
            "——\n",
            "⚠️\n",
            "“it”\n",
            "limerent\n",
            "untwisted\n",
            "coldest\n",
            "i'mma\n",
            "christion\n",
            "psychos\n",
            "cricked\n",
            "goosebumps\n",
            "pestering\n",
            "chunkier\n",
            "barretta\n",
            "haunekah\n",
            "'final\n",
            "puzzle'\n",
            "'mr\n",
            "right'\n",
            "'choosen\n",
            "'comfort'\n",
            "'compassion'\n",
            "uneventful\n",
            "averagely\n",
            "other'\n",
            "detestable\n",
            "altough\n",
            "consindering\n",
            "serveral\n",
            "issus\n",
            "emptyness\n",
            "motivationless\n",
            "preassure\n",
            "concoct\n",
            "how’d\n",
            "much”\n",
            "kneeled\n",
            "“digestion\n",
            "“nauseous”\n",
            "“laziness”\n",
            "dietitian\n",
            "fckton\n",
            "attemped\n",
            "banister\n",
            "legitimise\n",
            "fiancée\n",
            "snowballs\n",
            "regurgitations\n",
            "should't\n",
            "dually\n",
            "ruptured\n",
            "20min\n",
            "“ping\n",
            "ponged”\n",
            "1000s\n",
            "“healthy”\n",
            "drudged\n",
            "procrastinator's\n",
            "recklessness\n",
            "dreaming—or\n",
            "dead—\n",
            "callow\n",
            "summery\n",
            "hero’s\n",
            "corroded\n",
            "fascination\n",
            "unforgiven\n",
            "i—my\n",
            "selfishly\n",
            "little—like\n",
            "blissfuly\n",
            "ridiculousness\n",
            "cliff’s\n",
            "sweetly\n",
            "—————————————————————————\n",
            "“you’ve\n",
            "moodier\n",
            "grandparents’\n",
            "vibes”\n",
            "frelings\n",
            "perforation\n",
            "eardrum\n",
            "thoses\n",
            "”no\n",
            "not”\n",
            "jackshit\n",
            "something”\n",
            "thread”\n",
            "woop\n",
            "5”\n",
            "morrrreeeeeee\n",
            "xs”\n",
            "spewed\n",
            "interesst\n",
            "interacting”\n",
            "distresses\n",
            "absourdism\n",
            "inyected\n",
            "anhedonhia\n",
            "probaly\n",
            "litlle\n",
            "abour\n",
            "troublemaker\n",
            "dork\n",
            "smill\n",
            "placebos\n",
            "embarrassments\n",
            "qutiapine\n",
            "gatekeep\n",
            "divorces\n",
            "etcetc\n",
            "irreparably\n",
            "biding\n",
            "drudgery\n",
            "misfit\n",
            "platitudinous\n",
            "consciousnesses\n",
            "crossroads\n",
            "athough\n",
            "overshadow\n",
            "donald's\n",
            "arsed\n",
            "attually\n",
            "couping\n",
            "illness”\n",
            "👉😎👉\n",
            "backer\n",
            "scar's\n",
            "nightshift\n",
            "repulsed\n",
            "are”\n",
            "‘go\n",
            "out’\n",
            "400mg\n",
            "“pretty”\n",
            "haggard\n",
            "negligent\n",
            "quiver\n",
            "ofmy\n",
            "'you're\n",
            "squidward\n",
            "spongebob\n",
            "videocall\n",
            "lifesaver\n",
            "child”\n",
            "“hope\n",
            "future”\n",
            "darken\n",
            "abandonned\n",
            "jackman\n",
            "spurts\n",
            "sprinkled\n",
            "hest\n",
            "emancipated\n",
            "“funny\n",
            "“severely\n",
            "diagonsed\n",
            "lers\n",
            "empthay\n",
            "'happiness'\n",
            "lingered\n",
            "“hahaha”\n",
            "alleviating\n",
            "otp\n",
            "phoniex\n",
            "ariziona\n",
            "can'\n",
            "alonso\n",
            "uncomfortably\n",
            "jittery\n",
            "why’d\n",
            "hardplace\n",
            "dramatize\n",
            "halle\n",
            "badplace\n",
            "exaggerates\n",
            "thing’s\n",
            "excelent\n",
            "overwhelms\n",
            "skyrocket\n",
            "viciously\n",
            "handyman\n",
            "electriction\n",
            "sonething\n",
            "scuicide\n",
            "disapointed\n",
            "reapply\n",
            "bumfuck\n",
            "blisters\n",
            "supossed\n",
            "problesm\n",
            "growning\n",
            "completlty\n",
            "rhis\n",
            "fck\n",
            "siiiigghhhh\n",
            "zippo\n",
            "planed\n",
            "havn't\n",
            "“game\n",
            "over”\n",
            "battening\n",
            "overtake\n",
            "emoninal\n",
            "fumbling\n",
            "mean't\n",
            "bicker\n",
            "swirling\n",
            "get's\n",
            "24hrs\n",
            "’if\n",
            "gruff\n",
            "’i\n",
            "years’\n",
            "nparents\n",
            "'listen\n",
            "understand'\n",
            "faithfull\n",
            "dude's\n",
            "dingey\n",
            "neurotically\n",
            "posts”\n",
            "“depression\n",
            "trend”\n",
            "greather\n",
            "pribably\n",
            "“nonessential”\n",
            "practically—i\n",
            "selfcare\n",
            "92ibs\n",
            "42kg\n",
            "pancake\n",
            "babyface\n",
            "frankenstein\n",
            "certian\n",
            "100x\n",
            "amiss\n",
            "placement's\n",
            "isn't'\n",
            "unmatch\n",
            "quarantinage\n",
            "corporati\n",
            "batshittery\n",
            "befpre\n",
            "oxygenated\n",
            "anywyas\n",
            "patricks's\n",
            "pobably\n",
            "workaholics\n",
            "desensitization\n",
            "blinked\n",
            "that’ve\n",
            "deperate\n",
            "opertunity\n",
            "droopy\n",
            "harshest\n",
            "cartalage\n",
            "maxillary\n",
            "distractor\n",
            "unnessesary\n",
            "promiss\n",
            "16k\n",
            "promis\n",
            "asvap\n",
            "smarts\n",
            "astigmatism\n",
            "extream\n",
            "savire\n",
            "lasik\n",
            "halarious\n",
            "circomsition\n",
            "grandma's\n",
            "musican\n",
            "plummited\n",
            "simaltaniously\n",
            "grandpa's\n",
            "granpa\n",
            "oppisate\n",
            "pristine\n",
            "extreamly\n",
            "peaple\n",
            "cynic\n",
            "‘ok’ness\n",
            "courthouse\n",
            "felts\n",
            "firework\n",
            "shread\n",
            "souldnt\n",
            "5’7”\n",
            "granddad\n",
            "'living\n",
            "grey'\n",
            "'dark\n",
            "thoughts'\n",
            "tragically\n",
            "whet\n",
            "overflood\n",
            "skydiver\n",
            "chubbier\n",
            "laziest\n",
            "sweaters\n",
            "handguns\n",
            "“everyone’s\n",
            "boys”\n",
            "costed\n",
            "irate\n",
            "comprehends\n",
            "quantify\n",
            "compensating\n",
            "shamelessly\n",
            "shamelessness\n",
            "10yrs\n",
            "acuse\n",
            "trod\n",
            "antagonistic\n",
            "ambitiously\n",
            "meddling\n",
            "whiteout\n",
            "clusterfuck\n",
            "driveling\n",
            "'submit\n",
            "depression'\n",
            "tipp\n",
            "throght\n",
            "thoghts\n",
            "“hurt\n",
            "self”\n",
            "“bonded”\n",
            "thrusted\n",
            "iud\n",
            "penetrative\n",
            "breakfest\n",
            "internett\n",
            "no1\n",
            "dissopointment\n",
            "cruelest\n",
            "xplain\n",
            "garnishing\n",
            "peon\n",
            "eeughh\n",
            "“free”\n",
            "deluding\n",
            "ecetened\n",
            "lugging\n",
            "colorfully\n",
            "boatload\n",
            "philly\n",
            "“accept”\n",
            "cliches\n",
            "sketchbook\n",
            "pencilstroke\n",
            "shimmer\n",
            "losen\n",
            "inferno\n",
            "rocked\n",
            "gamestop\n",
            "oartners\n",
            "“was”\n",
            "5'1\n",
            "gettin'\n",
            "lettin'\n",
            "slittin'\n",
            "deceit\n",
            "156wpm\n",
            "tiptoes\n",
            "5'11\n",
            "linebacker\n",
            "coooool\n",
            "clowned\n",
            "forearms\n",
            "ebbed\n",
            "bachelor’s\n",
            "soonest\n",
            "innconvience\n",
            "there💀\n",
            "gloat\n",
            "screamjnf\n",
            "fuxking\n",
            "“realizations\n",
            "fuckjnf\n",
            "lol”\n",
            "“kookie”\n",
            "sketty\n",
            "clinicals\n",
            "shacking\n",
            "vomity\n",
            "gourmet\n",
            "jovial\n",
            "vibrancy\n",
            "restfully\n",
            "restful\n",
            "alfoat\n",
            "litterley\n",
            "whith\n",
            "unesential\n",
            "woozy\n",
            "akward\n",
            "persisting\n",
            "poptarts\n",
            "nudging\n",
            "blackmails\n",
            "14hours\n",
            "grit\n",
            "unluckiest\n",
            "mnetal\n",
            "backburner\n",
            "hassling\n",
            "gravitates\n",
            "uncared\n",
            "playin\n",
            "ingrains\n",
            "jsut\n",
            "cartoonish\n",
            "rambles\n",
            "intimidates\n",
            "theworst\n",
            "commitsuicide\n",
            "webtoon\n",
            "shot”\n",
            "'normal\n",
            "stuff'\n",
            "coffe\n",
            "glimpsing\n",
            "earfull\n",
            "gettting\n",
            "yezees\n",
            "blaring\n",
            "mange\n",
            "oooh\n",
            "polycystic\n",
            "unsympathetic\n",
            "wringer\n",
            "suposed\n",
            "resurface\n",
            "backstab\n",
            "uselessnes\n",
            "nagative\n",
            "toughts\n",
            "drungs\n",
            "discusting\n",
            "antidepressions\n",
            "shoud\n",
            "hillarious\n",
            "happed\n",
            "acutally\n",
            "nightclubs\n",
            "viking\n",
            "examiner\n",
            "councilling\n",
            "3's\n",
            "ciao\n",
            "jusy\n",
            "smacked\n",
            "blindsighted\n",
            "hallow\n",
            "eas\n",
            "panadol\n",
            "advzice\n",
            "nahhh\n",
            "helicoptering\n",
            "dinnertime\n",
            "dutton\n",
            "volkswagen\n",
            "passat\n",
            "2l\n",
            "infotainment\n",
            "premiums\n",
            "vw\n",
            "seuss\n",
            "baited\n",
            "dm's\n",
            "ticks\n",
            "doin'\n",
            "„that's\n",
            "bleeds\n",
            "sociopathic\n",
            "psychologist'\n",
            "colds\n",
            "softest\n",
            "narssistic\n",
            "overtakes\n",
            "kmkw\n",
            "eath\n",
            "abilites\n",
            "middleground\n",
            "billionaire’s\n",
            "dayly\n",
            "harassement\n",
            "meantioned\n",
            "basicaly\n",
            "whuch\n",
            "connoisseur\n",
            "morose\n",
            "“bracing\n",
            "iooked\n",
            "acclimatized\n",
            "uneasiness\n",
            "unlearn\n",
            "warping\n",
            "senselessly\n",
            "fruition\n",
            "refect\n",
            "wayyyy\n",
            "infects\n",
            "criticising\n",
            "sporty\n",
            "maggot\n",
            "ocassionally\n",
            "mtself\n",
            "althrough\n",
            "knwo\n",
            "iive\n",
            "wolud\n",
            "pervading\n",
            "screwy\n",
            "ya'll\n",
            "asthenia\n",
            "soundless\n",
            "shatters\n",
            "coldness\n",
            "undying\n",
            "lmthis\n",
            "emotionality\n",
            "repress\n",
            "bottom”\n",
            "flunks\n",
            "mopey\n",
            "devilish\n",
            "googled\n",
            "vomited\n",
            "pyshically\n",
            "boybestfriend\n",
            "fumble\n",
            "felonies\n",
            "busses\n",
            "moodiness\n",
            "ersonally\n",
            "misread\n",
            "hollistically\n",
            "“mind\n",
            "blowing”\n",
            "claptons\n",
            "cocain\n",
            "anasthesia\n",
            "feely\n",
            "responsabilities\n",
            "apprehensively\n",
            "‘pregnant’\n",
            "nervously\n",
            "lancaster\n",
            "infirmary\n",
            "sonographer\n",
            "partner’s\n",
            "sorry’\n",
            "grievance\n",
            "encephalocele\n",
            "crematorium\n",
            "tigerlily\n",
            "bereavement\n",
            "stillbirth\n",
            "neonatal\n",
            "healer\n",
            "just…i\n",
            " how\n",
            "“third\n",
            " they\n",
            " when\n",
            " ever\n",
            " haven’t\n",
            " even\n",
            "resigning\n",
            " better\n",
            " so\n",
            "time…he\n",
            " we’re\n",
            " he\n",
            " we’ll\n",
            "does…\n",
            "me…\n",
            "her…\n",
            " money\n",
            " always\n",
            "work…she\n",
            " she\n",
            " naïve\n",
            " in\n",
            "earnestly\n",
            " i’m\n",
            " surely\n",
            "unexplicable\n",
            " it’s\n",
            " instead\n",
            " part\n",
            " stop\n",
            " family\n",
            "spirituality…that\n",
            "unjustly\n",
            " please\n",
            " take\n",
            "2009ish\n",
            "airy\n",
            "dxed\n",
            "oskar\n",
            "misstep\n",
            "pulsates\n",
            "breathing—our nachlass in\n",
            "ledgers\n",
            "unlived\n",
            "unclaimed\n",
            "cloakroom\n",
            "imag9ning\n",
            "exert\n",
            "emphty\n",
            "lyie\n",
            "soelmeone\n",
            "ambitionless\n",
            "longggg\n",
            "lottt\n",
            "reallly\n",
            "yeaaaaaa\n",
            "ambtionless\n",
            "date”\n",
            "ckin\n",
            "sheeet\n",
            "pharmas\n",
            "lioe\n",
            "repulsively\n",
            "stranger's\n",
            "59f\n",
            "tiptoe\n",
            "“grade”\n",
            "pleaser\n",
            "absences\n",
            "whooole\n",
            "profusely\n",
            "snot\n",
            "augmentin\n",
            "absences”\n",
            "pass”\n",
            "“and\n",
            "dumpsters\n",
            "draw”\n",
            "stocker\n",
            "scabbed\n",
            "dating'\n",
            "'introvert'\n",
            "aound\n",
            "'guys'\n",
            "me😥\n",
            "'gettting'\n",
            "''cured''\n",
            "unbeknown\n",
            "faker\n",
            "ol'\n",
            "strugle\n",
            "braked\n",
            "“hey”\n",
            "childishly”\n",
            "robux\n",
            "friended\n",
            "comited\n",
            "atention\n",
            "duesorder\n",
            "lym\n",
            "froends\n",
            "unconsistant\n",
            "likr\n",
            "beleive\n",
            "slowy\n",
            "trued\n",
            "chapped\n",
            "paremts\n",
            "thisnmales\n",
            "myslef\n",
            "indid\n",
            "300ml\n",
            "smidge\n",
            "disapproval\n",
            "anonimus\n",
            "inportant\n",
            "avarge\n",
            "runing\n",
            "drinked\n",
            "reapeat\n",
            "taked\n",
            "autostop\n",
            "simester\n",
            "autostoping\n",
            "profesional\n",
            "enouth\n",
            "studing\n",
            "registrated\n",
            "manige\n",
            "profesors\n",
            "haircloth\n",
            "actualy\n",
            "95kg\n",
            "115kg\n",
            "160kg\n",
            "surivive\n",
            "gona\n",
            "inposible\n",
            "29yo\n",
            "abojt\n",
            "chiming\n",
            "liek\n",
            "flung\n",
            "wann\n",
            "downvotes\n",
            "distended\n",
            "malnourished\n",
            "languish\n",
            "reprieve\n",
            "clinched\n",
            "defibrillator\n",
            "collarbone\n",
            "dimly\n",
            "foresight\n",
            "tyson\n",
            "recents\n",
            "😭😭\n",
            "slink\n",
            "dismisses\n",
            "overworking\n",
            "“solitary\n",
            "confinement”\n",
            "£5\n",
            "camaraderie\n",
            "£500\n",
            "now”\n",
            "yikes”\n",
            "awful”\n",
            "“blame”\n",
            "“talk\n",
            "parents”\n",
            "slithers\n",
            "hasten\n",
            "starless\n",
            "constnatly\n",
            "checkign\n",
            "fuckwad\n",
            "forsee\n",
            "characterize\n",
            "45yo\n",
            "immigrate\n",
            "grapevine\n",
            "thrombosis\n",
            "curently\n",
            "😣\n",
            "90’s\n",
            "belted\n",
            "puppers\n",
            "webster’s\n",
            "forbearance\n",
            "trash”\n",
            "air”\n",
            "descends\n",
            "detractor\n",
            "daemons\n",
            "somwhere\n",
            "hands¿\n",
            "unskilled\n",
            "fruitlessly\n",
            "'losers'\n",
            "'make\n",
            "austistic\n",
            "mindsets\n",
            "'cos\n",
            "undermining\n",
            "vyvvance\n",
            "fastening\n",
            "shakiness\n",
            "fidgety\n",
            "eczema\n",
            "sadist\n",
            "electing\n",
            "workings—\n",
            "‘overly\n",
            "will’—\n",
            "hornier\n",
            "their's\n",
            "“depression”\n",
            "minneapolis\n",
            "wisconsin\n",
            "hesitantly\n",
            "goodmorning\n",
            "scorpio\n",
            "fondle\n",
            "eacothers\n",
            "physicality\n",
            "feeland\n",
            "alreadu\n",
            "knarsan\n",
            "biggie\n",
            "smalls\n",
            "gorge's\n",
            "tunneled\n",
            "shaggy\n",
            "anesthesia\n",
            "baby's\n",
            "decorating\n",
            "refinished\n",
            "mountains—hiking\n",
            "zelda\n",
            "mario\n",
            "“dont\n",
            "medicine”\n",
            "“act\n",
            "diont\n",
            "unforeseen\n",
            "sequester\n",
            "glazes\n",
            "knack\n",
            "scrutinize\n",
            "familys\n",
            "physcopath\n",
            "percocet\n",
            "“insane\n",
            "madeout\n",
            "how've\n",
            "christian's\n",
            "dugout\n",
            "5'8\n",
            "5'3ish\n",
            "buddy's\n",
            "indecision\n",
            "undercooking\n",
            "droning\n",
            "uuuuuuhhhhhhhhh\n",
            "subconsciousness\n",
            "mastering\n",
            "grafic\n",
            "dunkin\n",
            "overeat\n",
            "oversleep\n",
            "freeloader\n",
            "memorise\n",
            "supressing\n",
            "browses\n",
            "exasperates\n",
            "theast\n",
            "hellscape\n",
            "dimmed\n",
            "mortifying\n",
            "myaelf\n",
            "tom's\n",
            "goodluck\n",
            "stonewalling\n",
            "hairdresser\n",
            "cochlear\n",
            "cuboards\n",
            "fent\n",
            "megalomaniac\n",
            "resale\n",
            "redeemable\n",
            "mulch\n",
            "maternity\n",
            "casted\n",
            "“hold\n",
            "flickering\n",
            "“relentless\n",
            "faith”\n",
            "“immaculate\n",
            "captivated\n",
            "lapses\n",
            "coversations\n",
            "sabatoge\n",
            "mear\n",
            "othee\n",
            "therea\n",
            "malibu\n",
            "micromanager\n",
            "headstrong\n",
            "unapologetic\n",
            "absentmindedness\n",
            "depakote\n",
            "divalproex\n",
            "workday\n",
            "obliterate\n",
            "criterias\n",
            "frenetically\n",
            "doggies\n",
            "scamper\n",
            "action”\n",
            "“instead\n",
            "mindset”\n",
            "abhorrent\n",
            "extravaganza\n",
            "“didn’t\n",
            "veggie\n",
            "✌🏾\n",
            "hsv1\n",
            "m13\n",
            "misfits\n",
            "eardrums\n",
            "perforations\n",
            "sicknote\n",
            "berating\n",
            "boarder\n",
            "shushing\n",
            "skirting\n",
            "rescind\n",
            "‘shout\n",
            "hardline\n",
            "ratchets\n",
            "congested\n",
            "mothefucker\n",
            "metta\n",
            "skims\n",
            "goingg\n",
            "stubbornness\n",
            "chests\n",
            "subsides\n",
            "replays\n",
            "¨i\n",
            "happy¨\n",
            "¨did\n",
            "´´no\n",
            "it´´\n",
            "quintenvanommen\n",
            "quinten\n",
            "ommen\n",
            "chill”\n",
            "“most\n",
            "weed”\n",
            "atomized\n",
            "innocuous\n",
            "premonition\n",
            "internalizing\n",
            "alleviates\n",
            "capstone\n",
            "bailout\n",
            "bailouts\n",
            "i’s\n",
            "enabler\n",
            "teleport\n",
            "fallin\n",
            "onside\n",
            "off—no\n",
            "dampens\n",
            "flopped\n",
            "reguarly\n",
            "despire\n",
            "relationing\n",
            "foresaw\n",
            "fanstasies\n",
            "masochistic\n",
            "unpure\n",
            "idolized\n",
            "pushover\n",
            "hundredth\n",
            "distructive\n",
            "moisturiser\n",
            "sturggling\n",
            "baguettes\n",
            "inspectors\n",
            "starches\n",
            "fafsa\n",
            "name's\n",
            "punctured\n",
            "thrigger\n",
            "ideia\n",
            "hurst\n",
            "waning\n",
            "swishes\n",
            "heccin\n",
            "douchebags\n",
            "racists\n",
            "they´re\n",
            "doesn´t\n",
            "knowning\n",
            "weeeeelll\n",
            "throught\n",
            "paranoidc\n",
            "nini\n",
            "treatable\n",
            "shallowest\n",
            "well…\n",
            "home…\n",
            "y’know\n",
            "sitcomy\n",
            "funky\n",
            "blossoming\n",
            "hosue\n",
            "‘standard’\n",
            "messier\n",
            "brightens\n",
            "“hooked\n",
            "''real''\n",
            "''just\n",
            "relationship''\n",
            "frikkin\n",
            "‘buy\n",
            "trucks’\n",
            "pangs\n",
            "purposelessness\n",
            "disobeyed\n",
            "nosey\n",
            "that—my\n",
            "bff's\n",
            "up—call\n",
            "betraying\n",
            "voicemails\n",
            "up—which\n",
            "preschools\n",
            "scremead\n",
            "builied\n",
            "fixated\n",
            "classifying\n",
            "preformance\n",
            "haywire\n",
            "listlessly\n",
            "materialise\n",
            "neurofeedback\n",
            "dysregulation\n",
            "brainwaves\n",
            "“games”\n",
            "racecar\n",
            "barged\n",
            "beingless\n",
            "reminisce\n",
            "footstep\n",
            "supernarket\n",
            "knodded\n",
            "reheat\n",
            "thet\n",
            "churchmate\n",
            "desperates\n",
            "conveniences\n",
            "aprtment\n",
            "insided\n",
            "2x4\n",
            "suppossed\n",
            "hubg\n",
            "leashed\n",
            "stair\n",
            "slippe\n",
            "arressted\n",
            "stow\n",
            "haopy\n",
            "midfulness\n",
            "gettibg\n",
            "muxh\n",
            "does'nt\n",
            "seratonin\n",
            "comradeship\n",
            "gambler\n",
            "godforsaken\n",
            "whereabouts\n",
            "burrow\n",
            "dreary\n",
            "longed\n",
            "𝘵𝘳𝘺\n",
            "‘depression\n",
            "realer\n",
            "trivialise\n",
            "uselessly\n",
            "fuxk\n",
            "dee's\n",
            "joblessness\n",
            "painkiller\n",
            "backstabbers\n",
            "wealthier\n",
            "'delete'\n",
            "binged\n",
            "congealed\n",
            "'hey\n",
            "efffect\n",
            "precalc\n",
            "unburdened\n",
            "takeout\n",
            "brushy\n",
            "ziploc\n",
            "feeing\n",
            "meretricious\n",
            "fogs\n",
            "posh\n",
            "uncool\n",
            "everything and\n",
            "did moderately\n",
            "the mean\n",
            "crazy things\n",
            "thought my\n",
            " her\n",
            "intense loneliness\n",
            "thought that\n",
            "a human\n",
            "responders\n",
            "sould\n",
            "thoughful\n",
            "tasking\n",
            "ocupied\n",
            "jacksonville\n",
            "welbution\n",
            "fauling\n",
            "kindergarden\n",
            "0th\n",
            "unbeliveable\n",
            "instandly\n",
            "slided\n",
            "thesedays\n",
            "sexualized\n",
            "hidenseek\n",
            "3th\n",
            "2th\n",
            "imanigation\n",
            "fleed\n",
            "multible\n",
            "batteling\n",
            "mathematic\n",
            "precalculated\n",
            "equaltion\n",
            "overdraw\n",
            "childporn\n",
            "seach\n",
            "prositiv\n",
            "videochatted\n",
            "overjumped\n",
            "splitted\n",
            "aganist\n",
            "catergory's\n",
            "meorie\n",
            "remembler\n",
            "develope\n",
            "attacktion\n",
            "loli\n",
            "qna\n",
            "minaj\n",
            "omegle\n",
            "antention\n",
            "favoritism\n",
            "gravitated\n",
            "wholesomememes\n",
            "drunkenly\n",
            "etc”\n",
            "screenshooting\n",
            "“suicide\n",
            "attempts”\n",
            "srunchie\n",
            "p’s\n",
            "“fake\n",
            "“dad\n",
            "joke”\n",
            "hade\n",
            "“faking\n",
            "handdle\n",
            "boisterously\n",
            "m27\n",
            "nooo\n",
            "“first\n",
            "love”\n",
            "“that”\n",
            "—————————————————\n",
            "resents\n",
            "dumbly\n",
            "balled\n",
            "minshift\n",
            "cat’s\n",
            "anymorr\n",
            "worthles\n",
            "“thin\n",
            "okness”\n",
            "fun”\n",
            "transphobic\n",
            "finer\n",
            "laughingstock\n",
            "deters\n",
            "“messed\n",
            "baraz\n",
            "alzheimer's\n",
            "life—love\n",
            "all—\n",
            "before—\n",
            "what’re\n",
            "joyous\n",
            "reeeeeeeeeeeeeeeeeeeeeeeeee\n",
            "threated\n",
            "retraining\n",
            "excelling\n",
            "8ish\n",
            "soggy\n",
            "vacantly\n",
            "disassociation\n",
            "rampage\n",
            "didntstick\n",
            "soooooo\n",
            "eieuryehsuudiwjejrjeiieid\n",
            "culprits\n",
            "financier\n",
            "independant\n",
            "recuring\n",
            "voila\n",
            "perscription\n",
            "excecute\n",
            "adderal\n",
            "withdraws\n",
            "bottomline\n",
            "physically—\n",
            "skeletor\n",
            "ohysically\n",
            "coder\n",
            "poached\n",
            "dazing\n",
            "     for\n",
            "  well\n",
            "tittle\n",
            "   well\n",
            "shelby\n",
            "peaky\n",
            "     and\n",
            "sparknotes\n",
            "major's\n",
            "     i'm\n",
            "winging\n",
            "romance \n",
            "her… \n",
            "nowadays…\n",
            "crapy\n",
            "sprouted\n",
            "delt\n",
            "“perfect\n",
            "“others\n",
            "worse”\n",
            "comas\n",
            "23yo\n",
            "methadone\n",
            "rambly\n",
            "absolutes\n",
            "stung\n",
            "off”\n",
            "alomne\n",
            "coworkers’\n",
            "boss’s\n",
            "“evidence”\n",
            "rewatch\n",
            "ronav\n",
            "stomped\n",
            "powerleveled\n",
            "linkin\n",
            "park's\n",
            "complementing\n",
            "dispair\n",
            "methamphetamine\n",
            "ambivalence\n",
            "“boy\n",
            "crazy”\n",
            "takecare\n",
            "keppra\n",
            "strugglers\n",
            "takeaways\n",
            "councelor\n",
            "tellin\n",
            "dooming\n",
            "ehat\n",
            "refunds\n",
            "goung\n",
            "soothes\n",
            "spectacle\n",
            "depreciating\n",
            "helath\n",
            "”violent”\n",
            "deoderant\n",
            "tightening\n",
            "10hrs\n",
            "strucks\n",
            "blackness\n",
            "sucicde\n",
            "unworthiness\n",
            "mangaged\n",
            "horriblely\n",
            "withinh\n",
            "glonk\n",
            "anwser\n",
            "monthy\n",
            "hedache\n",
            "hydrating\n",
            "shittest\n",
            "disappering\n",
            "uninstalled\n",
            "latches\n",
            "visceral\n",
            "disorienting\n",
            "mbw78p0w5w\n",
            "😷\n",
            "idon't\n",
            "dystheist\n",
            "misotheist\n",
            "throughs\n",
            "🙏🏼\n",
            "obyerthink\n",
            "moght\n",
            "ypu\n",
            "eant\n",
            "stabilty\n",
            "ezperience\n",
            "gooooo\n",
            "5qiuqkhapcg\n",
            "ensued\n",
            "relived\n",
            "babydoll\n",
            "db1a8lt5jlq\n",
            "diferent\n",
            "bfrb\n",
            "bfrbs\n",
            "eyelids\n",
            "quirk\n",
            "infantryman\n",
            "rifleman\n",
            "warmed\n",
            "brothers'\n",
            "cinder\n",
            "bloodthirsty\n",
            "sloshed\n",
            "whimper\n",
            "streamer’s\n",
            "“calm\n",
            "“drama\n",
            "queen”\n",
            "'weird'\n",
            "'hoping'\n",
            "scapegoat\n",
            "“symptom”\n",
            "'be\n",
            "yourself'\n",
            "momma's\n",
            "hmph\n",
            "elated\n",
            "boorrrng\n",
            "approvingly\n",
            "your's\n",
            "jeers\n",
            "laugs\n",
            "how'd\n",
            "393253445\n",
            "b9mhu1inezg\n",
            "igshid\n",
            "dquwrkqm03s4\n",
            "farbe8\n",
            "jockes\n",
            "jocking\n",
            "fiercer\n",
            "towars\n",
            "textpost\n",
            "trashcan\n",
            "alwayssss\n",
            "gurl\n",
            "popcorn\n",
            "trolled\n",
            "catfished\n",
            "ungiven\n",
            "imho\n",
            "upend\n",
            "punchline\n",
            "ahhhhhh\n",
            "schoolyear\n",
            "worrries\n",
            "contemplation\n",
            "duffel\n",
            "frappuccino\n",
            "morning’s\n",
            "cr1116\n",
            "noodling\n",
            "play”\n",
            "only…\n",
            "rotted\n",
            "shears\n",
            "sweeting\n",
            "strats\n",
            "thiking\n",
            "myselfs\n",
            "crier\n",
            "norwood\n",
            "buzzcuts\n",
            "loooonnngg\n",
            "commited\n",
            "ungodly\n",
            "butted\n",
            "punchbag\n",
            "frantic\n",
            "petersen\n",
            "q2jwmpilgb0\n",
            "immmunity\n",
            "living”\n",
            "through \n",
            "gripe\n",
            "woodworking\n",
            "bookbinding\n",
            "boardgames\n",
            "youll\n",
            "mechanisn\n",
            "noradrenaline\n",
            "onservation\n",
            "leves\n",
            "presciption\n",
            "microdose\n",
            "benedictine\n",
            "2t8vdcsqm0agpbx\n",
            "suicidebreavement\n",
            "♥️\n",
            "hleppp\n",
            "unxared\n",
            "hnc\n",
            "wbu\n",
            "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n",
            "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n",
            "bipolarreddit\n",
            "media—including\n",
            "media—which\n",
            "549431db\n",
            "vesh\n",
            "549431da\n",
            "vese\n",
            "549431c5\n",
            "vesf\n",
            "549431c4\n",
            "vesc\n",
            "549431c7\n",
            "vesd\n",
            "549431c6\n",
            "vesa\n",
            "soul’s\n",
            "complimentary\n",
            "modalities\n",
            "neuropharmacologist\n",
            "549431c1\n",
            "vesb\n",
            "549431c0\n",
            "veso\n",
            "dbsa’s\n",
            "549431c3\n",
            "vesp\n",
            "to you—you’re\n",
            "cnn\n",
            "baszerz\n",
            "freeeeeeeeeeee\n",
            "sloppiest\n",
            "wipers\n",
            "sprawled\n",
            "excusable\n",
            "actuallg\n",
            "bgk9226\n",
            "mutilated\n",
            "immolate\n",
            "atbp\n",
            "peopel\n",
            "finch's\n",
            "😁😂\n",
            "wlnkfoy7ng8\n",
            "crocheting\n",
            "lexipro\n",
            "fapped\n",
            "trollish\n",
            "undoes\n",
            "'minister\n",
            "loneliness'\n",
            "'green\n",
            "thumb'\n",
            "'in\n",
            "person'\n",
            "dicaprio's\n",
            "kleenex\n",
            "chili\n",
            "piecing\n",
            "extraverted\n",
            "unlikeable\n",
            "rescheduled\n",
            "gazillion\n",
            "hygienic\n",
            "preyed\n",
            "crumbles\n",
            "boba\n",
            "‘change’\n",
            "prying\n",
            "deprive\n",
            "debilitates\n",
            "changes”\n",
            "learn”\n",
            "indistinguishably\n",
            "whouldnt\n",
            "eard\n",
            "relly\n",
            "menthal\n",
            "scurry\n",
            "misaligned\n",
            "cramped\n",
            "angering\n",
            "eminem\n",
            "autists\n",
            "coos\n",
            "vulgarly\n",
            "mediately\n",
            "singe\n",
            "physcology\n",
            "“feeling\n",
            "unanswerable\n",
            "“happiness”\n",
            "x4eqdfo\n",
            "b’s\n",
            "oposite\n",
            "tinitus\n",
            "crisscross\n",
            "“almost\n",
            "80's\n",
            "'of\n",
            "better'\n",
            "'perhaps\n",
            "'there\n",
            "away'\n",
            "nastiness\n",
            "deli\n",
            "tiktok\n",
            "waaaay\n",
            "sinapses\n",
            "underfed\n",
            "gushing\n",
            "cinched\n",
            "maxing\n",
            "babymonkey\n",
            "fervour\n",
            "apebrain\n",
            "❤️offering\n",
            "friendship❤️\n",
            "money's\n",
            "doomy\n",
            "barfing\n",
            "« no\n",
            "interiorize\n",
            "you »\n",
            "human’s\n",
            "nowi\n",
            "liveable\n",
            "inmature\n",
            "storm's\n",
            "coyotes\n",
            "crossposted\n",
            "facetimed\n",
            "'s\n",
            "partiers\n",
            "therapyy\n",
            "qualms\n",
            "you’\n",
            "ryhmed\n",
            "“new”\n",
            "keeo\n",
            "getiing\n",
            "hooping\n",
            "everybody’s\n",
            "arround\n",
            "jobtraining\n",
            "breadcrum\n",
            "yourself…\n",
            "was…\n",
            "fussy\n",
            "that…\n",
            "coaxing\n",
            "“unexpectedly”\n",
            "peeked\n",
            "drily\n",
            "helpless…\n",
            "studious\n",
            "ovarian\n",
            "loathed\n",
            "whatever…\n",
            "incomes\n",
            "time—that\n",
            "grades—all\n",
            "misinterpreted…\n",
            "“caught”\n",
            "“right\n",
            "all…”\n",
            "contaminate\n",
            "part…\n",
            "luxuries\n",
            "transferrable\n",
            "filial\n",
            "parents—something\n",
            "anything”\n",
            "stomps\n",
            "wheter\n",
            "asthmatic\n",
            "saviors\n",
            "hell's\n",
            "dejection\n",
            "‘friends’\n",
            "'open\n",
            "flue\n",
            "fked\n",
            "indiffenernce\n",
            "prefered\n",
            "lerned\n",
            "surfice\n",
            "procrastinator\n",
            "'punishment'\n",
            "'who\n",
            "alsmost\n",
            "expanses\n",
            "'places'\n",
            "deceives\n",
            "'to\n",
            "youth'\n",
            "bolbbalgan4\n",
            "morning…first\n",
            "reruns\n",
            "flicking\n",
            "boogers\n",
            "willies\n",
            "“class\n",
            "clown”\n",
            "farted\n",
            "audibly\n",
            "baptist\n",
            "freakin\n",
            "croniclly\n",
            "shelving\n",
            "ikea\n",
            "berrating\n",
            "waaay\n",
            "absorbtion\n",
            "patted\n",
            "person”\n",
            "panicattack\n",
            "fj0eys\n",
            "“disease”\n",
            "“illness”\n",
            "thewellwishers\n",
            "functionable\n",
            "hasoved\n",
            "gummy\n",
            "nightcore\n",
            "accessable\n",
            "bioparents\n",
            "🙄\n",
            "and'll\n",
            "​suddenly\n",
            "'bff'\n",
            "clichéd\n",
            "withought\n",
            "post’s\n",
            "ahhhhhhhhhhhhhhhhhh\n",
            "camhs\n",
            "awkard\n",
            "sympathized\n",
            "lucked\n",
            "unrestrained\n",
            "“death\n",
            "grasses\n",
            " sylvia\n",
            "t's\n",
            "disassociate\n",
            "intrest\n",
            "niceness\n",
            "fazed\n",
            "gogh's\n",
            "dunken\n",
            "doughnuts\n",
            "familer\n",
            "risperidone\n",
            "flustered\n",
            "“suicide”\n",
            "“final”\n",
            "auditioned\n",
            "yugioh\n",
            "shallowness\n",
            "smalle\n",
            "heave\n",
            "bottle's\n",
            "litteraly\n",
            "astricks\n",
            "house'\n",
            "'is\n",
            "too'\n",
            "groupchat\n",
            "handle'\n",
            "rambley\n",
            "'week'\n",
            "backtrack\n",
            "tting\n",
            "someoe\n",
            "roids\n",
            "'dont\n",
            "that'\n",
            "recehed\n",
            "'kalinka'\n",
            "lbh\n",
            "30lbs\n",
            "ulterior\n",
            "fine”\n",
            "”maybe\n",
            "“attitude”\n",
            "guardianship\n",
            "envied\n",
            "midwest\n",
            "hopin\n",
            "“acquaintances”\n",
            "remebered\n",
            "abstain\n",
            "predicament\n",
            "gordon\n",
            "ramseys\n",
            "13f\n",
            "earring\n",
            "'roll\n",
            "punches'\n",
            "funerals\n",
            "mister\n",
            "overstimulated\n",
            "sicken\n",
            "forsure\n",
            "recuperate\n",
            "shaky\n",
            "fizzling\n",
            "sqaud\n",
            "readjusting\n",
            "thirstier\n",
            "hibernation\n",
            "doubtfulness\n",
            "cracker\n",
            "incompletes\n",
            "envelopes\n",
            "wouldve\n",
            "“exit”\n",
            "simma\n",
            "kiddos\n",
            "malcontent\n",
            "anjmals\n",
            "tget\n",
            "motels\n",
            "emulating\n",
            "damsel\n",
            "superiors\n",
            "advising\n",
            "adminy\n",
            "tonight's\n",
            "agry\n",
            "i'v\n",
            "giess\n",
            "amazes\n",
            "yoh\n",
            "giddy\n",
            "behest\n",
            "dunst’s\n",
            "melancholia\n",
            "panics\n",
            "serene\n",
            "mri's\n",
            "biopsy\n",
            "usaully\n",
            "tonmy\n",
            "evwn\n",
            "tonfuxking\n",
            "blasio\n",
            "loophole\n",
            "mandela\n",
            "theblackholethaticreate\n",
            "esport\n",
            "forgetfulness\n",
            "stressor\n",
            "cheetos\n",
            "nighttime\n",
            "livelihoods\n",
            "screeching\n",
            "teetering\n",
            "myslelf\n",
            "cheyskye420\n",
            "raindrops\n",
            "planing\n",
            "sorrowful\n",
            "finnicky\n",
            "‘sick’\n",
            "attractice\n",
            "womanizer\n",
            "douchbag\n",
            "vibin\n",
            "stsy\n",
            "‘depressed‘\n",
            "“everything’s\n",
            "all”\n",
            "glum\n",
            "ebbing\n",
            "2months\n",
            "mediocrity\n",
            "damm\n",
            "juts\n",
            "restocked\n",
            "😅😭\n",
            "rela\n",
            "abosultue\n",
            "curls\n",
            "outings\n",
            "💚\n",
            "unbelonging\n",
            "unfitting\n",
            "unexplainable\n",
            "reoccuring\n",
            "unprompted\n",
            "exude\n",
            "addicition\n",
            "pretttyyyyy\n",
            "redditor\n",
            "acquittance\n",
            "trivialities\n",
            "overreacted\n",
            "sayin\n",
            "thaaat\n",
            "webchat\n",
            "“break”\n",
            "derealisation\n",
            "600lb\n",
            "chanels\n",
            "woundering\n",
            "narcissus\n",
            "intell\n",
            "thini\n",
            "gald\n",
            "'break\n",
            "down'\n",
            "'coffeeshops'\n",
            "soooooort\n",
            "chocolaty\n",
            "woooooshhh\n",
            "lentils\n",
            "rrrrrreeeeaaaally\n",
            "''i\n",
            "hair''\n",
            "twitchy\n",
            "''little''\n",
            "waive\n",
            "breel\n",
            "'suggestions'\n",
            "anythjng\n",
            "triviality\n",
            "fiance's\n",
            "hospitalisation\n",
            "audi\n",
            "lodged\n",
            "houseshare\n",
            "comforter\n",
            "handsom\n",
            "furloughed\n",
            "😡😡\n",
            "artbooks\n",
            "i̇\n",
            "nasally\n",
            "eloquently\n",
            "totoro\n",
            "things”\n",
            "scurrying\n",
            "expunged\n",
            "“laid\n",
            "indefinitely”\n",
            "‘wave’\n",
            "restlesness\n",
            "conscientious\n",
            "mbti\n",
            "faceats\n",
            "schizoaffective\n",
            "n's\n",
            "agreeableness\n",
            "qualitatively\n",
            "psychforums\n",
            "mentalhealthforums\n",
            "naughtier\n",
            "overstayed\n",
            "upsurge\n",
            "jaden\n",
            "becker\n",
            "straights\n",
            "pro's\n",
            "75in\n",
            "17yr\n",
            "drawled\n",
            "npcs\n",
            "whys\n",
            "replayed\n",
            "stomp\n",
            "disintegrate\n",
            "useless”\n",
            "flaws”\n",
            "shelterless\n",
            "way”\n",
            "“catholic”\n",
            "semster\n",
            "telltale\n",
            "creatives\n",
            "shitheads\n",
            "stfu\n",
            "gated\n",
            "hyperhidrosis\n",
            "distressing\n",
            "ruthlessly\n",
            "jest\n",
            "asics\n",
            "stains\n",
            "fashionable\n",
            "barrister\n",
            "amitbion\n",
            "hideout\n",
            "bough\n",
            "colapse\n",
            "awlays\n",
            "gramatic\n",
            "shitbag\n",
            "rationalizing\n",
            "somewhere”\n",
            "trudeau\n",
            "lolz\n",
            "snowballing\n",
            "“crush”\n",
            "pandamic\n",
            "disinfect\n",
            "overeater\n",
            "other”\n",
            "collabed\n",
            "pisstake\n",
            "“chaotic”\n",
            "amiright\n",
            "“care”\n",
            "deppresed\n",
            "scenario's\n",
            "restock\n",
            "xnxax\n",
            "thst\n",
            "dictator”\n",
            "gentile\n",
            "posioned\n",
            "barricaded\n",
            "cynincal\n",
            "despairing\n",
            "imprison\n",
            "luke\n",
            "shoild\n",
            "coronaviris\n",
            "rightnow\n",
            "'brother'\n",
            "'we\n",
            "forgot'\n",
            "friend'\n",
            "otger\n",
            "tgen\n",
            "aperantly\n",
            "definately\n",
            "lonelyness\n",
            "tgat\n",
            "mutualy\n",
            "unafraid\n",
            "forseeable\n",
            "splatters\n",
            "wack\n",
            "hikes\n",
            "antiemetics\n",
            "4months\n",
            "“groups”\n",
            "agresive\n",
            "closeth\n",
            "hiting\n",
            "'hope\n",
            "best'\n",
            "'forget\n",
            "past'\n",
            "'live\n",
            "present'\n",
            "lasagna\n",
            "‘story’\n",
            "misdirections\n",
            "rafters\n",
            "abattoir\n",
            "tock\n",
            "tooo\n",
            "intimated\n",
            "sinch\n",
            "everywhen\n",
            "idkkk\n",
            "unempathetic\n",
            "rverything\n",
            "derailing\n",
            "engineeeing\n",
            "lifeget\n",
            "deugs\n",
            "rhrough\n",
            "scuffle\n",
            "“pieces”\n",
            "scarxlrd\n",
            "brainiac\n",
            "chimes\n",
            "exuse\n",
            "becuse\n",
            "wimper\n",
            "lison\n",
            "compleately\n",
            "exclaiming\n",
            "mojor\n",
            "ssaying\n",
            "abortrion\n",
            "infroknt\n",
            "continious\n",
            "remindended\n",
            "expat\n",
            "untrustworthy\n",
            "'human\n",
            "project'\n",
            "'important'\n",
            "'you'\n",
            "tanking\n",
            "undervalue\n",
            "transpired\n",
            "exibit\n",
            "'lesser\n",
            "evils'\n",
            "azriel\n",
            "guidanced\n",
            "champaine\n",
            "roosevelt\n",
            "cubao\n",
            "tidied\n",
            "declutter\n",
            "qurantine\n",
            "themselvs\n",
            "apperantly\n",
            "backchat\n",
            "v7rshyz\n",
            "panicing\n",
            "“helicopter\n",
            "plummets\n",
            "looooooong\n",
            "m'dew😭\n",
            "buddhistic\n",
            "misinterpret\n",
            "bums\n",
            "“money\n",
            "obnoxiousness\n",
            "flashbacks”\n",
            "whooped\n",
            "zoomer\n",
            "shrill\n",
            "where’s\n",
            "fuckin’\n",
            "“mental\n",
            "professional”\n",
            "😢\n",
            "pj’s\n",
            "shrank\n",
            "septic\n",
            "forensics\n",
            "kwanzaa\n",
            "patrick’s\n",
            "father’\n",
            "valentine’s\n",
            "“search\n",
            "leprechaun”\n",
            "closees\n",
            "lockdowns\n",
            "meditators\n",
            "weirdos\n",
            "p4aoze5\n",
            "huged\n",
            "proprely\n",
            "askreddit\n",
            "fk81ih\n",
            "chbosky\n",
            "bookcase\n",
            "expiriences\n",
            "blacking\n",
            "‘wow\n",
            "alone’\n",
            "hooray\n",
            "😤\n",
            "lightyears\n",
            "reddits\n",
            "'friends\n",
            "fiano\n",
            "'complaining'\n",
            "hurtfulness\n",
            "bookworms\n",
            "bookworm\n",
            "saranghaeyo\n",
            "sexuallity\n",
            "insecurites\n",
            "probalby\n",
            "lebian\n",
            "backhanded\n",
            "seeps\n",
            "inexistent\n",
            "windowless\n",
            "paves\n",
            "doted\n",
            "powerfully\n",
            "blindfold\n",
            "unchain\n",
            "child’s\n",
            "truest\n",
            "fearlessly\n",
            "triggerred\n",
            "potrays\n",
            "exaggereative\n",
            "potrayed\n",
            "neccessary\n",
            "spectrum's\n",
            "traffic's\n",
            "sorryif\n",
            "ahhhhh\n",
            "egoistical\n",
            "anout\n",
            "soppose\n",
            "honestlh\n",
            "ghoul\n",
            "comebacks\n",
            "tackles\n",
            "earl's\n",
            "blankness\n",
            "rn”\n",
            "shockwaves\n",
            "worlld\n",
            "existences\n",
            "brainpower\n",
            "guility\n",
            "thorugh\n",
            "touchy\n",
            "disapear\n",
            "incomparable\n",
            "commuting\n",
            "happy”\n",
            "mhmm\n",
            "godess\n",
            "shoudnt\n",
            "unimaginably\n",
            "buttload\n",
            "unsupportive\n",
            "kartel\n",
            "objectivly\n",
            "gutsy\n",
            "‘someone\n",
            "it’\n",
            "unregulated\n",
            "google'd\n",
            "totaled\n",
            "voicemail\n",
            "da's\n",
            "thurs\n",
            "bray\n",
            "indure\n",
            "shity\n",
            "constricting\n",
            "strides\n",
            "doordash\n",
            "💖💖💖\n",
            "yourself’s\n",
            "resentful\n",
            "deceitful\n",
            "juiced\n",
            "oxytocin\n",
            "doin”\n",
            "offload\n",
            "reflection’s\n",
            "peeled\n",
            "‪only\n",
            "conversation”\n",
            "homophobes\n",
            "cyuh\n",
            "graysom\n",
            "reciprocative\n",
            "graven\n",
            "stephan\n",
            "bult\n",
            "nirm\n",
            "qe2d4r0\n",
            "15inch\n",
            "bruise\n",
            "teeter\n",
            "totter\n",
            "untouch\n",
            "regal\n",
            "leaped\n",
            "’you're\n",
            "seeking’\n",
            "downvoted\n",
            "prods\n",
            "shoddy\n",
            "finaly\n",
            "shine…\n",
            "trancing\n",
            "howling\n",
            "'ve\n",
            "'re\n",
            "pollan\n",
            "simpathy\n",
            "sears\n",
            "'favourite\n",
            "films'\n",
            "interupting\n",
            "disapointment\n",
            "☹🙁😔\n",
            "stagnancy\n",
            "sidelines\n",
            "accumulates\n",
            "balcanic\n",
            "decease\n",
            "livin\n",
            "wrigley\n",
            "yarejare\n",
            "countrie's\n",
            "weakest\n",
            "dysthymic\n",
            "tachycardia\n",
            "brendanbruce42\n",
            "desirability\n",
            "redpill\n",
            "practices'\n",
            "razor's\n",
            "feigning\n",
            "😭😞\n",
            "parkrun\n",
            "5km\n",
            "landscaping\n",
            "raving\n",
            "nitpick\n",
            "disengenuous\n",
            "knkw\n",
            "disheartens\n",
            "idilations\n",
            "5'5\n",
            "than20000\n",
            "20sf\n",
            "fuckmylife\n",
            "leav\n",
            "“love”\n",
            "shined\n",
            "slighty\n",
            "“content\n",
            "pining\n",
            "buddying\n",
            "frosty\n",
            "infested\n",
            "coffins\n",
            "“mean”\n",
            "bedriddened\n",
            "🎉\n",
            "sjsbbabaywuwushzbzbaiwjssss\n",
            "fucjdk\n",
            "uducujc\n",
            "hdbsb\n",
            "uci\n",
            "kxkdjdngsgsvzvsvbzfu\n",
            "kf\n",
            "ifck\n",
            "ufuck\n",
            "ucj\n",
            "mfjcjcjcu\n",
            "ndci\n",
            "wales\n",
            "🤷‍♂️\n",
            "lmfao\n",
            "i´ll\n",
            "perfer\n",
            "fleeced\n",
            "fezzl6\n",
            "haish\n",
            "somepoint\n",
            "sucky\n",
            "underwhelmed\n",
            "brittle\n",
            "athelstan\n",
            "kimberley\n",
            "nw6\n",
            "7sn\n",
            "charmaine\n",
            "kazmi\n",
            "unescapable\n",
            "hangsman\n",
            "‘larger’\n",
            "mope\n",
            "campfire\n",
            "cringeworthy\n",
            "fucjkkj\n",
            "sometype\n",
            "whyyy\n",
            "uicideboys\n",
            "botw\n",
            "moping\n",
            "already”\n",
            "upscale\n",
            "disobeying\n",
            "fining\n",
            "12k\n",
            "biden\n",
            "bernie\n",
            "good”\n",
            "qualities—and\n",
            "this—and\n",
            "marmalade\n",
            "warhorse\n",
            "aa3afg3fzaq\n",
            "detracting\n",
            "hassled\n",
            "flatter\n",
            "helooo\n",
            "ginseng\n",
            "nejfake\n",
            "sullen\n",
            "envelops\n",
            "taints\n",
            "capeable\n",
            "hyperfocus\n",
            "shone\n",
            "sedless\n",
            "calles\n",
            "wuestions\n",
            "daggers\n",
            "sniffling\n",
            "10y\n",
            "bj's\n",
            "ahhahahah\n",
            "excalibur\n",
            "tractor\n",
            "unspoken\n",
            "obliviousentity\n",
            "juss\n",
            "boyzafar\n",
            "groupmates\n",
            "aburden\n",
            "‘you’ve\n",
            "for’\n",
            "knowif\n",
            "needadvice\n",
            "idec\n",
            "808s\n",
            "kanye\n",
            "desparate\n",
            "deoression\n",
            "500d\n",
            "sighed\n",
            "paring\n",
            "crutches\n",
            "consultations\n",
            "f'ing\n",
            "tourette’s\n",
            "ridiculing\n",
            "tryyy\n",
            "f30\n",
            "propanol\n",
            "worrier\n",
            "rocketed\n",
            "woudnt\n",
            "caronavirus\n",
            "again—simple\n",
            "malingerer\n",
            "anxiety’s\n",
            "imediatedly\n",
            "agressive\n",
            "untrained\n",
            "suckssssss\n",
            "🗝\n",
            "f25\n",
            "tinkering\n",
            "chopping\n",
            "tinkered\n",
            "torure\n",
            "hells\n",
            "‘normal\n",
            "“blank”\n",
            "comorbid\n",
            "besties\n",
            "fatso\n",
            "terrify\n",
            "ahainst\n",
            "relaly\n",
            "developped\n",
            "psychosomatic\n",
            "getaway\n",
            "tireing\n",
            "diarree\n",
            "invision\n",
            "everday\n",
            "provokes\n",
            "titrating\n",
            "yesr\n",
            "readinh\n",
            "sayinh\n",
            "againyaddayadda\n",
            "potholes\n",
            "townies\n",
            "‘fine’\n",
            "flatly\n",
            "fluke\n",
            "📍question\n",
            "📍\n",
            "xpost\n",
            "bustling\n",
            "ncov\n",
            "hellooo\n",
            "chickens\n",
            "facetimes\n",
            "150xl\n",
            "singlehanded\n",
            "talk”\n",
            "ifunny\n",
            "dulls\n",
            "preconceptions\n",
            "homestudy\n",
            "conversating\n",
            "suddenley\n",
            "pychward\n",
            "eaiser\n",
            "hauntingly\n",
            "dentures\n",
            "organically\n",
            "agoraphobic\n",
            "resocialized\n",
            "socialable\n",
            "suga\n",
            "weeb\n",
            "depression's\n",
            "killmyself\n",
            "codamol\n",
            "unconcious\n",
            "airway\n",
            "receving\n",
            "jogged\n",
            "brainer\n",
            "rediscovering\n",
            "layaway\n",
            "muisic\n",
            "fanily\n",
            "tubthumping\n",
            "recapture\n",
            "spravato\n",
            "reimburse\n",
            "sevete\n",
            "wtt\n",
            "hahah\n",
            "homie\n",
            "sated\n",
            "ombudsmen\n",
            "“help”\n",
            "incapacitated\n",
            "“ask\n",
            "anziety\n",
            "coaxed\n",
            "scatterbrain\n",
            "bogged\n",
            "bobble\n",
            "greyest\n",
            "overcast\n",
            "combatted\n",
            "filrnp\n",
            "iossmf\n",
            "arn't\n",
            "haves\n",
            "drolling\n",
            "clinician\n",
            "invulnerable\n",
            "saught\n",
            "klonipin\n",
            "weekish\n",
            "awfull\n",
            "voise\n",
            "'m\n",
            "crestfallen\n",
            "crouched\n",
            "anthill\n",
            "“recovered”\n",
            "concussions\n",
            "waze\n",
            "thes\n",
            "sourts\n",
            "goofed\n",
            "lovan\n",
            "friedenbach\n",
            "talia\n",
            "95050\n",
            "'just'\n",
            "'puberty'\n",
            "'depression'\n",
            "emigrate\n",
            "exhauted\n",
            "carpooled\n",
            "airbnb\n",
            "outnumber\n",
            "descus\n",
            "bulied\n",
            "dislexik\n",
            "siptums\n",
            "sumthing\n",
            "gose\n",
            "cronic\n",
            "dusant\n",
            "colige\n",
            "imunity\n",
            "feal\n",
            "thal\n",
            "ferst\n",
            "brout\n",
            "atintion\n",
            "awfule\n",
            "tride\n",
            "menta\n",
            "coudnt\n",
            "woldnt\n",
            "whene\n",
            "allll\n",
            "ruind\n",
            "fellings\n",
            "48kg\n",
            "90kg\n",
            "themselfs\n",
            "peoplle\n",
            "homeowner\n",
            "snagged\n",
            "600mg\n",
            "worrisome\n",
            "l’ve\n",
            "anyting\n",
            "fridge’s\n",
            "vigilant\n",
            "absoluetly\n",
            "“stepfather”\n",
            "nutty\n",
            "lotsa\n",
            "10months\n",
            "i's\n",
            "12lb\n",
            "flaunt\n",
            "cornavirus\n",
            "impacting\n",
            "strugglling\n",
            "„die\n",
            "hoffnung\n",
            "stirbt\n",
            "zuletzt“\n",
            "ronaszn\n",
            "9ish\n",
            "randomally\n",
            "meaner\n",
            "eboy\n",
            "phlebotomist\n",
            "theys\n",
            "thems\n",
            "“solving\n",
            "pills”\n",
            "signer\n",
            "there’d\n",
            "miltary\n",
            "contracters\n",
            "skillset\n",
            "vanlafaxine\n",
            "stabilising\n",
            "tubing\n",
            "patting\n",
            "cpr\n",
            "rippled\n",
            "comfired\n",
            "thiavion\n",
            "adipizol\n",
            "ranol\n",
            "zaps\n",
            "atart\n",
            "accidently\n",
            "feeljng\n",
            "theycontinued\n",
            "resurfaced\n",
            "shattering\n",
            "doc’s\n",
            "tacs\n",
            "24f\n",
            "attachement\n",
            "banker\n",
            "shadowed\n",
            "cybernetics\n",
            "cyborg\n",
            "leach\n",
            "ramping\n",
            "27f\n",
            "wtfff\n",
            "comfident\n",
            "restablish\n",
            "mycology\n",
            "babbles\n",
            "skmething\n",
            "dwindles\n",
            "reinvent\n",
            "skated\n",
            "grunt\n",
            "depricating\n",
            "immobility\n",
            "months’\n",
            "wither\n",
            "consequentially\n",
            "mononucleosis\n",
            "admittance\n",
            "unenroll\n",
            "agitates\n",
            "ednos\n",
            "dexedrine\n",
            "dnri\n",
            "amphetamines\n",
            "“gift”\n",
            "explainging\n",
            "disassociative\n",
            "'mean'\n",
            "shudder\n",
            "matter”\n",
            "“woke\n",
            "bipolarism\n",
            "trivialized\n",
            "try’s\n",
            "delusionally\n",
            "militaristic\n",
            "hypersexuality\n",
            "hypercriminalty\n",
            "youngboy\n",
            "harvesters\n",
            "dehumanized\n",
            "lucrative\n",
            "avoidable\n",
            "unfuck\n",
            "attemp\n",
            "fiancés\n",
            "“can’t\n",
            "bestie”\n",
            "damndest\n",
            "innapropriately\n",
            "accomodations\n",
            "kratom\n",
            "brome\n",
            "passcode\n",
            "quiero\n",
            "feign\n",
            "leonarda\n",
            "marianelo\n",
            "5ish\n",
            "annihilating\n",
            "committment\n",
            "eloquent\n",
            "interning\n",
            "unfitness\n",
            "xxxx”\n",
            "zealous\n",
            "“living”\n",
            "finacial\n",
            "becausehe\n",
            "cushy\n",
            "cosign\n",
            "nessecity\n",
            "laughingly\n",
            "ebbs\n",
            "bringimg\n",
            "bedridden\n",
            "relative’s\n",
            "bodyload\n",
            "livable\n",
            "lustral\n",
            "hobby's\n",
            "compleatly\n",
            "bereating\n",
            "heys\n",
            "sellotaped\n",
            "some”\n",
            "“listen”\n",
            "jumpstart\n",
            "unscrew\n",
            "“haha\n",
            "emsam\n",
            "catastrophize\n",
            "'just\n",
            "more'\n",
            "'advice'\n",
            "“boundaries”\n",
            "“coming\n",
            "home”\n",
            "yous”\n",
            "vacation”\n",
            "“he\n",
            "❤️❤️\n",
            "gabapentin\n",
            "‘their\n",
            "whatever’\n",
            "sharpie\n",
            "“its\n",
            "fault”\n",
            "manically\n",
            "darting\n",
            "shortlived\n",
            "foreskin\n",
            "bothed\n",
            "thnks\n",
            "eacitalopram\n",
            "complication\n",
            "seperste\n",
            "“yes”\n",
            "withholds\n",
            "exhilarating\n",
            "ablut\n",
            "bulletproof\n",
            "deflating\n",
            "terrifyingly\n",
            "hitler\n",
            "wilted\n",
            "instilling\n",
            "grammy's\n",
            "coward's\n",
            "'a\n",
            "shame'\n",
            "potential'\n",
            "'should've\n",
            "problems'\n",
            "arresting\n",
            "sapping\n",
            "exacts\n",
            "eles\n",
            "hangman’s\n",
            "enraged\n",
            "fuuuuuuuuuuuuuuuuck\n",
            "autim\n",
            "medicaton\n",
            "convulsive\n",
            "yawns\n",
            "quietness\n",
            "hmmmmm\n",
            "horrifically\n",
            "prodding\n",
            "shlumped\n",
            "jazzed\n",
            "fricking\n",
            "copays\n",
            "psychotherapists\n",
            "“armour”\n",
            "miiiight\n",
            "“major\n",
            "department”\n",
            "personell\n",
            "“tools”\n",
            "adrift\n",
            "chics\n",
            "marty\n",
            "mcfly\n",
            "voynich\n",
            "camper\n",
            "thins\n",
            "overhear\n",
            "qutting\n",
            "flatlined\n",
            "pvd\n",
            "floaters\n",
            "tumer\n",
            "requir\n",
            "makea\n",
            "jeslous\n",
            "marrige\n",
            "succses\n",
            "Number of words not in the vocabulary:  8243\n",
            "Percentage of words not in the vocabulary:  8.243\n"
          ]
        }
      ],
      "source": [
        "max_words = 100000\n",
        "\n",
        "embedding_matrix = np.zeros((max_words, 300))\n",
        "c = 0\n",
        "for word, i in word_index.items():\n",
        "  if i < max_words:\n",
        "    embedding_vector = vocab_and_vectors.get(word)\n",
        "    if embedding_vector is None:\n",
        "          c +=1;\n",
        "          print(word)\n",
        "  # words that cannot be found will be set to 0\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "print(\"Number of words not in the vocabulary: \", c)\n",
        "print(\"Percentage of words not in the vocabulary: \", (c/max_words)*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "NtGRSmf9MoBd"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "\n",
        "accuracy_threshold = 0.99\n",
        "\n",
        "class myCallback(keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "      if logs.get('accuracy') > accuracy_threshold:\n",
        "        print(\"\\nReached %2.2f%% accuracy, so we will stop training\" % (accuracy_threshold*100))\n",
        "        self.model.stop_training = True\n",
        "\n",
        "acc_callback = myCallback()\n",
        "\n",
        "# Save the Best Model\n",
        "filepath = \"mymodel1.h5\"\n",
        "checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_accuracy', verbose=2, save_best_only=True,\n",
        "                                             save_weights_only=False, mode='max')\n",
        "\n",
        "# Callback list\n",
        "callback_list = [acc_callback, checkpoint]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f2bts_QM-Wq",
        "outputId": "cd8af270-4bdf-4ac1-9d73-26d189d550b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 200)]             0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 200, 300)          30000000  \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 199, 32)           19232     \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 99, 32)           0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 3168)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 3)                 9507      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 30,028,739\n",
            "Trainable params: 28,739\n",
            "Non-trainable params: 30,000,000\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "#the Model\n",
        "\n",
        "keras.backend.clear_session()\n",
        "\n",
        "max_length = 200\n",
        "embedding_dim = 300\n",
        "number_of_classes = 3\n",
        "# define CNN model\n",
        "\n",
        "def CNN(embedding_matrix):\n",
        "  input = Input(shape=(max_length,))\n",
        "  embedding = Embedding(max_words, embedding_dim, input_length=200, weights=[embedding_matrix], trainable=False)(input)\n",
        "  conv1 = Conv1D(32, 2, activation='relu')(embedding)\n",
        "  pool1 = MaxPooling1D(2)(conv1)\n",
        "  flat = Flatten()(pool1)\n",
        "  output_layer = Dense(number_of_classes, activation='softmax')(flat)\n",
        "  model = Model(inputs=input, outputs=output_layer)\n",
        "  return model\n",
        "\n",
        "\n",
        "# call the model\n",
        "cnn_model_pretrained = CNN(embedding_matrix)\n",
        "\n",
        "cnn_model_pretrained.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "nsLhMK-gM-hM"
      },
      "outputs": [],
      "source": [
        "cnn_model_pretrained.compile(optimizer = 'adam',\n",
        "                      loss = 'sparse_categorical_crossentropy',\n",
        "                      metrics =['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQOrM7_uM-uM",
        "outputId": "0b6e2e64-1887-4b22-ddfa-8d1f3d74fd8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "202/203 [============================>.] - ETA: 0s - loss: 0.8478 - accuracy: 0.6049\n",
            "Epoch 1: val_accuracy improved from -inf to 0.21082, saving model to mymodel1.h5\n",
            "203/203 [==============================] - 10s 46ms/step - loss: 0.8481 - accuracy: 0.6048 - val_loss: 1.7671 - val_accuracy: 0.2108\n",
            "Epoch 2/10\n",
            "202/203 [============================>.] - ETA: 0s - loss: 0.7893 - accuracy: 0.6318\n",
            "Epoch 2: val_accuracy improved from 0.21082 to 0.26907, saving model to mymodel1.h5\n",
            "203/203 [==============================] - 7s 33ms/step - loss: 0.7896 - accuracy: 0.6315 - val_loss: 1.5683 - val_accuracy: 0.2691\n",
            "Epoch 3/10\n",
            "202/203 [============================>.] - ETA: 0s - loss: 0.7453 - accuracy: 0.6587\n",
            "Epoch 3: val_accuracy did not improve from 0.26907\n",
            "203/203 [==============================] - 12s 58ms/step - loss: 0.7445 - accuracy: 0.6591 - val_loss: 1.9793 - val_accuracy: 0.1803\n",
            "Epoch 4/10\n",
            "201/203 [============================>.] - ETA: 0s - loss: 0.6725 - accuracy: 0.6867\n",
            "Epoch 4: val_accuracy did not improve from 0.26907\n",
            "203/203 [==============================] - 6s 29ms/step - loss: 0.6725 - accuracy: 0.6860 - val_loss: 1.7993 - val_accuracy: 0.2635\n",
            "Epoch 5/10\n",
            "202/203 [============================>.] - ETA: 0s - loss: 0.6054 - accuracy: 0.7262\n",
            "Epoch 5: val_accuracy improved from 0.26907 to 0.30236, saving model to mymodel1.h5\n",
            "203/203 [==============================] - 9s 43ms/step - loss: 0.6059 - accuracy: 0.7258 - val_loss: 1.8065 - val_accuracy: 0.3024\n",
            "Epoch 6/10\n",
            "201/203 [============================>.] - ETA: 0s - loss: 0.5497 - accuracy: 0.7610\n",
            "Epoch 6: val_accuracy improved from 0.30236 to 0.35922, saving model to mymodel1.h5\n",
            "203/203 [==============================] - 6s 32ms/step - loss: 0.5491 - accuracy: 0.7614 - val_loss: 2.1038 - val_accuracy: 0.3592\n",
            "Epoch 7/10\n",
            "203/203 [==============================] - ETA: 0s - loss: 0.4978 - accuracy: 0.7870\n",
            "Epoch 7: val_accuracy did not improve from 0.35922\n",
            "203/203 [==============================] - 7s 35ms/step - loss: 0.4978 - accuracy: 0.7870 - val_loss: 1.8746 - val_accuracy: 0.3329\n",
            "Epoch 8/10\n",
            "201/203 [============================>.] - ETA: 0s - loss: 0.4551 - accuracy: 0.8092\n",
            "Epoch 8: val_accuracy did not improve from 0.35922\n",
            "203/203 [==============================] - 7s 36ms/step - loss: 0.4548 - accuracy: 0.8093 - val_loss: 2.0901 - val_accuracy: 0.3148\n",
            "Epoch 9/10\n",
            "201/203 [============================>.] - ETA: 0s - loss: 0.4168 - accuracy: 0.8298\n",
            "Epoch 9: val_accuracy did not improve from 0.35922\n",
            "203/203 [==============================] - 6s 29ms/step - loss: 0.4168 - accuracy: 0.8296 - val_loss: 2.2021 - val_accuracy: 0.3204\n",
            "Epoch 10/10\n",
            "202/203 [============================>.] - ETA: 0s - loss: 0.3850 - accuracy: 0.8439\n",
            "Epoch 10: val_accuracy did not improve from 0.35922\n",
            "203/203 [==============================] - 9s 43ms/step - loss: 0.3851 - accuracy: 0.8440 - val_loss: 2.6404 - val_accuracy: 0.3356\n",
            "CPU times: user 1min 34s, sys: 4.46 s, total: 1min 38s\n",
            "Wall time: 1min 18s\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%time\n",
        "history = cnn_model_pretrained.fit(train_pad,\n",
        "                              train_labels_encoded,\n",
        "                              epochs = 10,\n",
        "                              batch_size = 32,\n",
        "                              verbose = 1,\n",
        "                              validation_split =0.1,\n",
        "                              callbacks = callback_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "9jLiDhLTNJQs"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Recreate the exact same model purely from the file:\n",
        "model = load_model(\"mymodel1.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3Vw60OZNKNc",
        "outputId": "e226fea7-9bbb-4f63-ba19-e868775e0fee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16/16 [==============================] - 0s 9ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,\n",
              "       0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,\n",
              "       0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,\n",
              "       1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,\n",
              "       0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
              "       1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,\n",
              "       1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 2, 1, 1, 1,\n",
              "       1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,\n",
              "       0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,\n",
              "       1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,\n",
              "       1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,\n",
              "       0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,\n",
              "       0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
              "       0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,\n",
              "       1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,\n",
              "       1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,\n",
              "       1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,\n",
              "       0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,\n",
              "       0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
              "       1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,\n",
              "       0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,\n",
              "       0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ],
      "source": [
        "y_pred = np.argmax(model.predict(test_pad), axis=-1)\n",
        "y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBA3OlYvNNTS",
        "outputId": "8c0d61c1-3e12-4eff-f6c9-3abfd8b32937"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.44      0.49       275\n",
            "           1       0.33      0.67      0.44       135\n",
            "           2       0.00      0.00      0.00        89\n",
            "\n",
            "    accuracy                           0.43       499\n",
            "   macro avg       0.29      0.37      0.31       499\n",
            "weighted avg       0.39      0.43      0.39       499\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(classification_report(test_labels_encoded,y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeSKbZYiNNV7",
        "outputId": "7fe767cc-7128-4c17-90e9-11875680cd11"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[122, 153,   0],\n",
              "       [ 43,  91,   1],\n",
              "       [ 56,  33,   0]])"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ],
      "source": [
        "\n",
        "confusion_matrix(test_labels_encoded,y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13kWqjR2hSNK"
      },
      "source": [
        "#BiLSTM With Fasttext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "PwIDMNqChVEh"
      },
      "outputs": [],
      "source": [
        "''' Callbacks with Checkpoint'''\n",
        "\n",
        "accuracy_threshold1 = 0.99\n",
        "\n",
        "class myCallback(keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "      if(logs.get('accuracy')>accuracy_threshold1):\n",
        "        print(\"\\nReached %2.2f%% accuracy so we will stop trianing\" % (accuracy_threshold1*100))\n",
        "        self.model.stop_training = True\n",
        "\n",
        "acc_callback0 = myCallback()\n",
        "  # Saved the Best Model\n",
        "filepath = \"mymodel000.h5\"\n",
        "checkpoint0 = keras.callbacks.ModelCheckpoint(filepath, monitor='val_accuracy', verbose=2, save_best_only=True,\n",
        "                                             save_weights_only=False, mode='max')\n",
        "  # callback list\n",
        "callback_list0= [acc_callback0, checkpoint0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4Afa9AIopQZ",
        "outputId": "1235f2cd-840b-429c-d3f5-81947a997708"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 200)]             0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 200, 64)           6400000   \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 64)               24832     \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dense (Dense)               (None, 3)                 195       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,425,027\n",
            "Trainable params: 6,425,027\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "#the Model\n",
        "\n",
        "keras.backend.clear_session()\n",
        "\n",
        "max_length = 200\n",
        "embedding_dim = 64\n",
        "number_of_classes = 3\n",
        "\n",
        "# Define BiLSTM model\n",
        "def lstm():\n",
        "    bi_text_inputs = Input(shape=(max_length,))\n",
        "    bi_embedding_layer = Embedding(max_words, embedding_dim, trainable=True, input_length=max_length)(bi_text_inputs)\n",
        "    LSTM_Layer_1 = Bidirectional(LSTM(32))(bi_embedding_layer)\n",
        "    bi_dense_layer_1 = Dense(number_of_classes, activation='softmax')(LSTM_Layer_1)\n",
        "    bilstm_model = Model(inputs=bi_text_inputs, outputs=bi_dense_layer_1)\n",
        "    return bilstm_model\n",
        "\n",
        "# Create the BiLSTM model with the embedding matrix\n",
        "lstm_model = lstm()\n",
        "\n",
        "# Print the model summary\n",
        "lstm_model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "xdytGEgKpItZ"
      },
      "outputs": [],
      "source": [
        "lstm_model.compile(optimizer = 'adam',\n",
        "                      loss = 'sparse_categorical_crossentropy',\n",
        "                      metrics =['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpY1_BMypWhh",
        "outputId": "eb4d7c6d-dd7c-48b9-d169-e53743afb77b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.9045 - accuracy: 0.5741\n",
            "Epoch 1: val_accuracy improved from -inf to 0.63205, saving model to mymodel000.h5\n",
            "226/226 [==============================] - 85s 348ms/step - loss: 0.9045 - accuracy: 0.5741 - val_loss: 0.8121 - val_accuracy: 0.6320\n",
            "Epoch 2/10\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.8152 - accuracy: 0.6209\n",
            "Epoch 2: val_accuracy did not improve from 0.63205\n",
            "226/226 [==============================] - 72s 317ms/step - loss: 0.8152 - accuracy: 0.6209 - val_loss: 0.8041 - val_accuracy: 0.6139\n",
            "Epoch 3/10\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.7503 - accuracy: 0.6598\n",
            "Epoch 3: val_accuracy did not improve from 0.63205\n",
            "226/226 [==============================] - 81s 358ms/step - loss: 0.7503 - accuracy: 0.6598 - val_loss: 0.7901 - val_accuracy: 0.6216\n",
            "Epoch 4/10\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.7184 - accuracy: 0.6774\n",
            "Epoch 4: val_accuracy did not improve from 0.63205\n",
            "226/226 [==============================] - 70s 312ms/step - loss: 0.7184 - accuracy: 0.6774 - val_loss: 0.8067 - val_accuracy: 0.6237\n",
            "Epoch 5/10\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.6755 - accuracy: 0.7025\n",
            "Epoch 5: val_accuracy did not improve from 0.63205\n",
            "226/226 [==============================] - 79s 351ms/step - loss: 0.6755 - accuracy: 0.7025 - val_loss: 0.8244 - val_accuracy: 0.6166\n",
            "Epoch 6/10\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.6466 - accuracy: 0.7164\n",
            "Epoch 6: val_accuracy did not improve from 0.63205\n",
            "226/226 [==============================] - 74s 328ms/step - loss: 0.6466 - accuracy: 0.7164 - val_loss: 0.8561 - val_accuracy: 0.5988\n",
            "Epoch 7/10\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.6122 - accuracy: 0.7335\n",
            "Epoch 7: val_accuracy did not improve from 0.63205\n",
            "226/226 [==============================] - 77s 343ms/step - loss: 0.6122 - accuracy: 0.7335 - val_loss: 0.9052 - val_accuracy: 0.5883\n",
            "Epoch 8/10\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.5720 - accuracy: 0.7555\n",
            "Epoch 8: val_accuracy did not improve from 0.63205\n",
            "226/226 [==============================] - 72s 321ms/step - loss: 0.5720 - accuracy: 0.7555 - val_loss: 0.9800 - val_accuracy: 0.5618\n",
            "Epoch 9/10\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.5391 - accuracy: 0.7682\n",
            "Epoch 9: val_accuracy did not improve from 0.63205\n",
            "226/226 [==============================] - 82s 363ms/step - loss: 0.5391 - accuracy: 0.7682 - val_loss: 0.9298 - val_accuracy: 0.6102\n",
            "Epoch 10/10\n",
            "226/226 [==============================] - ETA: 0s - loss: 0.5167 - accuracy: 0.7861\n",
            "Epoch 10: val_accuracy did not improve from 0.63205\n",
            "226/226 [==============================] - 71s 313ms/step - loss: 0.5167 - accuracy: 0.7861 - val_loss: 1.0272 - val_accuracy: 0.5683\n",
            "CPU times: user 17min 30s, sys: 38.9 s, total: 18min 9s\n",
            "Wall time: 12min 43s\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%time\n",
        "history4 = lstm_model.fit(train_pad,\n",
        "                              train_labels_encoded,\n",
        "                              epochs = 10,\n",
        "                              batch_size = 32,\n",
        "                              verbose = 1,\n",
        "                              validation_data=(val_pad, val_labels_encoded),\n",
        "                              callbacks = callback_list0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "IcoyBtvwpjfw"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Recreate the exact same model purely from the file:\n",
        "model4 = load_model(\"mymodel000.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ru_mcoUOpp2g",
        "outputId": "ed251c39-4f47-4bbe-e96c-b6f75f424bb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16/16 [==============================] - 2s 65ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,\n",
              "       1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,\n",
              "       0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,\n",
              "       0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,\n",
              "       0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
              "       0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,\n",
              "       1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
              "       1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
              "       0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,\n",
              "       0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,\n",
              "       0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
              "       1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,\n",
              "       0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,\n",
              "       1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,\n",
              "       0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,\n",
              "       0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
              "       0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ],
      "source": [
        "y_pred4 = np.argmax(model4.predict(test_pad), axis=-1)\n",
        "y_pred4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78T7jzslp1O7",
        "outputId": "11419ba5-8276-436a-ce75-9913f613209d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.44      0.49       275\n",
            "           1       0.33      0.67      0.44       135\n",
            "           2       0.00      0.00      0.00        89\n",
            "\n",
            "    accuracy                           0.43       499\n",
            "   macro avg       0.29      0.37      0.31       499\n",
            "weighted avg       0.39      0.43      0.39       499\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(classification_report(test_labels_encoded,y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHqqNffB2K3N"
      },
      "source": [
        "#CNN+BiLSTM with Fasttext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "oEE9oLtE2RKm"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}